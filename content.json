{"meta":{"title":"EyEular","subtitle":null,"description":null,"author":"Eulring","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-07-22T13:04:20.969Z","updated":"2018-07-22T13:04:20.969Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Indigo","date":"2018-07-23T03:30:34.473Z","updated":"2018-07-23T02:32:35.989Z","comments":true,"path":"custom/index.html","permalink":"http://yoursite.com/custom/index.html","excerpt":"","text":"Image image Blockquote 当blockquote、img、pre、figure为第一级内容时，在page布局中拥有card阴影，所有标题居中展示。 Content@card{ 目前的想法是预定义一系列内容模块，通过像输入 Markdown 标记一样来简单调用。好在 Markdown 没有把所有便于输入的符号占用，最终我定义了@moduleName{ ... }这种标记格式。如果你使用过Asp.Net MVC，一定会很熟悉这种用法，没错，就是razor。 page布局中的title和subtitle对应 Markdown 中的title和description。 基本的内容容器还是card，你可以这样使用card： 12345@card&#123;在`page`页中，建议把内容都放到`card`中。&#125; 需要注意的是：标记与内容之间必须空一行隔开。至于为何要这样，看到最后就明白了。 } Column@column-2{ @card{ 左与card标记类似，分栏的标记是这样的： 123456789101112131415@column-2&#123;@card&#123;# 左&#125;@card&#123;# 右&#125;&#125; 为了移动端观感，当屏幕宽度小于 480 时，column将换行显示。 } @card{ 右column中的每一列具有等宽、等高的特点，最多支持三栏： 123456789101112131415161718192021@column-3&#123;@card&#123;左&#125;@card&#123;中&#125;@card&#123;右&#125;&#125; } } Three columns@column-3{ @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } } Timeline@card{ 在timeline模块中，你的 5 号标题#####和六号标题######将被“征用”，用作时间线上的标记点： 123456789101112@timeline&#123;##### 2016@item&#123;###### 11月6日为 Card theme 添加 page layout。&#125;&#125; @item中多行内容可以换行输入，目前不允许隔行： 12345678910111213141516171819202122@timeline&#123;##### 2016@item&#123;###### 11月6日第一行 第二行 /* ok */&#125;@item&#123;###### 11月6日第一行第二行 /* error */&#125;&#125; } @timeline{ 2016@item{ 11月6日为 Card theme 添加 page layout。加快绿化空间好看 } @item{ 10月31日本地化多说。 } @item{ 10月24日为 Indigo 主题创建 Card 分支。 } 2015@item{ 2月24日发布 Indigo 主题到 hexo.io。 } @item{ 1月22日创建 Indigo 主题。 } } CodeBlock12345// 自定义内容块实现page.content.replace(/&lt;p&gt;&#125;&lt;\\/p&gt;/g, '&lt;/div&gt;') .replace(/&lt;p&gt;@([\\w-]+)&#123;&lt;\\/p&gt;/g, function(match, $1)&#123; return '&lt;div class=\"'+ $1 +'\"&gt;' &#125;) @card{ 这里可以解释，为什么标记之间必须要隔一行了。 当你在 Markdown 中隔行输入时，会形成新的段落，而如果一个段落中的内容仅仅是我们约定的标记，就可以用很容易的用正则匹配到替换为对应的模块容器。 } End@card{ 为了解决 Hexo 自定义页面slug为空不能很好的使用多说评论这个问题，现在已经给每个自定义页面自动生成了hexo-page-path这种格式的slug。本来准备用date做格式的最后一节，测试中发现 page 中的date值为修改时间，是动态的。综合考虑使用了路径path。 以后可以根据需要添加更多模块支持。 打赏和评论默认开启，可根据需要在 Markdown 头部定义是否关闭。 }"},{"title":"links","date":"2018-07-24T14:57:03.515Z","updated":"2018-07-24T14:57:03.515Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":"There some web links might be usefulMa Yi Lab @column-3{ @card{&gt;} @card{ right} @card{ right} }"},{"title":"","date":"2018-07-22T13:03:46.410Z","updated":"2018-07-22T13:03:46.410Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Image-Caption-2","slug":"Image-Caption-2","date":"2018-10-10T08:19:00.000Z","updated":"2018-10-10T08:25:52.914Z","comments":true,"path":"2018/10/10/Image-Caption-2/","link":"","permalink":"http://yoursite.com/2018/10/10/Image-Caption-2/","excerpt":"","text":"Image Caption：给定一\b张图片，生成和\b图片的文字描述 Image Retrieval：给定\b一个句子和一个图片的数据库，找到和句子最相关的那张图片 .","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"Image-Caption-1","slug":"Image-Caption-1","date":"2018-10-06T11:32:49.000Z","updated":"2018-10-08T06:18:52.160Z","comments":true,"path":"2018/10/06/Image-Caption-1/","link":"","permalink":"http://yoursite.com/2018/10/06/Image-Caption-1/","excerpt":"","text":"DataSetDataSet 分为 Single image generation 的 Image Caption DataSet Scale categories generations UIUC Pascal Sentence 1000 20 5 Flickr 8k 8000 - 5 Flickr 30k 8000 - 5 Microsoft CoCo 120000 91 - Abstract Scence Dataset 10000 - - Visual Genome Dataset - - - Krause - - - Kong - - - Video Caption DataSet Scale TACoS 127 YouCook 88 LSMDC 200 Referring Expression只介绍了两篇论文： Modeling Context in Referring Expressions Generation and Comprehension of Unambiguous Object Descriptions Measure首先回忆一下准确率和召回率 P = \\frac{TP}{TP+FP}R = \\frac{TP}{TP+FN}其中，P/N 是自己的判断，T/F 是真实的\b对于判断的衡量 BLEUn-gram based precision 首先 BLEU 是基于 n-gram 匹配的，也就是对于一个长为 N 的句子，\b有 $(N-n+1)$ 个 n-gram 组合，提取出这个句子的某个 n-gram 组合，然后计算这个 n-gram 的 precision，一个直观的方法就是计算 n-gram 在目标中也就是 groundtruth（NLP 里面我们称为 reference）是否有出现，然后统计所有的 n-gram 组合。 这个方法其实是有 bug 的。。。我们先来看一个 unigram（也就是 1-gram）的例子： Candidate（预测结果）：the the the the the the the.Reference 1: The cat is on the mat.Reference 2: There is a cat on the mat. 用最简单的方法，7 个 1-gram 都出现在了任意一个 reference \b中，于是正确率是 $100\\%$，但是，这是明显不合理的。。所以我们要把 n-gram 在 Candidate 中的出现次数也考虑进去，并当作分母。同时，取 1-gram 在所有 reference 中出现的最多的次数 $max(1,2) = 2$，之后 reference 中的统计和 candidate 中的统计取一个最小值作为分子 $min(2,7) = 2$，于是最后的正确率就是 $\\frac{2}{7}$。 下面给出，总的\b正确率公式，这个方法论文里面给出的公式真的不忍直视。。。。于是找了\b一种比较好的公式写法 BLEU_n(a,b) = \\frac{\\sum_{w_n \\in a} min \\left( c_a(w_n), \\mathop{max}_{j=1,...,|b| c_{b_j}(w_n)} \\right)}{\\sum_{w_n \\in a} c_a(w_n)} a：candidate sentenceb：所有的 reference sentence$w_n$：某个 n-gram 组合$c_x(y_n)$：在句子 x \b中统计 n-gram $y_n$ 出现的次数$BLEU_n$ 下面的 n 和 n-gram 中的\b n 对应BLEU 是 $BLEU_1$ 到 $BLEU_4$ 平均数 单单上面的公式还有是问题的，\b对于某个 candidate： Candidate（预测结果）：The cat is on the 这个句子明显是不完整的，但是依然\b可以计算出是满分。。所以我们要对于短的句子做一个惩罚 BP = \\left \\{ \\begin{array}{lL} 1 & \bif\\ l_c> l_r \\\\ e^{1-l_r/l_c }& \bif\\ l_c \\leq l_r \\end{array} \\right . $l_c$：是 candidate 的长度$l_r$：是 reference 中\b最短句子的长度\b 我们再对不同的 n-gram 加上一个权值，就是最终的评判公式了： BLEU = BP \\cdot exp \\left( \\sum_{n=1}^N w_n \\cdot log\\ BLEU_n \\right)缺点 对于 n-gram 的出现顺序没有约束 每一个 n-gram 都被相同\b对待，没有\b主次之分 Rougen-gram based Recall ROUGE_n(a,b) = \\frac{\\sum_{j=1}^{|b|} \\sum_{w_n \\in b_j} min\\left( c_a(w_n),c_{b_j}(w_n) \\right)}{\\sum_{j=1}^{|b|} \\sum_{w_n \\in b_j} c_{b_j}(w_n)} 分母是对于 reference 中所有的 n-gram 的统计分子是 candidate 中的 n-gram 与 reference 中的匹配 Rouge-LL 即是 LCS(longest common subsequence，最长公共子序列) 的首字母 \\begin{align} R_{lcs} &= \\frac{LCS(X,Y)}{m} \\notag \\\\ P_{lcs} &= \\frac{LCS(X,Y)}{n} \\notag \\\\ F_{lcs} &= \\frac{(1+\\beta^2)R_{lcs}P_{lcs}}{R_{lcs}+ \\beta^2 P_{lcs}} \\notag \\end{align}METEORMeteor也是来评测机器翻译的，对模型给出的译文与参考译文进行词对齐，计算词汇完全匹配、词干匹配和同义词匹配等各种情况的准确率、召回率和F值。 Metor = (1-Penalty)\\cdot F首先计算 unigram 情况下的准确率 P 和召回率 R（计算方式与BLEU、ROUGE类似），得到调和均值（F值） F = \\frac{10PR}{R+9P}Meteor的特别之处在于，它不希望生成很“碎”的译文：比如参考译文是“A B C D”，模型给出的译文是“B A D C”，虽然每个 unigram 都对应上了，但是会受到很严重的惩罚。惩罚因子的计算方式为 Penalty = \\frac{1}{2} \\left( \\frac{\\# chunks}{\\# matched\\ unigrams} \\right)^3式中的 $#chunks$ 表示匹配上的语块个数，如果模型生成的译文很碎的话，语块个数会非常多；$#unigrams matched$ 表示匹配上的 unigram 个数。所以最终的总的评分公式为： Meteor = \\mathop{max}_{j=1,...,|b|} \\left( \\frac{10PR}{R+9P} \\right) \\left( 1-\\frac{1}{2} \\left( \\frac{\\# chunks}{\\# matched\\ unigrams} \\right)^3 \\right)CIDEr\b这个指标将每个句子都看作“文档”，将其表示成 tf-idf 向量的形式，然后计算参考caption与模型生成的caption的余弦相似度 CIDEr_n(a,b) = \\frac{1}{|b|}\\sum_{j=1}^{|b|} \\frac{g^n(a)\\cdot g^n(b_j)}{||g^n(a)|| \\cdot ||g^n(b_j)||} 在这里，$g^n(x)$ 是句子 x 的所有 n-gram 的 TF-IDF scores 的向量化 最后的\b总的分数可以写成： CIDEr(a,b) = \\sum_{n=1}^N w_n CIDEr_n(a,b)SPICE上面讲到的这些\b衡量方式多数用于机器翻译的，而且依旧存在不足的地方，下图用传统的方式衡量，首先看左边的，其实是描述两张不同的图片，但是他们的描述用传统的方式相似度很大，结果是 FP，又比如右图，我们看到两个描述都是合理的，但是，由于相似度比较低，所以就没有判成正例。。 。。。。未完待续 Referencehttps://www.cnblogs.com/Determined22/p/6910277.html .","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"RL2-MDP","slug":"RL-2","date":"2018-10-06T08:01:06.000Z","updated":"2018-10-06T14:28:41.904Z","comments":true,"path":"2018/10/06/RL-2/","link":"","permalink":"http://yoursite.com/2018/10/06/RL-2/","excerpt":"","text":"Markov Process在 markov 决策过程中，环境是完全观测到的 基本上大多数的 RL 问题都是 MDP 的形式 Definition: markov process 是 $\\langle S,P \\rangle$ S 是一个有限的状态集合 P 是状态转移矩阵 Markov Property_”The Future is independent of the past given the present”_ P[S_{t+1}|S_t] = P[S_{t+1}|S_1,...,S_t]Markov ChainState Transition Matrix P_{ss^{\\prime}} = P[S_{t+1} = s^{\\prime} | S_t = s]P = \\left [ \\begin{array}{ccc} P_{11} &\\dots& P_{1n} \\\\ \\vdots & & \\vdots \\\\ P_{n1} & \\cdots & P_{nn} \\end{array} \\right ] 矩阵的每一行的和是 1 Markov Reward ProcessMarkov Reward Process is a Markov chain with values Definition: Markov Reward Process 是一个 $\\langle S,P,R,\\gamma \\rangle$ S 是一个有限的集合 P 是状态转移矩阵 R 是反馈方程 $R_s = E[R_{t+1} | S_t=s]$ $\\gamma$ 是\b衰减\b因子 Markov Decision ProcessExtensions.","categories":[],"tags":[{"name":"RL","slug":"RL","permalink":"http://yoursite.com/tags/RL/"}]},{"title":"R-CNN","slug":"RCNN","date":"2018-10-05T02:21:58.000Z","updated":"2018-10-12T04:48:24.715Z","comments":true,"path":"2018/10/05/RCNN/","link":"","permalink":"http://yoursite.com/2018/10/05/RCNN/","excerpt":"","text":"R-CNNR-CNN 流程 对输入图片使用 select search 选择 2000 个区域 select search 有区域合并的操作 拉伸缩放图片到固定尺寸（227 * 227）传入 CNN 生成 4096 维的特征向量 对于特征用 SVM 预测每个类别（C+1，1是背景），通过 IoU 来打分 预测最大的判别为一类 最后对这些区域根据极大值抑制来合并和省略 对于每一个类，都训练一个 SVM 分类器，优化的核心是建立一个 loss 方程，对于 SVM 的 loss function 我们要有 predict 的正负样本，以及 groundtruth 的正负样本 predict：对于每一个区域的对应的类 SVM 直接返回 0/1（负样本或者正样本） groundtruth：把所有的框和真实手工标注的框做一个 IoU 的计算，然后根据 IoU 的阈值来生成 data 的负样本与正样本，然后还要进行一次极大值抑制，尽管有些框的 IoU 大于阈值了，但是依旧判为负样本，因为它周围已经有比他更大的了。。 我们还可以对位置进行一个修正（用修正后的位置来计算 IoU）：输入为深度网络pool5层的4096维特征，输出为 xy 方向的缩放和平移。 训练样本判定为本类的候选框中，和真值重叠面积大于 0.6 的候选框，loss 为真实框和预测框的二范数。这个预测应该是在一次识别操作之后进行的。。 SPPnet R-CNN 对于每一个 region 图片都进行了卷积，这样效率很低，一个直观的 idea 就是直接做一次卷积生成一个 feature map，然后在这个 feature map 中找到对应的 region，但是这样有一个问题，就是最后我们的全连接层是固定尺寸的，在 R-CNN 中，我们通过 warp 区域来使输入一致，在 SPPnet 中，我们在 feature map 上面，是否也可以 warp 呢 ？答案是不可以的。。。因为在图像进行 warp，伸展以后图像的意义仍然保留了，但是特征图 warp 以后，不经过修正，就失去意义了。。。所以我们要对 feature 重新提取特征。 SPP 全名是 Spatial Pyramid Pooling，借用了图像金字塔的概念，如上图所示，我们将图像分成 $4\\times 4$, $2\\times 2$, $1\\times 1$ 的区域，然后做这些区域的 max-pooling，假设经过卷积\b我们得到的 feature map 是 $n\\times m\\times 256$ 的，经过 \bSPP 以后，就变成了 $21\\times 256$ 的了 SPP 还有要解决的点就是根据 region 在图像中的位置找到，对应在 feature map 中的位置。。计算公式是这样的：$(x, y) = (Sx^{\\prime}, Sy^{\\prime})$，其中 S 是所有步长的乘积，实际中的公式为： x^{\\prime} = \\lfloor x/S \\rfloor + 1 论文中用到的 ZF-5 模型，它的 S 就是 $2^4$ 也就是里面的四个 str 2 Fast R-CNNR-CNN 缓慢的原因是因为，对于每一张图片都进行来深度网络的卷积SPPnet 解决 R-CNN 的这个问题，但是 SPPnet 还是有缺陷的，就是和 R-CNN 一样仍然是 multi-stage 的框架 Fast R-CNN 的优点是： 更高的探测精度 在一个 stage 上通过不同任务的 loss 完成训练 训练可以更新网络的每一层 feature 的存储不需要放在硬盘上 在 fast R-CNN 中，每一个要训练 ROI 都有一个真实的类别 u（u=0 表示是背景） 和真实的目标区域 v于是 loss function 可以表示为\b两个 loss 之和： L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda[u\\geq 1]L_{loc}(t^u,v) p \b是 softmax 得出的预测结果 $u\\geq 1$ 说明背景的 loss 不计入考虑范围 faster FC by SVD位置修正和分类都是全连接层，\b一次前向传播可以\b由线性变换表示 $y=Wx$ 复杂度为 $u\\times v \\times v$ 现在我们对这个变换矩阵做一个\b\b奇异值分解，取前 t 大的 特征值保留下来复杂度变为 $(u\\times t + t\\times v) \\times v$ 训练的过程中应该不会用 SVD 分解。。 ROI Pooling BP这是这个文章的计算细节中的难点，我还没完全理解。。以后用到再说。。 Faster R-CNNFaster R-CNN \b将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，faster R-CNN 的整体结构如下所示： Bounding Box 回归系数 \b原始\b的边框表示为左上角点和长宽 $(O_x,O_y,O_w,O_h)$，目标边框表示为 $(T_x,T_y,T_w,T_h)$，regression 可以看作是一个框与\b框之间的移动过程，这个过程\b的参数如下表示，并且这样的表示对于图像的缩放是 robust 的 Faster R-CNN Procedure__ Anchor Generation Layer这一层为每个像素点生成 9 个不同大小，不同长宽比的框 Region Proposal LayerProposal Layer: 将每一个 anchor 做一个位置修正，同时生成\b生成这些 anchor 的概率，然后根据极大值抑制对这些 anchor 做一个筛选 Anchor Target Layer：和 gt 相比较，\b判断 anchor 是前景\b\b还是背景，如果是前景的话，计算出对应的框与它待修正偏移 Calculating RPN Loss：\bloss 等于框的 regression loss 加上 classification loss 这里面的分类是二分类，来判断是前景还是背景 Proposal Target Layer：proposal layer \b提供了初步筛选的框，这里再进行一次删选，这里的筛选是根据 gt 进行的，对每个标定的 ground true box 区域，与其重叠比例最大的 ancho r记为 正样本 (保证每个 ground true 至少对应一个正样本anchor) 对上一步剩余的 anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本（每个 ground true box可能会对应多个正样本 anchor。但每个正样本 anchor 只可能对应一个 grand true box）；如果其与任意一个标定的重叠比例都小于0.3，记为负样本 Classification Layer 上面 softmax 出现了两个分支，其中一个是要加入 loss function 的另一个是为了找到 target bounding-box 的。。 Training Method源代码里用了一种叫做4-Step Alternating Training的方法 用ImageNet模型初始化，独立训练一个RPN网络 仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享 使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的learning rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层 仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个unified network，继续训练，fine tune Fast-RCNN 特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测proposal并实现检测的功能 Mask R-CNN \b\b再加一个输出，用来做 \bmask 的 loss，并且把这个 loss 加入到总的 loss 里面 ROI 和 GT 的 IOU 大于 0.5 就认为是 positive loss_mask 只在 positive 的样例上进行定义 Mask RCNN 相对于 faster RCNN 的一个很大的改进就是 ROI pooling 变成了 ROI align 我们来看一下 ROI pooling 的问题是什么，下面是一个 ROI pooling 的过程 ROI 在原图的区域对应到了 feature map 中的区域，为此，我们要将原图的 ROI 缩减 S 倍，这里 S 是 32，然后对于感知域，要进行 ROI pooling，要得出 pooling 的 subregion，我们要，除以 pooling 的变长，也就是 7，但是这里面的除法结果是要取整的，取完整以后，我们发现一些区域被边缘化了。。。 ROI align 保留了所有的浮点计算结果，对于完整的 ROI 进行均匀的采样 ROI pooling 的 BP 求导公式为： \\frac{\\partial L}{\\partial \bx_i} = \\sum_r \\sum_j [i = i^*(r,j)] \\frac{\\partial L}{\\partial y_{rj}} $y_{rj}$ 是最后的输出也就是 pooling 得到的最大像素值$x_i^*(r,j)$ 表示 $y_{rj}$ 的来源$r$ 表示的是每个做 pooling \b的区域$j$ 是 $r$ 中选择的先像素点 ROI align 是先进行一次双线性插值（bilinear interpolation）也就是，红色区域最近的四个点通过距离加权得到一个值，得到一个中间层，然后对这个中间层进行 max pooling. ROI align 的 BP 求导公式为： \\frac{\\partial L}{\\partial \bx_i} = \\sum_r \\sum_j [d(i, i^*(r,j)) < 1]\\cdot (1-\\Delta h )(1-\\Delta w)\\frac{\\partial L}{\\partial y_{rj}} $d()$ 操作是计算两点的坐标距离的$\\Delta h$ 和 $\\Delta w$ 表示 $x_i$ 与 $x_i^*(r,j)$ 横纵坐标的差值 FCN FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类 Results Backbone architecture 指的是最前面的特征提取的部分 Referencehttps://zhuanlan.zhihu.com/p/31426458https://zhuanlan.zhihu.com/p/23006190https://blog.csdn.net/xjz18298268521/article/details/52681966http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/https://ardianumam.wordpress.com/https://blog.csdn.net/WZZ18191171661/article/details/79453780http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"http://yoursite.com/tags/DL/"}]},{"title":"RL1-introduction","slug":"RL-1","date":"2018-10-04T08:51:19.000Z","updated":"2018-10-06T07:57:27.365Z","comments":true,"path":"2018/10/04/RL-1/","link":"","permalink":"http://yoursite.com/2018/10/04/RL-1/","excerpt":"","text":"RL 和传统的监督学习和无监督学习是有区别的，RL \b没有 label\b 只有 reward 数据是有时序性的，也就是数据之间并不是独立的 数据只会对它后面的数据造成影响 比如一个棋局只会对后面的落子有影响，当前位置只会对后面的位置移动产生作用 Reward一个 Reward $R_t$ \b就是一个常数的信号，用来对状态的衡量，或者 agent 在时间 $t$ 做的好坏程度，agent 的目标就是最大化 reward，RL 的基础就是建立在最大化 reward 的假设 下面是一些 reward 在特定任务下的例子： State History：可以由上面介绍的组成一个序列 $H_t= O_1,R_1,A_1,…,A_{t-1},O_t,R_t$ State： 其实就是信息，决定着接下来会发生的信息是什么，State 只和 history 有关 $S_t = f(H_t)$ Environment State：隐藏在能给出反馈的环境中，一般是看不到的，看到也没用。。 Agent State：决策者的状态 $S_t^a = f(H_t)$ Information State：只包含历史中有用信息，例如假设了 Markov 性，inf state 只有之前的状态\b Fully Observation Environments：$O_t = S_t^a = S_t^e$ Partially Observation Environments：间接的观察环境 \bAgentRL 的 agent 一般有如下几个组件构成 Policy：agent 的行为函数 Value function：评价 state \b或者 action 的好坏 Model：agent 对于 Environment 的表示 Policypolicy 是 state 到 action 的映射，是 agent 的行为 Deterministic Policy：$a = \\pi(s)$ Stochastic policy：$\\pi(a|s) = P[A_t = a | S_t = s]$ Value Function对于未来的预测 v_{pi}(s) = E_{\\pi}[R_{t+1} + \\gamma R_{t+1} + \\gamma^2 R_{t+3}+...|S_t=s] Modelmodel 是用来预测 environment 接下来会做什么 $\\mathcal{P}$ 用来预测接下来的\b的 state $\\mathcal{R}$ 用来预测接下来的 reward \\mathcal{P}_{ss^{\\prime}} = P[S_{t+1} = S^{\\prime}|S_t = s, A_t = a]\\mathcal{R}_s^a = E[R_{t+1}|S_t = s, A_t = a] .","categories":[],"tags":[{"name":"RL","slug":"RL","permalink":"http://yoursite.com/tags/RL/"}]},{"title":"NLP-2","slug":"NLP-2","date":"2018-09-25T11:26:50.000Z","updated":"2018-09-25T14:10:27.580Z","comments":true,"path":"2018/09/25/NLP-2/","link":"","permalink":"http://yoursite.com/2018/09/25/NLP-2/","excerpt":"","text":"Machine TranslationStatistical Machine Translation早期的机器翻译的方法是 SMT，S 代表 statistic 表示统计的意思，然后 14 年的时候，一切都变了。。。 Neural Machine TranslationNMT 降临了，吊打了 SMT，所以 SMT 我就不学啦，哈哈哈 Seq-to-Seq model 上图称为 sequence-to-sequence 模型，有两个 RNN 待翻译的句子为输入 $x$，翻译的结果称为 $y$ 第一个 RNN 用来将 $x$ \b做一个 encoding，类似于将输入生成一个特征 生成这个特征的结果\b\b放到下一个 RNN 中做 Decodeing Decoding 的过程相当于做一连串的单词预测 这是 seq2seq \b模型的训练过程 Beam search decoding 对于每一个单词后面一个单词的\b预测，最简单的方法就是上面的贪心法 (greedy decoding), 每一步直接输出\b概率最高的，但是其实这并不是一定我们期望的最优解，甚至离最优解的很远，一组输出\b的概率表示形式如下： P(y|x) = P(y_1|x) P(y_2|y_1,x) P(y_3|y_2,y_1,x),...,P(y_T|y_{T-1},...,y_1,x)我们希望输出的\b值概率尽可能的大，要想获得最大值，\b最粗暴有效的方法就是遍历上面的所有取值，但是上面的所有取值的复杂度是 $O(V^T)$，于是我们可以做一个在 greedy 和 optimal 之间做一个折衷，这个方法就叫 Beam Search。\b \bBeam search: On each step of decoder, keep track of the k most probable partial translations 举个 $k=2$ 的例子 每个单词我们生成它最大概率的 2 个，然后每次选取概率最大的两个分支继续进行生成 .","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"PGM-CRF","slug":"PGM-CRF","date":"2018-09-22T02:25:18.000Z","updated":"2018-10-10T07:18:46.477Z","comments":true,"path":"2018/09/22/PGM-CRF/","link":"","permalink":"http://yoursite.com/2018/09/22/PGM-CRF/","excerpt":"","text":"CRF 可以看作是 logistics 回归的一个扩展 ModelingNotions $\\mathbf{X}$: 我们观测到的输入变量 $\\mathbf{Y}$: 我们待预测的输出变量 $\\sum_{\\mathbf{y} \\setminus y_s}$: 在$y_s$ 给定的情况下，y 中所有其他变量的可能的取值的遍历 G &amp; DClassificationClassification 是分类问题，通过给定的特征向量 $\\mathbf{x}$ 我们来估计隐藏在这些数据后面的类别 $y$ 是什么，其中一个简单的方法是假设特征向量里面的变量都是相互独立的，这称为 _naive Bayes classifier_ 这是基于 x 的联合分布： p(y|\\mathbf{x}) = p(y) \\prod_{k=1}^K p(x_k | y)上面的公式也是可以写成因子图的表示形式的 另一个比较常见的模型就是 logistic regression： p(y|\\mathbf{x}) = \\frac{1}{Z(\\mathbf{x})} exp \\left\\{ \\sum_{k=1}^K \\theta_k f_k (y, \\mathbf{x}) \\right\\}Sequence ModelsClassifier 只能对于单变量做预测，我们希望能对多变量做预测，为了引出这个模型，我们来讨论一个 NLP 的应用，称为 NER，NER 是用来定义一个词的类别的，比如，china，它的 NER 就是 location，更加具体一些，给定一个句子，我们确定哪些词是组合在一起的，同时对于这些词做一个区分。 一个直观的 NER 方法是对于单词做独立的区分，这种方法每个单词和它周围的单词是独立的，比如 new york 是一个地名，但是 new york times 它就是一个报纸了。。。一个方法就是把这些输出变量都串起来，形成一个链式的模型，称为 HMM，HMM 有两个独立性假设 每一个 state $y_t$ 只和前一个 state $y_{t-2}$ 相关 观测 $x_t$ 由 $y_t$ 导出 于是，state y 和 观测 x 的联合分布可以写成 p(\\mathbf{y},\\mathbf{x}) = p(y_1) \\prod_{t=2}^ T p(y_t|y_{t-1}) p(x_t|y_t)ComparisionGenerative Model Naive Bayes HMM Discriminative Model logistic regression 我们还回忆一下生成模型和判别模型的区别，生成模型中概率的一部分是分给 x 的，所以生成模型最后的联合分布的概率通常很小，就比如 naive bayes，对于判别模型，概率是 0-1 之间的条件，这就符合了逻辑回归呀，在条件概率中，x 只对 y 的概率有影响，但是完全不出现在概率中。之所以不对 $p(x)$ 建模，是因为输入的特征变量之间的相关性太高 Linear Chain CRFs我们重写 HMM 模型到一个更加 general 的 case： p(\\mathbf{y}, \\mathbf{x}) = \\frac{1}{Z} \\prod_{t=1}^T exp \\left \\{ \\sum_{i,j \\in S} 1_{\\{y_t=i\\}} 1_{\\{y_{t-1}=j\\}} + \\sum_{i \\in S} \\sum_{o \\in O} \\mu_{oi} 1_{\\{y_t = i\\}} 1_{\\{x_t = o\\}} \\right \\}当 $\\theta_{ij} = log p(y^{\\prime} = i | y = j)$ 时，$\\mu_{oi} = log p(x=o|y=i)$ 时，就是 HMM 了，写成更一般的简洁一些的形式，即把指数的参数直接写成 feature function 为 $f_k(y_t,y_{t-1},x_t)$ 的形式： p(\\mathbf{y}, \\mathbf{x}) = \\frac{1}{Z} \\prod_{t=1}^T exp \\left\\{ \\sum_{k=1}^K \\theta_k f_k (y_t, y_{t-1},x_t) \\right\\}上面是生成模型，根据 bayes 定理我们把它写成条件概率的判别模型： p(\\mathbf{y}| \\mathbf{x}) = \\frac{p(\\mathbf{y}, \\mathbf{x})}{\\sum_{\\mathbf{y^{\\prime}}}} = \\frac{\\prod_{t=1}^T exp \\left\\{ \\sum_{k=1}^K \\theta_k f_k(y_t,y_{t-1},x_t) \\right\\}}{\\sum_{\\mathbf{y^{\\prime}}} \\prod_{t=1}^T exp \\left \\{ \\sum_{k=1}^K \\theta_k f_k(y_t^{\\prime}, y_{t-1}^{\\prime},x_t) \\right \\} }上面的条件概率就是 linear-chain CRF， Definition：linear-chain CRF p(\\mathbf{y}| \\mathbf{x}) = \\frac{1}{Z(\\mathbf{x})} \\prod_{t=1}^T exp \\left\\{ \\sum_{k=1}^K \\theta_k f_k(y_t,y_{t-1},\\mathbf{x}_t) \\right\\} $Y,X$ 是随机变量 $\\theta$ 是参数向量，总共有 K 个变量 $f_k(y,y^{\\prime},x_t)$ 是 feature function General CRFs把 linear-chain CRF 中的 linear-chain factor \b用 more general 的 factor 表示就是 General CRFs p(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{Z(\\mathbf{x})} \\prod_{\\Psi_A \\in G} exp \\left\\{ \\sum_{k=1}^{K(A)} \\theta_{ak} f_{ak}(\\mathbf{y}_a,\\mathbf{x}_a) \\right\\}General form 的参数似乎更多，对于 linear-chain，相同的 weight 作用于每一个时间轴上的 $\\Psi_t(y_t,y_{t-1},x_t)$，对于 General 的形式，我们将 G 的 factors 分成 $C= \\{ C_1,C_2,…,C_p \\}$ \b每一个 clique $C_p$ 对应这一个 suffcient statistics $\\{ f_{pk}(\\mathbf{x}_p, \\mathbf{y}_p) \\}$ 以及参数 $\\theta_p \\in R^{K(p)}$，于是 CRF 可以写成： p(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{Z(\\mathbf{x})} \\prod_{C_p\\in C} \\prod_{\\Psi_c \\in C_p} \\Psi (\\mathbf{x}_c,\\mathbf{y}_c; \\theta_p)\\Psi (\\mathbf{x}_c,\\mathbf{y}_c; \\theta_p) = exp \\left\\{ \\sum_{k=1}^{K(p)} \\theta_{pk} f_{pk} (\\mathbf{x}_c,\\mathbf{y}_c) \\right\\}Inference。","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Information-Theory","slug":"Information-Theory","date":"2018-09-18T02:06:08.000Z","updated":"2018-10-10T07:18:01.436Z","comments":true,"path":"2018/09/18/Information-Theory/","link":"","permalink":"http://yoursite.com/2018/09/18/Information-Theory/","excerpt":"","text":"信息熵：编码方案完美时，最短平均编码长度 交叉熵：编码方案不一定完美时，平均编码长度 相对熵：不同方案之间的差异性的衡量 信息论引用了熵（entropy）的概念，熵的概念在物理中十分常见，是用来衡量系统的混沌程度的，相应的在信息科学中，熵是用来衡量不确定性的，信息熵的本质是香农信息量 $log \\frac{1}{p}$ 的期望 下面我们就来讲讲这三个熵到底有什么含义 首先我们通过一个例子来引出什么是信息熵：假设一个箱子里面有4种不同颜色的球（a,b,c,d），A 从其中拿出一个，B 来猜（B 可以向 A 提问），B希望提问的次数越少越好。 case1：B 不知道颜色的分布，于是先问 “是否是 a,b”，如果是再问 “是否是 a”，不是改为问 “是否是 c”，猜球的次数为：$H=\\frac{1}{4}\\times 2+\\frac{1}{4}\\times 2+\\frac{1}{4}\\times 2+\\frac{1}{4}\\times 2=2$ case2：B 了解到 a 的比例为 1/2，b 的比例为 1/4，c 和 d 的比例都是 1/8，于是 B 先问 a，再问 b，最后问 c，猜球期望为：$H=\\frac{1}{2}\\times 1+\\frac{1}{4}\\times 2+\\frac{1}{8}\\times 3+\\frac{1}{8}\\times 3=1.75$ case3：B 了解到了里面的球全是 a，那么不用猜了：$H=1\\times 0=0$ _假设上面的 B 是足够聪明的，所以他做出策略是最优的_ 信息熵通过上面我们发现：针对特定概率为 $p$ 的小球，猜球的次数为 $log_2 \\frac{1}{p}$，所以整体的期望为： \\sum_{i=1}^N p_k log_2 \\frac{1}{p_k}这就是信息熵，case1 的信息熵为 2，case2 的信息熵为 1.75，case3 的信息熵为 0. 信息熵代表随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。上面的熵 case1&gt;case2&gt;case3，在 case1 中，B 对于系统一无所知，在 case2 中，B 知道了系统的分布，但是取了哪个球并不知道，case3 中，B 对于系统完全了解了！所以2是这个系统熵的最大值 交叉熵每个系统都会有一个真实的概率分布，称为真实分布，case1 的真实分布为 $(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4})$，case2 的真实分布为 $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{8})$,而根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的 [最小努力]（猜题次数、编码长度等）的大小就是信息熵。具体来讲，case1 只需要猜两次就能确定任何一个小球的颜色，case2 只需要猜测1.75次就能确定任何一个小球的颜色 回到 case2 假设现在是 C 来猜，C 的智商不高，所以仍然使用了 case1 的策略来，相当于认为小球出现的概率是一样的，即分布为 $(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4})$，这个分布就是非真实分布，最后的信息熵算出来又是 2 了，显然这个策略是不好的比最优策略多了 0.25，那么，当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？ 这就需要引入 交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。 交叉熵的公式如下，其中 $p_k$ 表示真实分布，$q_k$ 表示非真实分布 \\sum_{i=1}^N p_k log_2 \\frac{1}{q_k}上面所讲将策略 1 用于 case 2，真实分布 $p_k=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{8})$，非真实分布 $q_k=(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4})$，交叉熵为 $\\frac{1}{2}\\times log_2 4+\\frac{1}{4}\\times log_2 4+\\frac{1}{8}\\times log_2 4+\\frac{1}{8}\\times log_2 4=2$ 比最优分布的 $1.75$ 大 因此，交叉熵越低，策略就越好，最低的也就是真实的信息熵了，此时 $p_k=q_k$，这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布 相对熵我们如何衡量不同策略之间的差异呢？这里就需要用到相对熵（KL divergence），其用来衡量两个取值为正的函数或者概率分布之间的差异（可以发现相同的话每一项都是 0）： KL(P||Q) = \\sum_x p(x) log \\frac{p(x)}{q(x)} $p(x)$ 是真实分布 $q(x)$ 是近似分布 相对熵 可以写成 交叉熵 和 信息熵 之差。。 KL(p||q) = H(p,q) - H(p) = \\sum_{k=1}^N p_k log_2 \\frac{p_k}{q_k} Referencehttps://www.zhihu.com/question/41252833answer/195901726","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"MatplotLib-1","slug":"MatplotLib-1","date":"2018-09-16T08:14:42.000Z","updated":"2018-09-16T08:14:42.834Z","comments":true,"path":"2018/09/16/MatplotLib-1/","link":"","permalink":"http://yoursite.com/2018/09/16/MatplotLib-1/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"NLP-","slug":"NLP-1","date":"2018-09-14T12:01:35.000Z","updated":"2018-09-25T07:02:56.473Z","comments":true,"path":"2018/09/14/NLP-1/","link":"","permalink":"http://yoursite.com/2018/09/14/NLP-1/","excerpt":"","text":"Language Model语言模型（Language Model） 语言模型是用来预测下一个出现的单词会是哪一个的，说的数学一些就是给定一个单词序列 $x^1,x^2,…,x^t$，来计算下一个单词 $x^{t+1}$ 的概率分布： P(x^{t+1}=w_j|x^1,x^2,...,x^t) 其中 $w_j$ 是单词 j 的词向量 我们将任务做一个简化，我们不去考虑离下一个词太远的词的影响，只考虑下一个词前面的 $n-1$ 个，这称为 n-gram Language Model 于是概率可以写成\b下面的形式了： \\begin{align} P(x^{t+1} | x^t,...,x^{t-n+2}) &= \\frac{P(x^{t+1},x^t,...,x^{t-n+2})}{P(x^t,...,x^{t-n+2})} \\notag \\\\ &\\approx \\frac{count(x^{t+1},x^t,...,x^{t-n+2})}{count(x^t,...,x^{t-n+2})} \\end{align} 第一行通过 Bayesian 概率转换 第二行通过统计量来近似概率 假如 n 过大，会导致表格的内存呈指数级上升，所以一般不会超过 5 fixed-window Neural Language Model我们先看一种基于窗口\b的神经网络的结构 优点 不存在 n-grim 的稀疏性 模型的 size 大大减小 缺点 窗口尺寸太小 每一个 $x^i$ 只使用了 $W$ 一行的参数，并没有共享参数 Recurrent Neural Networks (RNN) Language Model我们先来看一看常见的一种 RNN 的结构： RNN 的核心 Idea 就是复用权重 $W$ 下面是一个用于 Language Model 的 RNN 模型： RNN 的优点： 由于序列之间相连的传递是共享了参数的，所以这个序列可以任意的延长 模型的大小不会随着输入的增加而增加 某一步的计算，会考虑之前几步计算的结果 RNN 的缺点： 计算速度比较慢 很难考虑到之前好几步的信息 存在梯度\b消失 有了模型，接下来介绍这个模型是如何进行优化的，对于这个模型有一点\b要说明的就是，\b输出 $\\hat{y}^t$ 其实是在给定\b单词到 $x^t$ 时，下一个单词 $x^{t+1}$ 出现的概率的预测，然后\b交叉熵来代替\b目标优化函数： \\begin{align} J^t(\\theta) &= CE(y^t,\\hat{y}^t)= - \\sum_{j=1} ^ {|V|} y_j^t \\mathop{log} \\hat{y}_j^t \\notag \\\\ J(\\theta) &= \\frac{1}{T} \\sum_{t=1}^T J^t(\\theta) \\end{align} 上面的真实 label $y^t$ 是根据 $x^{t+1}$ 生成的 one-hot vector 这个模型的 BP 是 $\\frac{\\partial J^t}{\\partial W_h} \\sum_{i=1}^t \\frac{\\partial J^t}{\\partial W_h}|_i$ 训练所有的数据是庞大且消耗巨大的任务，我们一般会把使用 SGD 也就是每一次拿一个句子进去做梯度下降 衡量某个模型的好坏的函数是 ： PP = \\prod_{t=1}^T \\left( \\frac{1}{\\sum_{j=1}^{|V|} y_j^t \\hat{y}_j^t} \\right)^{1/T} 上面的计算结果越小越好 RNN Gradient多维变量的链式求导法则： \\frac{d}{dt} f(x(t), y(t)) = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t}RNN 的基本公式可以写成： \\begin{align} h_t &= W f(h_{t-1}) + W^{hx} x_t \\notag \\\\ \\hat{y}_t &= W^S f(h_t) \\notag \\end{align} 根据上面这张图，RNN 的梯度为： \\begin{align} \\frac{\\partial E}{\\partial W} &= \\sum_{t=1}^T \\frac{\\partial E_t}{\\partial W} \\notag \\\\ \\frac{\\partial E_t}{\\partial W} &= \\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W} \\notag \\\\ \\frac{\\partial h_t}{\\partial h_k} &= \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} \\end{align} $\\frac{\\partial h_t}{\\partial h_k}$ 这一部分可能造成梯度消失 解决 RNN 梯度消失的方法clipping trick： Initialization + Relus： 初始矩阵 $W$ 为单位矩阵 函数 $f$ 变为 $f(z) = max(z, 0)$ 这些只是一些小的 trick 要从根本上解决，就需要建立新的模型 GRUS LSTM 决定哪些信息我们要从 cell state 中删除 上面黄色的区域是有参数需要学习的 \blayer，粉色的是直接进行的计算操作，计算操作有相乘还有相加等等。。 LSTM Structure 上面这条线贯穿着整个结构的称为 cell state，传递着最主要的信息 LSTM 有能力控制这个信息传输的增和删 确定了哪些信息我们要从 cell state 中删去 黄色的是 sigmoid 函数，输出在 0-1 之间 状态 1 表示完全保留\b cell state $C_{i-1}$ 状态 0 表示完全抛弃 cell state $C_{i-1}$ \b这一步决定了我们要保存哪些数据 sigmoid 函数决定哪些数据我们要\b更新 tanh 层生成了一个新的\b向量 $\\hat{C}_t$ 待用于生成新的 $C_t$ 将删选后的信息 $f_t * C_{t-1}$ \b和新生成的信息 $\\hat{C}_t$ 做一个相加 最后我们决定信息的输出 上面 sigmoid 节点的输出 $o_t$ 和输入 $x_t$ 以及之前输出 $h_{t-1}$ 有关 最终的输出 $h_t$ 和 $o_t$ 以及 cell state $C_t$ 相关 $h_t$ 有两个去处，一个是直接输出，还有一个是做为\b $h_{t+1}$ 的生成信息 Reference https://colah.github.io/posts/2015-08-Understanding-LSTMs/ .","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"PGM-Approximate Inference","slug":"PGM-4","date":"2018-09-11T00:50:05.000Z","updated":"2018-10-10T07:18:43.102Z","comments":true,"path":"2018/09/11/PGM-4/","link":"","permalink":"http://yoursite.com/2018/09/11/PGM-4/","excerpt":"","text":"概率的推断就是计算 conditional 和 marginal，之前我们学习了 exact inference 也就是准确的推断概率图概率，我们学习message passing 算法、sum-product、 inference \bis answer a query Approximate inference 就是对于 inference 的一个数值估计，不一定最后的结果要在 0-1 之内 Exact Inference RevisitSum-Product Factor Graph Junction Tree 在 Junction Tree 中 local Consistency 等价于 global Consistency Loopy Belief PropagationJunction Tree 虽然可以处理所有的 graph，但是只适用于树形结构，在密集结构中用 Junction Tree 复杂度依然会比较高，假设树变成了 gird，我们不打算用 Junction Tree 算法来计算出这个图的 inference 的精确解。\b LBP : The Algorithm我们在这个图上运行直接做 Belief Propagation，在树形结构上，Belief Propagation 只要来回传递两次就能够得到精确解了，但是在这个图上，\b我们需要多次的运行 BP ，最终可能会收敛，也有可能会呈现出周期性的数值变化，也就是不收敛。 一般来讲好的 近似可以通过以下的方式 在固定的迭代次数后停止 如果结果没有明显变化，就停止 如果在数值上没有震荡，并且收敛了，那么通常就是接近真实了 LBP : The Bethe Approximation我们对于 LBP 算法的正确性做一个分析： 一般来讲，真实的分布 P 是这样子的： P(X) = \\frac{1}{Z} \\prod_{f_a \\in F} f_a(X_a)但是这种分布的计算很困难（$f_a 是 factor graph 的\b算子$）。。 于是我们转向另一个分布 Q ，在后面，Q 是我们近似得到的分布，我们希望来评价 Q，也就是 Q 和 P 的相似度，对于某一事件不同概率的衡量，最常用的就是相对熵 KL Divergence \b： KL(Q\\Vert P) = \\sum_X Q(X) log(\\frac{Q(X)}{P(X)})相对熵是来衡量两个取值为正的函数或者概率分布之间的差异的，有以下的特性： $KL(Q\\Vert P) \\geq 0$ $KL(Q\\Vert P) = 0$ iff $Q=P$ 相对熵还可以写成 信息熵 减去 交叉熵： \\begin{align} KL(Q||P) &= \\sum_X Q(X)log Q(X) - \\sum_X Q(X) log P(X) \\\\ &= -H_Q(X) -E_Q logP(X) \\end{align}我们把真实分布带入到 $P(X)$ 中，可以得到： \\begin{align} KL(Q||P) &= -H_Q(X) - E_Q log (\\frac{1}{Z} \\prod_{f_a \\in F} f_a(X_a)) \\\\ &= -H_Q(X) - E_Q\\sum_{f_a \\in F}log f_a(X_a) + E_Q log Z \\end{align}我们定义一下 free-energy 为前两项： F(P,Q) = -H_Q(X) - \\sum_{f_a \\in F} E_Q log f_a(X_a)对于 Energy Functional： $\\sum_{f_a \\in F} E_Q log f_a(X_a)$ 的计算比较方便。。。 $H_Q$ 的计算会比较复杂，因为我们需要遍历所有 $X$ 的取值再做计算- 树形结构的 Energy Functional Tree 是有 closed-forms，也就是说某些 energy functional 是好可以计算的 当因子图树一颗树的时候 Bathe Approximation 和 Gibbs free energy 是等价的 我们用一张图来说明一下推导的流程 首先我们将原图转化成因子图，可以得出图的分布的表示 我们假设一种分布和原分布进行比较，得出和原分布之间的 energy function 我们定义这种近似分布为 bathe approximation，这个分布只和 $b_a$ 和 $b_i$ 相关\b 我们对 bathe 的 energy function 进行优化，从而得到 $b_a$ 和 $b_i$ 的更新值 $b_a$ 和 $b_i$ 优化的方式引入到图里面就是在做 BP ！ 用 Bathe 来近似原分布相当于在图上做 BP 每一种假设的近似分布都对应这一种更新\b策略我们先来考虑图 (a) \b树形的结构，树形结构的概率图都可以转换成树形结构的因子图，树形结构的因子图的概率可以写成 b(\\mathbf{x}) = \\prod_a b_a(\\mathbf{x}_a) \\prod_i b_i(x_i)^{1-d_i} $d_i$: degree of point i $b_a$: doubleton (pairwise) factor $b_i$: singleton factor 我们会对图 \b(b) 也使用 $b(\\mathbf{x})$ 来近似计算概率，这种近似称为 bethe approximation，\b若 bethe 近似和 gibbs 分布完全相等，\b当且仅当\b因子图是树形的 \b下面，我们来求这两个图的 free energy 并进行优化，来求解近似分布 b 从而得到 b 的更新策略，\b而后要用来从数值优化的形式转化到结构上 a H_{tree} = -\\sum_a \\sum_{x_a} b_a(x_a) log b_a(x_a) + \\sum_i (d_i-1) \\sum_{x_i} b_i (x_i) log b_a(x_i) \\begin{align}F_{tree} &= \\sum_a \\sum_{x_a} b_a(x_a) log \\left( \\frac{b_a(x_a)}{f_a(x_a)} \\right) + \\sum_i (1-d_i) \\sum_{x_i} b_i(x_i) log b_i (x_i) \\notag \\\\ &= F_{12} + F_{23} + ... + F_{67} + F_{78} - F_1 - F_5 - F_2 - F_6 - F_3 - F_7 \\end{align}$H_{tree}$ 是分布 $b(\\mathbf{x})$ 的信息熵，H 的形式应该是根据连续变量的信息熵公式得出的 b H_{Bethe} = -\\sum_a \\sum_{x_a} b_a(x_a) log b_a(x_a) + \\sum_i (d_i-1) \\sum_{x_i} b_i (x_i) log b_a(x_i) \\begin{align}F_{tree} &= \\sum_a \\sum_{x_a} b_a(x_a) log \\left( \\frac{b_a(x_a)}{f_a(x_a)} \\right) + \\sum_i (1-d_i) \\sum_{x_i} b_i(x_i) log b_i (x_i) \\notag \\\\ &= F_{12} + F_{23} + ... + F_{67} + F_{78} - F_1 - F_5 - 2F_2 - 2F_6 - ... - F_8 \\end{align}free energy 公式\b想要进行优化，还\b要有一些约束： $\\sum_{\\mathbf{x}_a} b_a(\\mathbf{x}_a) = 1$ $\\sum_i b_i(x_i) = 1$ $\\sum_{\\mathbf{x}_a \\setminus x_i} b_a(\\mathbf{x}_a) = b_i(x_i)$ \b最终的\b优化目标函数\b变为了: L = F_{Bethe} + \\sum_i \\gamma_i \\left \\{ 1 - \\sum_{x_i} b_i (x_i) \\right \\} + \\sum_a \\sum_{i\\in N(a)} \\sum_{x_i} \\lambda_{ai} (x_i) \\left\\{ b_i(x_i) - \\sum_{\\mathbf{x}_a \\setminus x_i} b_a(\\mathbf{x}_a) \\right\\}对目标函数求导得到上面的 $\\lambda_{ai}$ 可以替换成： \\lambda_{ai} = log (m_{i\\rightarrow a} (x_i)) = log \\prod _{b\\in N(i) \\setminus a} m_{b\\rightarrow i} (x_i).","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"NLP-Basis","slug":"NLP-0","date":"2018-09-09T07:50:49.000Z","updated":"2018-09-21T14:49:38.320Z","comments":true,"path":"2018/09/09/NLP-0/","link":"","permalink":"http://yoursite.com/2018/09/09/NLP-0/","excerpt":"","text":"Word VectorWord Vector 也就是词向量可以分为两种 Count Based：这种方法是通过统计完全局的信息最后来做特征提取 Direct Prediction：这种方法只选取来局部的统计，但是能够直接进行计算 有一种方法能够统一上面两种性质称为 GloVe 那么我们来介绍一下几种常见的方法 SVD CBOW SkipGram GloVe 词向量也就是把单词转换成为向量的表示，这样方便计算机进行计算运算 首先，英语单词的数量很多有将近 13m，同时我们定义所有的单词集合为 $V$，以及单词的数量为 $|V|$ 最简单的词向量是 one-hot Vector 在讲词向量\b时，我们先来引出共现矩阵 co-occurrence matrix 的概念，我们假设共现矩阵为 X ，其元素 $X_{i,j}$ 表示单词 i 和单词 j 在同一个窗口一起出现的次数的统计，这里的统计是对于某个数据库下的统计。共现矩阵在很多的算法中都会出现，为了让共现矩阵更加完善，我们会作出一下修改和限制，比如，我们会抑制共现矩阵中出现的较大元素让他们\b $\\leq 100$ ，比如一些常见的单词 ‘the’ ‘he’ 等等造成的统计\b\b使用 ramp window \b也就是共现矩阵中的元素更新不再是 +1 而是根据离中心词的距离加权考虑 SVD Method直接对于\b共现矩阵\b做 SVD 分解，分解后选取\b前 k 大的特征值对应的特征向量来作为词向量 这个方法其实是有很多缺点的 矩阵是稀疏的，因为很多次是不会一起出现的 矩阵的维度很大，所以做 SVD 很花费时间 CBOW （Continuous Bag of words Models）CBOW 其实就是计算以某个单词为中心，固定一个窗口，计算周围单词出现的概率乘积，然后对于这个乘积就是这个事件的概率，与此同时在训练的过程中，我们 首先我们\b来看看这个问题的几个参数 $w_i$ : 在字典 $|V|$ 中的单词 i $V (n\\times |V|)$ : context 词向量矩阵 $U (n\\times |V|)$ : center \b词向量矩阵 $v_i (n\\times 1)$ : V 矩阵的某一列，也就是 $w_i$ 的上下文词向量 $u_i (n\\times 1)$ : U 矩阵的某一列，也就是 $w_i$ 的中心词向量 $m$ : 窗口的大小 CBOW Algorithm 对于 context 单词，我们生成 2m 个的 one-hot 向量 $[ x^{(c-m)},…,x^{(c-1)},x^{(c+1)},x^{(c+m)} ]$ 用 context 词向量矩阵乘以 one-hot vector 从而得到 context 单词对应的 context 词向量 $v_i = Vx^i$ 将这些得到的 context 词向量取均值得到 $\\hat{v} = \\frac{v_{c-m}+…+v_{c+m}}{2m}$ 用 center \b词向量矩阵去乘以\b\b上面得到的 context 均值词向量矩阵得到：所有单词以中心词向量表示于所有 context 的乘积，结果就是对于每个单词的一个 score : $z = U\\hat{v}$ 我们将 score 转换成概率，用 softmax \b来实现：$\\hat{y} = softmax(z)$ 首先我们是知道真实的分布的也就是中心单词的 one-hot vector ，为 $y$ , 这样我们就可以用各种 loss function \b来优化了，一般是 cross entropy 。。 \b 上面这张图 $W$ 相当于 context matrix ，$W^{\\prime}$ 相当于 center matrix Skip-GramSkip-Gram Algorithm 对于 center 单词，生成它的 one-hot \b向量 然后再获取中心单词的词向量 $v_c = V x$ 用上面的中心词向量乘以 context \b词向量矩阵得到\b一个 $|V|$ 维向量 对于这一个向量，我们取不同的位置的值做 softmax 预测，从而生成对应的\b loss function 上面一张图是 CBOW 和 Skip-Gram 算法之间的比较，总的来说 CBOW ：根据周围单词，来估计中心单词出现的概率\b Skip-Gram ：根据中心单词，来估计周围单词出现的概率 GloVe直接写出 Glove 模型的优化函数吧： J(\\theta) = \\frac{1}{2} \\sum_{i,j=1}^w f(X_{ij}) (u_i^Tv_j - log X_{ij})^2 \b对于上面这个公式，f 其实是一个权重函数，$\\theta$ 是所有的变量 $U,V$ Dependecy Parsing如何描述语法，有两种主流观点，其中 一种是短语结构文法：这种短语语法用固定数量的rule分解句子为短语和单词、分解短语为更短的短语或单词。。。 一种是依存结构：用单词之间的依存关系来表达语法。如果一个单词修饰另一个单词，则称该单词依赖于另一个单词。 为什么要引用描述语法呢，因为一个句子，可以看作是几个单词的组合，但是机器\b要理解的是这些单词传递给人的意思，所以不仅仅是单词的出现，这些单词是如何表达意思的\b同样重要，\b\b因为这些句子可能会有多种意思的表达。。 比如这句活就可能有两个意思，但是确定了句法树，也就是\b上面的箭头，一个句子的意思就得到了确定。","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"Sparse-Coding","slug":"Sparse-Coding","date":"2018-09-09T01:21:38.000Z","updated":"2018-09-09T07:48:19.909Z","comments":true,"path":"2018/09/09/Sparse-Coding/","link":"","permalink":"http://yoursite.com/2018/09/09/Sparse-Coding/","excerpt":"","text":"神经学启发稀疏编码的概念来自于神经生物学。生物学家提出，哺乳类动物在长期的进化中，生成了能够快速，准确，低代价地表示自然图像的视觉神经方面的能力。我们直观地可以想象，我们的眼睛每看到的一副画面都是上亿像素的，而每一副图像我们都只用很少的代价重建与存储。我们把它叫做稀疏编码，即Sparse Coding。 从上可以看出稀疏编码的目的是：在大量的数据集中，选取很小部分作为元素来重建新的数据。 数学推导稀疏编码是一种 unsupervised learning，我们希望找到一组 over-complete 的 基向量 basic vector 来表示我们的数据，也就是数据 $\\textbf{x}$ 可以表示为这些基向量的线性组合： \\textbf{x} = \\sum_{i=1}^k a_i \\phi_i对于基向量的学习，我们一般有一组训练数据，另外，\b基向量的大小规定为 $\\textbf{x}\\in R^n$，同时，也是数据的大小。一般来讲，$n$ 维的数据最多只需要 $n$ 个线性不相关的基向量就可以了（使用 PCA），但是就 n 个基向量的话，可能有一个现象就是，没个数据可能都需要接近 n 个基向量来表示，我们希望 稀疏编码 能有一个优势，就是组成数据的基向量个数尽可能的小一些，也就是上面的 $\\mathbf{a}$ 是稀疏的。。。 为了达成这个目的，我们可以增加基向量的个数 $k &gt; n$，也就是说，一个数据，可能由多种基向量来表示了，在所有的表示中，我们可以尽可能的选取稀疏的表示方法。。 Sparse Coding 可以分为两个部分，一个是 Training 阶段，一个是 Coding阶段 Training给定一些列样本数据 $[ x_1,x_2,… x_m]$ 我们希望学到一组基 $[ \\phi_1,\\phi_2,…,\\phi_k ]$ 来表示前面的数据，训练的 objective function 如下： \\mathop{min}_{a,\\phi} \\sum_{i=1}^m \\left \\Vert x_i-\\sum_{j=1}^k a_{i,j}\\phi_j \\right \\Vert^2 + \\lambda \\sum_{i=1}^m \\sum_{j=1}^k |a_{i,j}|优化的迭代分为两部（都可以用凸优化来求解） 固定字典 $\\phi$ 更新 $a$，这个问题其实就是一个 Lasso 问题 固定\b表达 $a$ 更新 $\\phi$，这个其实就是一个 QP 问题 Coding给定一个新的数据，获得它关于字典的表达，这一步，其实就是上面的优化的第二步 \\mathop{min}_{a} \\sum_{i=1}^m \\left \\Vert x_i-\\sum_{j=1}^k a_{i,j}\\phi_j \\right \\Vert^2 + \\lambda \\sum_{i=1}^m \\sum_{j=1}^k |a_{i,j}| Referencehttps://www.cnblogs.com/aixueshuqian/p/3936892.htmlhttps://www.cnblogs.com/caocan702/p/5666175.html.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"}]},{"title":"PGM-3","slug":"PGM-3","date":"2018-08-18T13:21:46.000Z","updated":"2018-10-10T07:18:39.517Z","comments":true,"path":"2018/08/18/PGM-3/","link":"","permalink":"http://yoursite.com/2018/08/18/PGM-3/","excerpt":"","text":"The Exponential Familyrandom variable $\\mathbf{X}$ is in the exponential family \\begin{align} P(\\mathbf{X}=x;\\eta) &= h(x)\\mathop{exp}\\{ \\eta^T\\mathbf{T}(x) - A(\\eta) \\} \\\\ &= \\frac{1}{Z(\\eta)} h(x) exp \\{ \\eta^T T(x) \\} \\end{align} $\\eta$ : vector of natural parameters $\\mathbf{T}$ : vector of sufficient statistics $\\mathbf{A}$ : log partition function $log Z(\\eta) = A(\\eta)$ ExampleMultivariate Gaussian\\begin{align} P(\\mathbf{x};\\mu;\\Sigma) &= \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\text{exp}\\left( -\\frac{1}{2} (\\mathbf{x}-\\mu)^T \\Sigma^{-1} (\\mathbf{x}-\\mu) \\right) \\\\ &= \\frac{1}{(2\\pi)^{p/2}} \\text{exp} \\left( -\\frac{1}{2}(\\text{tr } \\mathbf{x}^T\\Sigma^{-1} \\mathbf{x} +\\mu^T \\Sigma^{-1} \\mu - 2 \\mu^T \\Sigma^{-1} \\mathbf{x} +ln |\\Sigma| )\\right) \\\\ &= \\underbrace{ \\frac{1}{(2\\pi)^{p/2}} }_{h(\\mathbf{x})} \\text{exp} \\left( -\\frac{1}{2} \\underbrace{\\text{tr } \\Sigma^{-1}\\mathbf{x}\\mathbf{x}^T}_{\\text{vec}(\\Sigma^{-1})^T \\text{vec}(\\mathbf{x}\\mathbf{x}^T)} + \\mu^T\\Sigma^{-1}\\mathbf{x} - \\underbrace{\\frac{1}{2} \\mu^T \\Sigma^{-1} \\mu - \\frac{1}{2} \\text{ln}|\\Sigma| }_{A(\\eta)} \\right) \\end{align}This implies that: $\\eta = \\left( \\Sigma^{-1} \\mu, -\\frac{1}{2} vec (\\Sigma^{-1}) \\right)$ $\\mathbf{T}(\\mathbf{x}) = (\\mathbf{x}, vec(\\mathbf{x}\\mathbf{x}^T))$ $A(\\eta) = \\frac{1}{2} (\\mu^T \\Sigma^{-1} \\mu + \\text{ln} |\\Sigma|)$ $h(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}}$ Bernoulli\\begin{align} P(x;p) &= p^x (1-p)^{1-x} \\\\ \\text{ln} P(x;p) &= x\\text{ln}(p)+(1−x)\\text{ln}(1−p) \\\\ &= x\\text{ln}(p)−x\\text{ln}(1−p)+\\text{ln}(1−p) \\\\ &= x(\\text{ln}(p)−\\text{ln}(1−p))+\\text{ln}(1−p) \\\\ &= x\\text{ln} (\\frac{p}{1-p}) + \\text{ln} (1-p) \\\\ \\text{exp} (\\text{ln} P(x;p)) &= \\text{exp} \\left( x\\text{ln} (\\frac{p}{1-p}) + \\text{ln} (1-p) \\right) \\end{align}This implies that: $\\eta = \\text{ln} (\\frac{p}{1-p})$ $T(x) = x$ $A(\\eta) = -\\text{ln}(1-p)$ $h(x) = 1$ Othersthe univariate Gaussian, Poisson, gamma, multinomial, linear regression, Ising model, restricted Boltzmann machines, and conditional random fields (CRFs) are all in the exponential family Why Exponential FamilyMoment generating property\\begin{align} \\int P(x,\\eta) dx = \\int h(x) e^{\\eta^T T(x) - A(\\eta)} dx &= 1 \\\\ \\int h(x) e^{\\eta^T T(x)} &= Z(\\eta) \\end{align}- \\begin{align} \\frac{dA}{d\\eta} &= \\frac{d}{d\\eta} log (Z(\\eta)) = \\frac{1}{Z(\\eta)} \\frac{d}{d\\eta} Z(\\eta) \\\\ &= \\frac{1}{Z(\\eta)} \\frac{d}{d\\eta} \\int h(x) e^{\\eta^T T(x)}dx \\\\ &= \\int T(x) \\frac{h(x) e^{\\eta^T T(x)}}{Z(\\eta)} = E[T(x)] \\end{align} - \\begin{align} \\frac{d^2 A}{d^2 \\eta} &= \\int T^2(x) \\frac{h(x) e^{\\eta^T T(x)}}{Z(\\eta)} dx - \\int T(x) \\frac{h(x) e^{\\eta^T T(x)}}{Z(\\eta)} dx \\frac{1}{Z(\\eta)} \\frac{d}{d \\eta} Z(\\eta) \\\\ &= E[T^2(x)] - E^2[T(x)] \\\\ &= Var[T(x)] \\end{align} $A(\\eta)$ is convex since \\frac{d^2 A(\\eta)}{d \\eta^2} = Var[T(x)] > 0 specific $\\eta$ map to mean $\\mu$, so we define an invert $\\psi (\\mu) = \\eta$ .","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"PGM-Exact Inference","slug":"PGM-2","date":"2018-08-14T02:46:32.000Z","updated":"2018-10-10T07:18:35.794Z","comments":true,"path":"2018/08/14/PGM-2/","link":"","permalink":"http://yoursite.com/2018/08/14/PGM-2/","excerpt":"","text":"Variable Elimination某个确定了的概率图，它的推断可以看作是一个关于所有变量的函数，我们要求的是这个函数的具体值是多少，从概率的角度上消除变量，其实就是\b做这个函数的边缘化 marginalization，我们尽量争取每次计算(消除)的变量比较少，这样总的复杂度不会高。。Elimination 似乎可以适用于所有结构的 graph 。 Directed Chain假设我们的图的结构是这样的 $A\\rightarrow B\\rightarrow C\\rightarrow D\\rightarrow E$ \\begin{align} P(e) &= \\sum_{a,b,c,d} p(a,b,c,d) \\\\ &= \\sum_{a,b,c,d} P (a)P (b|a)P (c|b)P (d|c)P (e|d) \\\\ &=\\sum_{d,c,b} P(c|b) P(d|c) P(e|d) \\sum_a P(a) P(b|a) \\\\ &\\text{this is an one variable elimination cost } k^2\\\\ &= \\sum_{d,c,b} P(c|b) P(d|c) P(e|d) p(b) \\\\ &\\ \\cdots \\\\ &= \\sum_d P(e|d) p(d) \\end{align}复杂度 Complexity: Eliminate 方法: costs $O(k^2 n)$ _这里面的 $k^2$ 表示迭代 k 次，每次计算概率也要 k_ Naive 方法: cost $O(k^n)$ Undirected Chain假设我们的图是这样的无向图 $A - B - C - D - E$ \\begin{align} P(e) &= \\sum_{a,b,c,d} \\frac{1}{Z} \\phi(b,a) \\phi(c,b) \\phi(d,c) \\phi(e,d) \\\\ &\\propto \\sum_{a,b,c,d} \\phi(b,a) \\phi(c,b) \\phi(d,c) \\phi(e,d) \\\\ &= \\sum_{a,b,c,d} \\phi(c,b) \\phi(d,c) \\phi(e,d)\\sum_a \\phi(b,a) \\\\ &= \\sum_{a,b,c,d} \\phi(c,b) \\phi(d,c) \\phi(e,d) m_a(b) \\\\ &\\ \\cdots \\\\ &= m_d(e) \\end{align}这里是无向图，原始的势函数运算成为 m 的结果不是概率，所以这里我们要 normalize 一下: P(e) = \\frac{m_d(e)}{\\sum_e m_d(e)}Graph Elimination我们从一张图来看 Elimination 的每一次过程后，剩下的图的结构 对于一张 graph 首先我们确认 elimination 的 order 对于每一个待消除的变量，它会连接一些变量，我们将这些变量两两相连 消除待消除的变量 我们再来观察每次 Elimination 后\b，也就是边缘化操作后形成的函数，发现这些函数的变量在一起，刚好能组成这个\b概率图结构\b的 cliques 集合，如果我们考虑这些 cliques 组成的树，那么 elimination 操作其实就是在这颗树上进行 message\b passing。 Key insight 就是这些 message 其实是可以 reused 的，重复使用是指，当我们要进行多次 querying 的时候，信息的重复使用，所以我们希望设计好的算法，能够在 querying 的过程中保存下来这些信息，于是有了后面的 Sum-Product 算法。 Complexity of Variable Elimination这里 $y$ 未知的，后面操作可能要消除的变量，$x$ 是正在消除的变量，$m$ 是当前要消除的乘子，sum-product 算法分为下面两部 Sum:m_x(y_1,...,y_k) = \\sum_x m^{\\prime}_x (x,y_1,...,y_k) Product:m_x^{\\prime} (x,y_1,...,y_k) = \\prod_{i=1}^k m_i(x,y_{c_i}) \b 乘起来就是一次 Elimination 的复杂度： $k\\cdot | Var(X) |\\cdot \\prod_i |Var(Y_{C_i})|$也就是当前变量的状态乘上，乘子也就是对应的 clique 的所有变量的状态叉乘 我们发现整个算法的复杂度取决于最大的\b最大的 clique，我们称这个 clique 的变量数量 k 为 Tree-width , 同时要注意的是，不同的 elimination order 的 Tree-Width 是不同的，找到最优的 order 是 np-hard 问题。。。 \b Belief Propagation\basd Trees Two-pass Algorithm Factor Trees Message Passing on Factor Graph Non-Trees (General Graph) Junction Tree Algorithm 概率图中有向图模型其实是无向图的一种特例，从有向图到无向图的转换关系如下 Undirected Tree: p(x) = \\frac{1}{Z}\\left( \\prod_{i\\in V} \\psi(x_i) \\prod_{(i,j)\\in E} \\psi (x_i,x_j) \\right) Directed Tree: p(x) = p(x_r) \\prod_{(i,j)\\in E} p(x_i|x_j) Equivalence: \\psi (x_r) = p(x_r);\\ \\psi(x_i,x_j)=p(x_j|x_i);\\ Z=1,\\psi(x_i) = 1 TreesElimination 操作可以看作是 message passing. 令 $m_{ji}(x_i)$ 当作是从 i 那里变量消除后生成的乘子，同时，这就是 $x_i$ 的函数: m_{ji}(x_i) = \\sum_{x_j} \\left( \\psi(x_j)\\psi(x_i,x_j) \\prod_{k\\in N(j)\\setminus i} m_{kj}(x_j) \\right)上面的公式可以理解为：从 $i$ 到 $j$ 的信息，只和传递信息箭头相反的范围内的那些节点相关 对于某个节点所对应的概率，我们可以这样表示： p(x_i)\\propto \\psi(x_i) \\prod_{e\\in N(i)} m_{ei}(x_i)可以看到计算 $p(x_i)$ 的时候， $m_{ij}(x_i)$ 会被重复的使用, 所以我们可以存储 $m$ 的值 树的 Elimination 来做 querying 算法的复杂度是 $O(NC)$ (where N=nodes, C=complexity of one complete passing/clique bottleneck). 但是使用了 two path 算法（因为是无向图所以每条边有两个方向）以后，复杂度就变成了: 2C, or $O(C)$ belief propagation is only valid on trees \bFactor Trees\b首先，我们定义一个变换，这个变换把一个图变成了一个新的图，变换后的图称为 Factor Graph,如果变换后刚好是一颗树，那么我们也可以称之为 Factor \bTree。 在新的 Factor Graph 中，每一个\b factor (clique) 在图中表示一个节点 f, 下面是一个例子, 其中的一个性质就是 $f$ 节点只和 $x$ 节点相连，也就是说，x 的某个变量把它和其他变量的关系都托付给了 f 节点。 对于一个图，可能有好几种变换方式，我们希望变换后的结果就是一个树，和下面的 Example 3 一样。 另外变换后的图其实是一个二分图（bipartite），二分图每一侧都是一种类型的节点，所以信息传递策略于传统的方法有些不同。。有两种信息的传递方式\b $\\nu$\b : from variables to factors（左图）\\nu_{is}(x_i) = \\prod_{t\\in N(i)\\setminus s} \\mu_{ti}(x_i) $\\mu$ : from factors to variables（右图）\\mu_{si}(x_i) = \\sum_{x_{N(s)}\\setminus i}\\left( f_s(x_{N(s)}) \\prod_{j\\in N(s)\\setminus i} \\nu_{js}(x_j)\\right) _上面的 $\\sum$ 操作是遍历变量的赋值，$\\sum$ 操作下面的 x 可以看作是一个向量，遍历向量里面所有的赋值._ Factor Tree 算法只能够处理一些长得像树的概率图\b\b\b Junction TreesJunction tree data-structure for exact inference on general graphs Algorithm Moralization Triangulation Junction tree Message Propagation Moral Graph因为我们要处理的是广泛结构的概率图模型，所以我们先把 BN 纳入到 MRF 的框架里面，这一步骤叫做 Moralization，我们知道 BN 中的 factor 是某些父变量对于指定变量的条件概率，我们不管哪些是条件变量，我们就把他们看成是一个整体的函数，我们的终极目的是生成一个 clique，clique 有要求是全联通的，于是我们就将这个变量的父节点两两配对相连，这样\b就形成了一个 clique，原来的 factor 就变成了势函数 potential。 在这里我们得到一个启发，就是增加一条边后，原本的 graph 是新的\b graph 的一种特殊情况。 Triangulation对于三角化的操作，我们可以先看后面两个操作的介绍再回来，因为这是为了解决后面问题的而诞生的一个步骤 问题就是 Local Consistency 不能导出 Global Consistency，只有在三角化后的图中 三角化以后的图是没有大于 4 个节点以上的环的， 三角化的方法就是在大的环中添加额外边 Clique Tree 下面的推断可以知道，有向图条件概率乘积的表达形式，其实就是 clique tree 表达形式的一种特殊情况\b \\begin{align} &P(X_1,X_2,X_3,X_4,X_5,X_6)\\\\ & = P(X_1)P(X_2)P(X_3 | X_1,X_2)P(X_4 | X_3)P(X_5 | X_3)P(X_6 | X_4,X_5) \\\\ & = P(X_1,X_2,X_3) \\frac{P(X_3,X_4,X_5)}{P(X_3)} \\frac{P(X_4,X_5,X_6)}{P(X_4,X_5)} \\\\ & = \\psi(X_1,X_2,X_3) \\frac{\\psi(X_3,X_4,X_5)}{\\phi(X_3)} \\frac{\\psi(X_4,X_5,X_6)}{\\phi(X_4,X_5)} \\end{align}General Form : 之所以下面是要除以 cliques 之间的交集 S ，是因为交集的信息可能出现了多次 P(\\mathbf{X}) = \\frac{\\prod_{c} \\psi_c(\\mathbf{X_c})}{\\prod_{s} \\phi_s(\\mathbf{X_s})}Message Passing\b传递方式有两种，这两种方法算出来的结果应该是一样的，这是我们做出的假设称为 Local Consistency P(S) = \\sum_{V\\setminus S} \\psi(V) \\qquad \\qquad P(S) = \\sum_{W\\setminus S} \\psi(W)下面的第一行是 forward update，第二行是 backward update，其中 $\\frac{\\phi_S^*}{\\phi_S}$ 是通过 Local Consistency 得出的，是建立起矩形节点两边沟通的桥梁 上面是 clique tree 信息传递的方式， Shafer-Shenoy algorithm AppendixGeneral Variable Elimination为了让计算机能够自动的处理各种各样结构的概率图的\b Elimination，我们可以设计一种更加 general 的形式，但是这个形式的设定，主要还是为了进行计算机的运算的。。 Let $X$ be set of all random variables Let $F$ denote the set of factors and then for each $\\phi \\in F$,$Scope[\\phi] \\in X$ There three type of variables in Elimination Model Let $Y\\subset X$ be a set of query variables Let $Z = X - Y$ would be the set of variables to be eliminated Let $\\mathcal{E}$ be the known variables, and $\\bar{e}_i$ is the assignment The core operation can be view as the form of, we can extend it to general form by import \bevidence potential \\tau(Y) = \\sum_z \\prod_{\\phi \\in F} \\phi The evidence potantial: \\begin{align} \\delta(\\mathcal{E}) = \\left \\{ \\begin{array}{ll} 1& if\\ \\mathcal{E_i} \\equiv \\bar{e}_i \\\\ 0 & if\\ \\mathcal{E_i} \\neq \\bar{e}_i \\end{array} \\right . \\end{align} Total evidence potential: \\begin{align} \\delta(\\mathbf{\\mathcal{E}},\\mathbf{\\bar{e}})= \\prod_{i\\in I_{\\mathcal{E}}} \\delta (\\mathcal{E}_i,\\bar{e}_i) \\end{align} Introducing evidence:\\tau(\\mathbf{Y},\\mathbf{\\bar{e}}) = \\sum_{z,e}\\prod_{\\phi \\in F} \\phi \\times \\delta(\\mathbf{\\mathcal{E}},\\bar{\\mathbf{e}}) The elimination algorithm… … Referencehttps://www.jianshu.com/p/f90100680749 .","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Random-Record","slug":"Random-Record","date":"2018-08-09T10:55:56.000Z","updated":"2018-08-24T14:41:49.394Z","comments":true,"path":"2018/08/09/Random-Record/","link":"","permalink":"http://yoursite.com/2018/08/09/Random-Record/","excerpt":"","text":"如果一个变量在两个 clique 之间出现，那么\b这个变量一定在这两个 clique 的路径之间 learning is parameter estimation sufficient statistics $T(x)$ means x self or some transformation of x 先验概率可理解为统计概率，后验概率可理解为条件概率 In Bayesian probability theory, if the posterior distributions $p(\\theta | x)$ are in the same probability distribution family as the prior probability distribution $p(\\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also GaussianLearning Graphical Models The target is given the assignments and predict the best (most likely) structure of the network. “Optimal” here means the employed algorithms guarantee toreturn a structure that maximizes the objectives (e.g., LogLik) \\begin{align} l(\\theta_G,G;D) &= log p(D|\\theta_G,G) \\\\ &= log \\prod_n \\left( \\prod_i p(x_{n,i}|\\mathbf{x}_{n,\\pi_i(G)}, \\theta_{i|\\pi_i(G)}) \\right) \\end{align} $\\prod_n$ means enum $n$ data $\\prod_i$ means enum all nodes $\\mathbf{x}_{n,\\pi_i(G)}$ is the assignments of x’s parents M is the number of the state, add this we can turn the count function into probability representation the right decomposited part is entropy ! 。","categories":[],"tags":[]},{"title":"PGM-Representation","slug":"PGM-1","date":"2018-08-08T12:51:00.000Z","updated":"2018-10-10T07:18:32.202Z","comments":true,"path":"2018/08/08/PGM-1/","link":"","permalink":"http://yoursite.com/2018/08/08/PGM-1/","excerpt":"","text":"Introduction of PGMWhy Using PGMFor a joint distribution $P(X_1,X_2,…,X_n)$, can be written in two ways All Dependent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2|X_1)P(X_2|X_1,X_2)…P(X_n|X_1,…,X_{n-1})$ pro: the formular are correct in any case con: require huge probability table $O(k^n)$ Independent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2)…P(X_n)$ pro: probability table only take $O(kn)$ memory con: independent are restrictive hypothese The PGM are target a middle-ground between two extremes Two types of GMs Directed edges give causality relationships (Bayesian Network or Directed Graphical Model) Undirected edges simply give correlations between variables (Markov Random Field or Undirected Graphical model) Bayesian NetworkFirstly the Bayesian Network is DAG (Directed Acyclic Graph) Factorization Theorem : The probability of specific BN P(X_1,X_2,...,X_n) = \\prod_{i=1:n} P(X_i|Parents(X_i))Graph model and BayesianSuppose the distribution is $A\\leftarrow B \\rightarrow C$ Bayesian Interpretation: $P(ABC) = P(B) P(A|B) P(C|B)$ Graph Model Interpretation: $I(G) = \\{ A\\bot C |B \\}$now we want to prove $I(P(ABC)) = I(\\{ A\\bot C |B \\})$ \\begin{align} P(AC|B) &= P(A|B)P(C|B) \\leftarrow \\{ A\\bot C |B \\}\\\\ P(AC|B) &= \\frac{P(ABC)}{P(B)} = \\frac{ P(B) P(A|B) P(C|B)}{P(B)} \\leftarrow P(ABC) \\end{align} conditional Independence :for $X\\leftarrow Z \\rightarrow Y$, $Z$ represent the height of father, $X,Y$ are the brother, the relation between $X$ and $Y$ : Dependent : we dont know the height of father, but we know the height of $Y$, so $Y$ may effect the distribution of $Z$, at the same time also influence the $X$ Independent : we already know the height of father, so whether $Y$ is dont influence $X$ local Markov assumption :each node $X_i$ is independent of its nondescendants given its parents._when you confirm the parents, you only dependent with your childs_ I_l(G):\\{ X_i\\bot NonDescendants_{X_i} | Pa_{X_i} : \\forall i \\}Independencies (Three Basic Model) a: Cascade $Y$ observed $X,Z$ are independent b: Common parent $Y$ observed $X,Z$ are independent $Y$ unknow $X,Z$ are dependent c: V-structure $Y$ observed $X,Z$ are dependent $Y$ unknow $X,Z$ are independent Let $P$ be a distribution of $X$, $I(P)$ is the set of independence assertions of the form $(X \\bot Y | Z)$ that hold in $P$. $I(G)$ is the sub-set of the $I(P)$, We say that $K$ is an $I$-map for a _set_ of independencies $I$ if $I(K) ⊆ I$ Active trail Causal Trail $X → Z → Y$ : active iff $Z$ is not observed. Evidential Trail $X ← Z ← Y$ : active iff $Z$ is not observed. Common Cause $X ← Z → Y$: active iff $Z$ is not observed. Common Effect $X → Z ← Y$ : active iff $Z$ (or any of its descendents) is observed._here active means the dependent relation estibilish_ Definition : Let $\\textbf{X}, \\textbf{Y} , \\textbf{Z}$ be three sets of nodes in $G$. We say that $\\textbf{X}$ and $\\textbf{Y}$ are d-separated given $\\textbf{Z}$, denoted $d\bsep_G(\\textbf{X};\\textbf{Y}|\\textbf{Z})$, if there is no active trail between any node $X \\in \\textbf{X}$ and $Y \\in \\textbf{Y}$ given $\\textbf{Z}$ Definition: $I(G)=$ all independence properties that correspond to d- separation: I(G) = {X \\bot Y | Z : dsep_G(X \\bot Y | Z)} Undirected GMAn undirected graphical model represents a distribution $P(X1,…Xn)$ defined by an undirected graph H, and a set of positive-valued potential functions $\\psi_c$ corresponding to each clique $c \\in C$ of $H$ such that: \\begin{align} P(X_1,...,X_n) = \\frac{1}{Z} \\prod_{c\\in C} \\psi_c(X_c) \\\\ Z = \\sum_{X_1,...,X_n} \\prod_{c\\in C} \\psi_c(X_c) \\end{align}potential function can be joint and conditional probability function, or even a table of values Clique Example max-cliques = $\\{A,B,D\\}, \\{B,C,D\\}$, sub-cliques = $\\{A,B\\}, \\{C,D\\}, …\\text{ all edges and single point}$ \\begin{align} P^{\\prime}(x_1,x_2,x_3,x_4) = \\frac{1}{Z} \\psi_c(X_{124}) \\times \\psi_c(X_{234}) \\\\ Z = \\sum_{x_1,x_2,x_3,x_4} \\psi_c(X_{124}) \\end{align}Independence: global Markov independencies $I(H)$ $= \\{ A\\bot C|B:sep_H(A,C|B) \\}$ any disjoint A,B,C in distribution, B separates A and C, A is independent of C given B. local Markov independencies $I_l(H)$ $=\\{X_i \\bot V \\setminus (X\\cup MB_{X_i})|MB_{X_i} :\\forall i \\}$ Independent with node that dont near it. pairwise Markov independencies $I_p(H)$ $=\\{X \\bot Y|V \\setminus \\{X,Y\\}:\\{X,Y\\}\\notin E\\}$ Independent when no shared edge. Markov blanket of $X_i$ denoted $MB_{X_i}$, is the neighbors of $X_i$ in graph B separates A and C if every path from A to C through B: $sep_H(A,C|B)$ relation of local and global Thm: $P\\models I(H) \\Rightarrow P \\models I_l(H) \\Rightarrow P \\models I_p(H)$ Corollary: For a positive distribution P, global, local, and pairwise indepedencies are equivalent Exponential Model\bForm discussed above, we need find a clique potential can ensure the positive distribution, one form is negative exponential: $\\Psi (\\mathbf{x}_c) = exp\\{ -\\phi_c(\\mathbf{x_c}) \\}$ The exponential form of the distribution structure is: p(\\mathbf{x}) = \\frac{1}{Z}\\mathop{exp}\\left\\{ - \\sum_{c\\in C} \\phi_c(\\mathbf{x_c}) \\right\\}=\\frac{1}{Z} \\mathop{exp}\\{ -H(\\mathbf{x}) \\}$H(\\mathbf{x})$ is the “free energy”. Boltzmann MachinesA Boltzmann Machine is a fully connected graph with pairwise potentials on binary-valued nodes. The energy function for this is expressed in sub-clique form, which comes from the physics tradition. \\begin{align} P(\\mathbf{x}) &= \\frac{1}{Z} \\mathop{exp} \\left\\{ \\sum_{i,j} \\phi_{ij}(x_i,x_j) \\right\\} \\\\ &= \\frac{1}{Z} \\mathop{exp} \\left\\{ \\sum_{i,j} \\theta_{ij}x_ix_j + \\sum_i \\alpha_i x_i + C \\right\\} \\\\ &= \\frac{1}{Z} \\mathop{exp} \\left\\{ (x-\\mu)^T \\Theta (x-\\mu) \\right\\} \\end{align}Restricted Boltzmann MachinesThis is inspired by the Boltzmann Machine and is responsible for much of the deep learning craze. An RBM consists of many layers. Within each layer, there are two sublayers: one of hidden units (factors, $h_j$), and one of visible units ($x_i$). The probability function for an RBM is p(x,h|\\theta) = \\mathop{exp} \\left\\{ \\sum_i \\theta_i \\phi_i (x_i) + \\sum_j \\theta_j \\phi_j(h_j) + \\sum_{i,j} \\theta_{i,j} \\phi_{i,j} (x_i,h_j) - A(\\theta) \\right\\}Conditional Random Fields… .","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process-11","slug":"Stochastic-Process-11","date":"2018-08-04T08:05:19.000Z","updated":"2018-10-10T07:19:31.021Z","comments":true,"path":"2018/08/04/Stochastic-Process-11/","link":"","permalink":"http://yoursite.com/2018/08/04/Stochastic-Process-11/","excerpt":"","text":"i.i.d sequence of random variables: too restrictive assumption completely dependent among random variables: hard to analysis balance between complete independence &amp; complete dependence Classification of Markov Process Discrete-Time Markov Chain: Discrete $S$ &amp; Discrete $T$ Continuous-Time Markov Chain: Discrete $S$ &amp; Continuous $T$ Discrete Markov Chain: Continuous $S$ &amp; Discrete $T$ Continuous Markov Chain: Continuous $S$ &amp; Continuous $T$ Definition (Markov Chain)A sequence of random variables $X_0,X_1,X_2,…$ taking values in the state space $\\{1,2,…,M\\}$ is called Markov Chain, the event $X_i+1$ only influenced by $X_i$ P(X_{n+1}=j|X_n = i,X_{n-1}=i_{n-1},...,X_0=i_0) = P(X_{n+1}=j|X_n=i)Definition (Transition Matrix)Let $X_0,X_1,X_2,…$ be a Markov Chain with state space $\\{1,2,…,M\\}$, and let$q_{ij} = P(X_{n+1}=j|X_N=i)$ be the transition probability from state $i$ to state $j$. The $M \\times M$ matrix $Q=(q_{ij})$ is called transition matrix of the chain. Definition (n-step Transition Probability)The n-step transition probability from $i$ to $j$ is the probability of being at $j$ exactly $n$ steps after being at $i$. Denote this by $q^{(n)}_{ij}$ q^{(n)}_{ij} = P(X_n=j|X_0=i)=\\sum_{k\\in S} q^{(1)}_{kj} q^{(n-1)}_{ik}which implies Q^n = Q^{n-1}\\cdot QTheorem (Chapman-Kolmogorov Equation)q_{ij}^{(n+m)} = \\sum_{k\\in S} q^{(n)}_{ik} q^{(m)}_{kj}First Step AnalysisExample (Toss A Coin till HH Appear)This problem can be formulated as a 3-state markov chainThe Transition graph is equivalent toLet $e_s = E[\\text{waiting time for HH|initial state = s}]$, then we have \\begin{align} e_{Null} &= \\frac{1}{2} (1+e_{Null})+\\frac{1}{2} (1+e_H) \\\\ e_H &= \\frac{1}{2} (1+e_{HH}) + \\frac{1}{2} (1+e_{Null})\\\\ e_{HH} &= 0 \\end{align}Example (Toss A Coin till HTHT Appear)Let see a more complicate case , this can be done by establish a linear equation Classification of StatesDefinition (Recurrent and Transition States) Recurrent State $i$ of Markov chain have the probability of $1$ eventually return to $i$ Transient Other-wise, the state is Transient Definition (Irreducible &amp; Reducible Chain) Irreducible any state $i$ and $j$, possible to go from $i$ to $j$ in a finite number of steps. Reducible not irreducible Theorem (Irreducible Implies All States Recurrent)In an irreducible Markov Chain with a finite state space, all states are recurrent Example (Coupon Collector)We want to collect all $C$ types coupons, Let $X_n$ be the number of distinct coupon types in our collection after $n$ attempts. Then $X_0,X_1,…$ is a Markov Chain on the state space $\\{ 0,1,…,C \\}$ Definition (Period)The period of state $i$ in a markov chain is the gcd of the possible numbers of steps can return to $i$ when starting at $i$. A state is called aperiodic if its period equals 1, and periodic otherwise. .","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process-10","slug":"Stochastic-Process-10","date":"2018-08-04T08:05:13.000Z","updated":"2018-10-10T07:19:33.935Z","comments":true,"path":"2018/08/04/Stochastic-Process-10/","link":"","permalink":"http://yoursite.com/2018/08/04/Stochastic-Process-10/","excerpt":"","text":"Bayesian InferenceBayesian Inference FrameworkWe aim to extract information about $\\Theta$, based on observing a collection $X = (X_1,…,X_n)$ Unknow $\\Theta$ treated as a random variable prior distribution $p_\\Theta$ or $f_\\Theta$ Observation $X$ observation model $p_X|\\Theta$ or $f_X|\\Theta$ Use appropriate version of the bayes rule to find $p_{\\Theta|X}(\\cdot|X=x)$ Principal Bayesian Estimation Method Maximum a posterior probability (MAP) rule Select the possible parameter with maximum conditional/posterior probability given the data Least mean squares (LMS) estimation Select an estimator/function of the data that minimizes the mean squared error between the parameterand its estimate $p_{\\Theta|X}(\\theta^*|x)=\\mathop{max}_{\\theta}p_{\\Theta|X}(\\theta|x)$ Example (Inferring the Unknown Bias of A Coin)We wish to estimate the probability of heads, denoted by $p$, suppose the prior is a beta density with $a,b$, that is , $p \\sim Beta(a,b)$. We consider n independent tosses and let $X$ be the number of heads observed MAP Estimate The posterior PDF of $p$ has the form \\begin{align} f(p|X=k) &= \\frac{P(X=k|p)f(p)}{P(X=k)}=\\frac{\\left( \\begin{array}{c} n\\\\k \\end{array} \\right) p^k(1-p)^{n-k}\\frac{1}{\\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)} \\\\ &= c\\cdot p^{a+k-1} (1-p)^{b+(n-k)-1} \\end{align}Hence the posterior density is beta with parameters $a+k$ and $b+(n-k)$ By MAP rule, we select the eatimator as \\hat{p}_{MAP} = \\mathop{arg}\\mathop{max}_p f(p|X=k) = \\mathop{arg}\\mathop{max}_p p^{a+k-1}(1-p)^{b+(n-k)-1}Let $g(p) = p^{a+k-1}(1-p)^{b+(n-k)-1}$,then we have log(g(p)) = (a+k-1)\\mathop{log}p + (b+(n-k)-1)\\mathop{log}(1-p)To find $p^*$ let \\partial \\frac{ \\mathop{log}(g(p))} {\\partial p}|_{p=p^*} = 0which yields \\hat{p}_{MAP} = \\frac{a+k-1}{a+b+n-2}When the prior distribution of $p$ is $Unif(0,1)$, that is $a=0,b=0$, the estimator under MAP rule is $\\hat{p}_{MAP} = \\frac{k}{n}$ LMS Estimate By Beta-Binomial conjugacy, $f(p|X=k) \\sim Beta(a+k,b+n-k)$, the expectation of random variable $Y\\sim Beta(a,b)$ is $E(Y) = \\frac{a}{a+b}$, we have \\hat{p}_{LMS} = E(p|X=k) = \\frac{a+k}{(a+k)+(b+n-k)} = \\frac{a+k}{a+b+k}When the prior distribution of $p$ id $Unif(0,1)$, that is $a = 1,b = 1$, the estimator under MAP rule is $\\hat{p}_{MAP} = \\frac{k+1}{n+2}$ Classical Inference Classical Statistics: unknown constant $\\theta$ also for vectors $X$ and $\\theta$ : $p_{X_1,…,X_n}(x_1,…,x_n; \\theta_1,…,\\theta_m)$ $p_X(x;\\theta)$ are NOT conditional probabilities; $\\theta$ is not random mathematically: many models, one for each possible value of $\\theta$ For example, the data observation model is $X\\sim Binomial(n,\\theta)$, Then under each possible value of $\\theta$, the candidate model is p_X(x;\\theta) = P(X=x;\\theta) = \\left( \\begin{array}{c} n\\\\k \\end{array} \\right) \\theta^x (1-\\theta)^{n-x}Classical Inference use the maximum likelihood to estimate the $\\theta$ Sampling MomentsDefinition (Moments)Let $X$ be an r.v. with mean $\\mu$ and variance $\\sigma^2$, The $n^{th}$ moment of $X$ is $E(X^n)$, the $n^{th}$ central moment is $E((X-\\mu)^n)$, and the $n^{th}$ standardized moment is $E((\\frac{X-\\mu}{\\sigma})^n)$ Definition (Sample Moments)Let $X_1,…,X_n$ be i.i.d. random variables, the $k^{th}$ sample moment is the M_k = \\frac{1}{n} \\sum_{j=1}^n (X_j)^kThe sample mean $\\bar{X}_n$ is the first sample moment: \\bar{X}_n = \\frac{1}{n} \\sum_{j=1}^n X_jTheorem (Mean and Var of Sample Mean)Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\\mu$ and variance $\\sigma^2$. Then the sample mean $\\bar{X}_n$ is unbiased for estimating $\\mu$. That is E(\\bar{X}_n) = \\muThe variance is Var(\\bar{X}_n) = \\frac{\\sigma^2}{n}Definition (Sample Variance)Let $X_1,…,X_n$ be i.i.d. random variables. The sample variance is the r.v. S_n^2 = \\frac{1}{n-1} \\sum_{j=1}^n (X_j-\\bar{X}_n)^2Theorem (Unbiaseness of Sample Var)Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\\mu$ and variance $\\sigma^2$. The Sample Var $S_n^2$ is unbiased for estimating $\\sigma^2$ E(S_n^2) = \\sigma^2Definition (Convergence with Probability)Let $X_1,X_2,…$ be random variables. $X_n$ converges almost surely (a.s.) to the random variable $X$ as $n\\rightarrow \\infty$ and only if P(\\left\\{ \\omega : X_n(\\omega) \\rightarrow X(\\omega)\\ as\\ n\\rightarrow \\infty \\right\\}) = 1Notation: $X_n \\xrightarrow{a.s.} X \\text{ as } n\\rightarrow \\infty$ Example Definition (Convergence in Probability)Let $X_1,X_2,…$ be random variables. $X_n$ converges in probability to the random variable $X$ as $n\\rightarrow \\infty$ if and only if for every $\\epsilon &gt;0$ P(|X_n-X|>\\epsilon) \\rightarrow0 \\ as \\ n\\rightarrow \\inftyNotation: $X_n \\xrightarrow{P} X \\text{ as } n\\rightarrow \\infty$ Example Law of large NumbersDefinitionLet $X_1,…,X_n$ be i.i.d. r.v. with finite mean $\\mu$ and finite variance $\\sigma^2$. The samplw mean $\\bar{X}_n$ is defined as \\bar{X}_n = \\frac{1}{n}\\sum_{j=1}^n X_jThe Sample mean $\\bar{X}_n$ is itself an r.v. with mean $\\mu$ and variance $\\sigma^2/n$ Theorem (Strong Law of Large Numbers)The event $\\bar{X}_n \\rightarrow \\mu$ has probability $1$ Theorem (Weak Law of Large Numbers)For all $\\epsilon &gt;0, P(|\\bar{X}_n - \\mu&gt;\\epsilon) \\rightarrow 0$ as $n\\rightarrow \\infty$ Definition (Time Average)\\bar{N}^{Time\\ Average}(\\omega)=\\mathop{lim}_{t\\rightarrow \\infty} \\frac{\\int_0^t N(v,\\omega)dv}{t}Definition (Ensemble Average)\\bar{N}^{Ensemble}(\\omega)=\\mathop{lim}_{t\\rightarrow\\infty} E[N(t)]=\\sum_{i=0}^{\\infty} i p_i Central Limit TheoremTheorem (Central Limit)As $n\\rightarrow \\infty$ \\sqrt{n}\\left( \\frac{\\bar{X}_n - \\mu}{\\sigma} \\right) \\rightarrow N(0,1)CLT Approximation For a large $n$, the distribution od $\\bar{X}_n$ is approximately $N(\\mu,\\sigma^2/n)$ Example (CLT Example) Poisson Convergence to Normal Let $Y\\sim Pois(n)$. Consider $Y$ as sum of $n$ i.i.d. $Pois(1)$ r.v.s. For large $n$:Y\\sim N(n,n) Gamma Convergence to Normal Let $Y\\sim Gamma(n,\\lambda)$. Consider $Y$ as sum of $n$ i.i.d. $Expo(\\lambda)$ r.v.s. For large $n$:Y\\sim N(\\frac{n}{\\lambda},\\frac{n}{\\lambda^2}) Binomial Convergence to Normal Let $Y\\sim Bin(n,p)$. Consider $Y$ as sum of $n$ i.i.d. $Bern(p)$ r.v.s. For large $n$:Y\\sim N(np,np(1-p)) De Moivre-Laplace Approximation\\begin{align} P(Y=k) &= P(k-\\frac{1}{2} < Y < k + \\frac{1}{2}) \\\\ &\\approx \\Phi\\left( \\frac{k+\\frac{1}{2} - np}{\\sqrt{np(1-p)}} \\right) -\\Phi\\left( \\frac{k-\\frac{1}{2} - np}{\\sqrt{np(1-p)}} \\right) \\end{align} Possion Approximation When $n$ is large and $p$ is samll Normal Approximation When $n$ is large and $p$ is around $1/2$ InequalityBasic Inequalities Cauchy-Schwarz Inequality |E(XY)| \\leq \\sqrt{E(X^2)E(Y^2)} Jensen’s Inequality If $g$ is a convex function and $X$ is a r.v. thenE(g(x)) \\geq g(E(X)) If $g$ is a concave functionE(g(x)) \\leq g(E(X)) Markov’s Inequality P(|X|\\geq a) \\leq \\frac{E|X|}{a} Chebyshev’s Inequality Let $X$ have mean $\\mu$ and variance $\\sigma^2$ P(|X-\\mu|\\geq a) \\leq \\frac{\\sigma^2}{a^2} Chernoff’s Inequality P(X\\geq a) \\leq \\frac{E(e^{tX})}{e^{ta}} Concentration InequalitiesHoeffding Lemma Let r.v. $X$ satisfy $E(X) = 0$ and $X\\leq b$, Then for any $h&gt;0$ E(e^{hX}) \\leq e^{\\frac{1}{8}h^2 (b-a)^2}Hoeffding Inequality Let the r.v. $X_1,X_2,…,X_n$ be independent, with $x_k\\leq X_k\\leq b_k$ for each $k$, Let $S_n = \\sum_{k=1}^n X_k$. Then P(|S_n- \\mu|\\geq t) \\leq 2e^{-\\frac{2t^2}{\\sum_{k=1}^n(b_k-a_k)^2}}.","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process (Conditional Expectation)","slug":"Stochastic-Process-8","date":"2018-08-03T12:33:40.000Z","updated":"2018-10-10T07:19:37.000Z","comments":true,"path":"2018/08/03/Stochastic-Process-8/","link":"","permalink":"http://yoursite.com/2018/08/03/Stochastic-Process-8/","excerpt":"","text":"Conditional Expectation Given An EventIf $Y$ is a discrete r.v. E(Y|A) = \\sum_y P(Y=y|A)If $Y$ is continuous r.v. E(Y|A) = \\int_{-\\infty}^{\\infty} yf(y|A)dyApproximationImage a large number of $n$ of replication of experiments $y_1,…,y_n$ E(Y)\\approx \\frac{1}{n}\\sum_{j=1}^n y_jIf $I_j$ is the indicator of $A$ occurring E(Y|A) \\approx \\frac{\\sum_{j=1}^n y_j I_j}{\\sum_{j=1}^n I_j}Example (Life Expectation)Yang is 24 years old, he hear average life expectancy is $80$, Should he conclude he has 50 years of life left ? Of Course not, cause he already live $24$ years and some people may die less than $24$ E(T) < E(T|T\\geq 30)Law of Total ExpectationLet $A_1,…,A_n$ be partition of a sample space, $Y$ be a random variable on sample space. Then E(Y) = \\sum_{i=1}^n E(Y|A_i) P(A_i)Example (Geometric Expectation Redux)Let $X\\sim Geom(p)$, as the number of Tails before the first Heads in a sequence of coin flips with p. $p$ of head. To get $E(X)$ from sum of series, it also can be obtained in another way. We condition on the outcome of the first toss: if it lands heads, then $X$ is $0$ and we’re done ; if it lands Tails, then we wasted one toss and back to where we started by memorylessness Therefore \\begin{align} E(X) &= E(X|\\text{first toss }H)\\cdot p + E(X|\\text{first toss }T)\\cdot q \\\\ &= 0 \\cdot p + (1+E(X)) \\cdot q \\end{align}which gives $E(X) = q/p$ Example (Time until HH vs. HT)You toss a fair coin repeatedly. What is the expected number of tosses until the pattern HT/HH appears for the first times ? Times until HT $W_{HT}$: number of tosses untill HT appears $W_1$: waiting time for first H $W_2$: additional waiting time for the first T Then $W_1\\sim Fs(\\frac{1}{2}),E[W_1] = 2$ $W_2\\sim Fs(\\frac{1}{2}),E[W_2] = 2$ E[W_{HT}] = E[W_1+W_2]=E[W_1] + E[W_2] = 4 Times until HH E[W_{HH}] = E[W_HH|\\text{first toss }H]\\cdot \\frac{1}{2} + E[W_HH|\\text{first toss }T]\\cdot \\frac{1}{2} where E[W_{HH}|\\text{first toss }T] = 1 + E[W_{HH}] and \\begin{align} E[W_{HH}|\\text{first toss }H] =& E[W_{HH}|\\text{first toss }H, \\text{second toss }H]\\cdot \\frac{1}{2} \\\\ &+ E[W_{HH}|\\text{first toss }H, \\text{second toss }T]\\cdot \\frac{1}{2} \\\\ =& 2\\cdot \\frac{1}{2} + (E[W_{HH}]+2)\\cdot \\frac{1}{2} \\end{align} Thus we get $E[W_{HH}] = 6$ As we can see the above example use the memorylessness property of the Conditional Expectation of distribution, and construct target in both side to calculate the target Conditional Expectation Given An R.V.Let $g(x) = E(Y|X=x)$ Then the conditional expectation of $Y$ given $X$, denoted $E(Y|E)$ is defined to be the random variable $g(X)$ Example (Stick Length)Suppose we have a stick of length $1$ and break the stick at a point $X$ chosen uniformly at random. Given that $X=x$, we then choose another breakpoint $Y$ uniformly on the interval $[0,x]$, find $E(Y|X)$, and its mean and variance E(Y|X) = X/2E(E(Y|X)) = E(X/2) = \\frac{1}{4}Var(E(Y|X)) = Var(X/2) = \\frac{1}{48} Properties of Conditional ExpectationTheorem (Dropping independent)If $X$ and $Y$ are independent, then $E(Y|X)=E(Y)$ Taking Out What’s KnownE(h(X)Y|X) = h(X) E(Y|X)Theorem (Linearity)E(Y_1+Y_2|X) = E(Y_1|X) + E(Y_2|X)Theorem (Adam’s Law)For any r.v.s $X$ and $Y$ E(E(Y|X)) = E(Y)Proof by LOTP For $X$ discrete E(Y) = \\sum_x E(Y|X=x) P(X=x) We let $E(Y|X=x) = g(x)$, then E(E(Y|X)) = E(g(X)) = \\sum_x g(x) P(X=x) = \\sum_x E(Y|X=x) P(X=x) So E(E(Y|X)) = E(Y)Theorem (Adam’s Law with Extra Conditioning)For any r.v.s $X,Y,Z$ E(E(Y|X,Z)|Z) = E(Y|Z)E(E(X|Z,Y)|Y) = E(X|Y)Definition (Conditional Variance)Var(Y|X) = E((Y-E(Y|X))^2|X)this equivalent to Var(Y|X) = E(Y^2|X) - (E(Y|X))^2Theorem (Eve’s Low)Var(Y) = E(Var(Y|X)) + Var(E(Y|X))Example (Random Sum)A store receives $N$ customers a day, $N$ is an r.v. with finite mean and variance. Let $X_j$ be the amount spend by the $j^{th}$ customer, $X_j$ has the mean $\\mu$ and variance $\\sigma^2$, $N$ and $X_j$ are independent of one another. Find the random sum $X = \\sum_{j=1}^N X_j$ in terms of $\\mu,\\sigma^2,E(N),Var(N)$ For E(X) E(X|N) = E\\left( \\sum_{j=1}^N X_j|N \\right) = \\sum_{j=1}^N E(X_j|N)= \\sum_{j=1}^N E(X_j) = N\\mu Finally, by Adam’s Law E(X) = E(E(X|N)) = E(N\\mu) =\\mu E(N)For Var(X) We conditon on $N$ get $Var(X|N)$ Var(X|N) = Var\\left( \\sum_{j=1}^N X_j|N \\right) = \\sum_{j=1}^N Var(X_j|N) = \\sum_{j=1}^N Var(X_j) = N\\sigma^2 Eve’s Law give the unconditional variance of $X$ \\begin{align} Var(X) =& E(Var(X|N)) + Var(E(X|N))\\\\ =& E(N\\sigma^2) + Var(N\\mu) \\\\ =& \\sigma^2 E(N) + \\mu^2 Var(N) \\end{align} Prediction and EstimationTheorem (Projection Interpretation)For any function $h$, the r.v. $Y-E(Y|X)$ is Uncorrelated with $h(X)$: $Cov(Y-E(Y|X),h(X)) = 0$, equivalently E((Y-E(Y|X))h(X)) = 0.","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"PGM-0","slug":"PGM-0","date":"2018-08-01T02:28:56.000Z","updated":"2018-10-10T07:18:26.397Z","comments":true,"path":"2018/08/01/PGM-0/","link":"","permalink":"http://yoursite.com/2018/08/01/PGM-0/","excerpt":"","text":"BasisDiscriminative &amp; Generative Model 判别模型 Discriminative Model，又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution) 生成模型 Generative Model，又叫产生式模型。估计的是联合概率分布（joint probability distribution） 对于这两个模型，我们用一个例子就能很好的解释：假设我们有一些数据 $(x,y)$，数据只有四组：$(1,0), (1,0), (2,0), (2,1)$ Generative Model : p(x,y) y=0 y=1 x=0 1/2 0 x=1 1/4 1/4 Discriminative Model : p(x\\ y) y=0 y=1 x=0 1 0 x=1 1/2 1/2 PGM概率图有这么几种推断方式 概率图的任务通俗的讲有两个 Querying：给定或者不给定条件概率下，计算特定变量的概率称为：Inference Estimation：当模型某部分是未知的时候，通过数据 $D$ 来推断出未知的模型，这个称为：learning InferenceQuerying 任务通常来说有三种 Likelihood : Likelihood is calculated to get the conditional probability of a different subset of variables conditioned based on Evidence $\\mathbf{E} = \\{ X_{k+1},…,X_n \\}$, Evidence is the unknown variables, so we need eliminate the unsure variables to get the specific lieklihood. P(\\mathbf{e}) = \\sum_{x_1}\\cdots \\sum_{x_k} P(x_1,...,x_k,\\mathbf{e})Conditional Probability : The conditional probability distribution of some query nodes conditioned on anevidence. P(X|\\mathbf{e}) = \\frac{P(X,\\mathbf{e})}{P(\\mathbf{e})}= \\frac{P(X,\\mathbf{e})}{\\sum_{x}P(X=x,\\mathbf{e})}Let $\\mathbf{Y}$ be a subset of all domain variables $\\mathbf{X} = \\{ \\mathbf{Y},\\mathbf{Z} \\}$, $\\mathbf{Z}$ is the set of variables under elimination. P(\\mathbf{Y}|\\mathbf{e}) = \\sum_{z}P(\\mathbf{Y},\\mathbf{Z}=z|\\mathbf{e})Most Probable Assignment : In this query, we are interested in finding only one set of values for the query variables that maximize the given conditional probability instead of finding the entire distribution. MPA(\\mathbf{Y}|\\mathbf{e})=\\mathop{arg}\\mathop{max}_{y\\in \\mathbf{Y}}P(y|\\mathbf{e})=\\mathop{arg}\\mathop{max}_{y\\in \\mathbf{Y}}\\sum_{z} P(y,z|\\mathbf{e})Inference 算法可以分为两种 Exact inference（准确推断） Elimination Message-passing sum-product belief propagation Junction Tree Approximate inference（近似推断）a Variational algorithms Loopy belief propagation Mean field approximation 一般 Inference 最简单的方法是 Elimination 和 brute force，但是由于概率图模型往往有一些比较特别的结构，我们可以用 Message Passing 的算法来做 Learning根据观测数据，来推理模型中的参数 在做 inference 的时候，我们希望能将有向图和无向图结合起来，比如，原先的子节点的父节点们，我们希望将他们","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process (Conjugacy & Bayesian)","slug":"Stochastic-Process-7","date":"2018-07-31T02:35:18.000Z","updated":"2018-10-10T07:19:40.463Z","comments":true,"path":"2018/07/31/Stochastic-Process-7/","link":"","permalink":"http://yoursite.com/2018/07/31/Stochastic-Process-7/","excerpt":"","text":"Beta-Binomial DistributionDefinition (Beta Distribution)An r.v. $X$ is said to have Beta distribution with parameters $a$ and $b$, if its PDF is f(x) = \\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1},0","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process (Multivariate)","slug":"Stochastic-Process-6","date":"2018-07-29T00:41:09.000Z","updated":"2018-10-10T07:19:44.403Z","comments":true,"path":"2018/07/29/Stochastic-Process-6/","link":"","permalink":"http://yoursite.com/2018/07/29/Stochastic-Process-6/","excerpt":"","text":"Discrete Multivariate R.V.sDefinition (Joint CDF) The Joint CDf of r.v.s $X$ and $Y$ is the function $F_{X,Y}$ given by F_{X,Y}(x,y) = P(X\\leq x,Y\\leq y)Definition (Joint PMF) The Joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X,Y}$ given by p_{X,Y}(x,y) = P(X=x,Y=y)Definition (Marginal PMF) For discrete r.v.s $X$ and $Y$, Marginal PMF of $X$ is P(X=x) = \\sum_y P(X=x,Y=y)Definition (Conditional PMF) For discrete r.v.s $X$ and $Y$, the Conditional PMF of $X$ given $Y=y$ is P_{X|Y}(x|y) = P(X=x|Y=y)=\\frac{P(X=x,Y=y)}{P(Y=y)}Definition (Independence of Discrete R.V.s) Random variables $X$ and $Y$ are independent if for all x and y F_{X,Y}(x,y) = F_X(x) F_Y(y)for all x and y also equivalent to the condition P(Y=y|X=x) = P(Y=y) Continuous Multivariate R.V.sDefinition (Joint PDF) If $X$ and $Y$ are continuous with joint CDF $F_{X,Y}$ then f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)Definition (Marginal PDF) If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ then f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) dyDefinition (Conditional PDF) For continuous r.v.s. $X$ and $Y$ with joint PDF $f_{X,Y}$ the Conditional PDF of $Y$ given $X=x$ is f_{Y|X}(y|x)= \\frac{f_{X,Y}(x,y)}{f_X(x)}Definition (Independence of Continuous R.V.s) Random variables $X$ and $Y$ are independent if for all x and y F_{X,Y}(x,y) = F_X(x)F_Y(y)If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ f_{X,Y}(x,y) = f_X(x) f_Y(y)Theorem (2D LOTUS) Let g be a function from $R^2$ to $R$ If $X$ and $Y$ are discrete E(g(X,Y)) = \\sum_x \\sum_y g(x,y) P(X=x,Y=y)If $X$ and $Y$ are continuous E(g(X,Y)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{X,Y}(x,y) dxdyGeneral Bayes’ Rule Convariance and CorrelationCovariance Measure a tendency of two r.v.s $X\\&amp;Y$ to go up or down together Positive Covariance: $X$ go up, $Y$ tends go up Negative Covariance: $X$ go up, $Y$ tends go down Definition (Covariance) The covariance between r.v.s $X$ and $Y$ is Cov(X,Y) = E((X-EX)(Y-EY))=E(XY)-E(X)E(Y)Theorem (Uncorrelated) If $X$ and $Y$ are independent, then they are Uncorrelated($Cov(X,Y)=0$) Properties of Covariance $Cov(X,X) = Var(X)$ $Cov(X,Y) = Cov(Y,X)$ $Cov(X,c) = 0$ $Cov(a\\cdot X,Y) = a\\cdot Cov(X,Y)$ $Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$ $Cov(X+Y,W+Z) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$ $Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$ For n r.v.s $X_1,\\dotsb ,X_n$ Var(X_1+\\dotsb +X_n)=Var(X_a)+\\dotsb+Var(X_n)+2\\sum_{i x)\\dotsb P(X_n> x) \\\\ &=1-[1-F(x)]^n \\end{align}The result here can be rewrite as $\\sum_{k=1}^n\\left( \\begin{array}{c} n\\\\k \\end{array} \\right) F(x)^k (1-F(x))^{n-k}$ This result can be obtained by expand $[F(x) + 1 -F(x)]^n$ Finally, let’s consider more general case where $1&lt;j&lt;n, X_{(j)}\\leq x$, this means at least $j$ of $\\{X_i \\}$ fall to the left of $x$ Denote $N$ as the nunber of $X_i$ landing to the left of $x$. $X_i$ lands to the left of $x$ w.p. $P(X_i\\leq x) = F(x)$. Then $N\\sim Bin(n,F(x))$ P(X_{(j)}\\leq x) = P(N\\geq j=\\sum_{k=j}^n \\left( \\begin{array}{c} n\\\\k \\end{array} \\right) F(x)^k(1-F(x))^{n-k} Theorem (PDF of Order Statistic) Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with CDF $F$ and PDF $f$. Then the marginal PDF of $j^{th}$ order statistic $X_{(j)}$ is f_{X_{(j)}} (x) = n \\left( \\begin{array}{c} n-1\\\\j-1 \\end{array} \\right) f(x) F(x)^{j-1} (1-F(x))^{n-j}Theorem (Joint PDF) Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with PDF $f$, Then the joint PDF of all order statistics is f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n) = n! \\prod_{i=1}^n f(x_i), x_1","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Topology-(引论)","slug":"Topology-1","date":"2018-07-28T12:19:29.000Z","updated":"2018-10-10T07:19:54.104Z","comments":true,"path":"2018/07/28/Topology-1/","link":"","permalink":"http://yoursite.com/2018/07/28/Topology-1/","excerpt":"","text":"Eular 定理对于一个多面体 P，我们定义 v：定点数 e：棱边数 f：面数 Eular 定理：$v+f-e = 2$ 但是满足这个定理的多面体是有条件的： P 的任何两个顶点可以用一串棱相连接 反例：中空的立方体 P 上任意由直线段构成的圈，把 P 分割成两片 反例：螺帽柱状体 Eular 定理证明：我们首先来看看树形，这个在图论里面常常出现，树有个性质就是 $v-e=1$，我们可以尝试用一棵树 T 来表示一个多面体，表示的方法是，树中的点就是 P 中的点（树 T 中的点囊括了所有 P 的点），树中的边就是 P 中的棱（边只是一部分的棱哦）。 然后我们来构造 T 的一种对偶，称为 $\\Gamma$，$\\Gamma$ 也是一颗树后面会证明，只不过这棵树的点由 P 中面的中心点来表示（也就是用来表示面的数量），这样面与面之间的边在多面体中是可以有一个曲折的，可以想象一下。。 上面采用这个形式只是因为这样构造能囊括所有的面，下面来证明一下这个 $\\Gamma$ 是树，而且曲折所在的棱刚好是 T 的边对于 P 中棱的补集： 连通性：如果 $\\Gamma$ 的某两个顶点不能用 $\\Gamma$ 内的一串棱连接，则它们必然被一个圈分开。由于 T 不含任何圈，$\\Gamma$ 必然联通。 无圈：如果 $\\Gamma$ 有圈，那么就会把顶点分开成两份，T 中的棱想要连接所有顶点就不可避免的要触碰到这个圈，所以 $\\Gamma$ 无圈 $T,\\Gamma$ 包含所有棱：假设一条棱没有被用着，这个棱本可以这样被用：棱两侧的面中点相连（$\\Gamma$），或者棱两端的点相连（T），但是都没用着，这样 $\\Gamma ,T$ 就会在后面相交。。（这是我的数学直觉，书上并没有这个的证明，我自己补的。。。不是很严谨。。。） 最后我们有 $v(T) - e(T) = 1$, $v(\\Gamma) - e(\\Gamma) = 1$, 加起来有 v(T) - [e(T)+e(\\Gamma)] + v(\\Gamma) = 2同时，根据构造有 v(T) = v, e(T) + e(\\Gamma)+e, v(\\Gamma) = f其他的证明方式可以用数学归纳法拓扑等价我们考虑一个正四面体未冲气的气球，我们把它吹胖，吹成了一个圆形。 这样多面体的点和球面的点之间的对应就是拓扑等价或同胚的一个例子，确切的说就是一对一的连续满映射","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process (Generating Function)","slug":"Stochastic-Process-5","date":"2018-07-27T02:23:34.000Z","updated":"2018-10-10T07:19:47.295Z","comments":true,"path":"2018/07/27/Stochastic-Process-5/","link":"","permalink":"http://yoursite.com/2018/07/27/Stochastic-Process-5/","excerpt":"","text":"Generating FunctionThree kinds of generating functions Probability Generating Function (PGF) : related to Z-transform Moment Generating Function (MGF) : related to Laplace transform Characteristic Function (CF) : related to Fourier transform Motivation PGF: handling non-negative integral random variables MGF: handling general random variables CF: equally useful with MGF Application Easy to characterizing the distribution of the sum of independent random variables Play a central role in the study of branching processes Provide a bridge between complex analysis and probability …… Moment Generating FunctionDefinition (Moment Generating Function) MGF of an r.v. $X$ is $M(t) = E(e^{tX})$, as a function of $t$ (different t denote different valued moment) and this must finite on some open interval (-a,a) containing 0 or dont exist. Why we need MGF MGF encodes the moments of an r.v. MGF of an r.v. Determines its distribution, like CDF and PMF/PDF MFG make it easy to find the distribution of a sum of i.r.v.s. Theorem (Moments via Derivatives of the MGF) $E(X^n) = M^{(n)}(0)$Using Taylor expansion of $M(t)$ at 0 M(t) = \\sum_{n=0}^{\\infty} M^{(n)}(0) \\frac{t^n}{n!}Using Taylor expansion of $E(X)$ M(t) = E(e^{tX}) = E\\left( \\sum_{n=0}^{\\infty} X^n \\frac{t^n}{n!} \\right)=\\sum_{n=0}^{\\infty} E(X^n) \\frac{t^n}{n!}Matching the coefficients of two expansions, we get $E(X^n) = M^{(n)}(0)$ MGF of DistributionTheorem (MGF Determines the Distribution) Two r.v. have the same MGF have the same distribution, more strictly, if there is even a tiny interval containing 0 on which the MGF are equal, the the r.v.s must have same distribution. Example 1 (Bernoulli MGF) MGF of $X\\sim Bern(p)$$e^{tX}=e^t$ with probability $p$, and $1$ with probability $q$, so $M(t) = E(e^{tX})=pe^t + q$ Example 2 (Geometric MGF) MGF of $X\\sim Geom(p)$ M(t) = E(e^{tX})=\\sum_{k=0}^{\\infty} e^{tk}q^kp=p\\sum_{k=0}^{\\infty} (qe^t)^k=\\frac{p}{1-qe^t}t in $(-\\infty, log(1/p))$ Example 3 (Uniform MGF) MGF of $U\\sim Unif(a,b)$ M(t) = E(e^{tU}) = \\int_a^b e^{tu}\\frac{1}{b-a} du = \\frac{e^{tb}-e^{ta}}{t(b-a)}and $M(0) = 1$ Example 4 (Binomial MGF) $Bin(n,p)$ M(t) = (pe^t+q)^nExample 5 (Negative Binomial) $NBin(r,p)$ M(t) = \\left( \\frac{p}{1-qe^t} \\right)^rTheorem (MGF of Location-Scale Transformation) If $X$ has MGF $M(t)$, then MGF of $a+bX$ is E(e^{t(a+bX)})=e^{at}E(e^{btX})=e^{at}M(bt)Example 6 (Normal MGF) MGF of $(X = \\mu + \\sigma Z) \\sim N(\\mu,\\sigma^2)$ M_Z(t) = E(e^{tZ})=\\int_{-\\infty}^{\\infty}e^{tz}\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}dz=e^{t^2/2}Use the Theorem above then M_X(t) = e^{\\mu t}M_Z(\\sigma t) = e^{\\mu t}e^{(\\sigma t)^2/2} = e^{\\mu t + \\frac{1}{2} \\sigma^2 t^2}Sum of Independent DistributionsTheorem (MGF of A Sum of Independent R.V.s) If $X$ and $Y$ are independent, Then M_{X+Y} (t) = M_X(t) M_Y(t)Example 1 (Sum of Poissons) $X\\sim Pois(\\lambda), Y\\sim Pois(\\mu)$, $X$ and $Y$ are independent. Then $X+Y \\sim Pois(\\lambda + \\mu)$The MGF of $X$ is E(e^{tX}) = \\sum_{k=0}^{\\infty}e^{tk}\\frac{e^{-\\lambda}\\lambda^k}{k!}=e^{-\\lambda}\\sum_{k=0}^{\\infty}\\frac{(\\lambda e^t)^k}{k!}=e^{-\\lambda}e^{\\lambda e^t}=e^{\\lambda(e^t-1)}The MGF of $X+Y$ is E(e^{tX})E(e^{tY}) = e^{\\lambda (e^t-1)} e^{\\mu (e^t-1)} = e^{(\\lambda + \\mu)(e^t-1)}Which is the $Pois(\\lambda + \\mu)$, so $X+Y\\sum Pois(\\lambda+\\mu)$ Example 2 (Sum of Normals) $X_1\\sim N(\\mu_1,\\sigma_1^2)$ and $X_2 \\sim N(\\mu_2,\\sigma_2^2)$, $X_1+X_2 = ?$MGF of $X_1+X_2$ is M_{X_1+X_2}(t)= M_{X_1}(t)M_{X_2}(t)= e^{\\mu_1t+\\frac{1}{2}\\sigma^2_1t^2}\\cdot e^{\\mu_2 t+\\frac{1}{2}\\sigma_2^2 t^2}=e^{(\\mu_1+\\mu_2)t+\\frac{1}{2}(\\sigma_1^2+\\sigma_2^2)t^2}Which is the N(\\mu_1 + \\mu_2, \\sigma_2^2 + \\sigma_1^2) MGF. Probability Generating FunctionDefinition (PGF) PGF of a nonnegative integer-valued r.v. $X$ with PMF $p_k = P(X=k)$ is the generating function of the PMF, By LOTUS , this is E(t^X) = \\sum_{k=0}^{\\infty} p_k t^kExample 1 (Generating Dice Probabilities) Let $X$ be the sum from rolling 6 pair dice, $X_1,…,X_6$ be the individual rolls, what is $P(X=18)$ ?The PGF of $X_1$ is E(t^{X_1}) = \\frac{1}{6}(t+t^2+\\dotsb+t^6)The PGF of $X$ is E(t^X) = E(t^{X_1}\\dotsb t^{X_6}) = E(t^{X_1})\\dotsb E(t^{X_6})=\\frac{t^6}{6^6}(1+t+\\dotsb +t^5)^6The coefficient of $t^{18}$ in the PGF is $P(X=18)$, so P(X=18) = \\frac{3421}{6^6}Theorem (PMF \\&amp; PGF) P(X=k) = \\frac{g_{X}^{(k)}(0)}{k!} Characteristic FunctionDefinition CF The Characteristic function of a random variable $X$ is the function $\\phi : R \\rightarrow C$ defined by \\phi(t) = E(e^{itX}), i = \\sqrt{-1}","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process (Continuous Random Variable)","slug":"Stochastic-Process-4","date":"2018-07-25T12:04:57.000Z","updated":"2018-10-10T07:19:24.045Z","comments":true,"path":"2018/07/25/Stochastic-Process-4/","link":"","permalink":"http://yoursite.com/2018/07/25/Stochastic-Process-4/","excerpt":"","text":"Probability Density FunctionDefinition (Probability Density Function): For a continuous r.v. $X$ with CDF $F$, the probability density function (PDF) of $X$ is the derivative $f$ of the $F$ P(a\\leq X \\leq b) = \\int_a^b f_X(x)dx = F(b) - F(a)P(X\\in [x,x+\\delta]) \\approx f_X(x) \\cdot \\deltaRelation between PDF &amp; PMF: The PDF is the analogous to the PMF in many ways. But, the PDF $f(x)$ is not a probability. Relation between PDF &amp; CDF: Let $X$ be a continuous r.v. with PDF $f$. Then the CDF of $X$ is given by F(x) = \\int_{-\\infty}^x f(x) dtCDF of Logistic Distribution F(x) = \\frac{e^x}{1+e^x}, x\\in RCDF of Rayleigh Distribution F(x) = 1 - e^{-x^2/2}, x>0Theorem (Expectation of Continuous R.V.) E(X) = \\int_{-\\infty}^{\\infty} x f(x) dxTheorem (Expectation via Survial Function) Let $G$ be the survial function of $X$, Then E(X) = \\int_0^{\\infty} G(x) dxTheorem (LOTUS: Continuous) E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f(x) dx Uniform DistributionDefinition (Uniform Distribution) Distribution on the interval$(a,b)$, and its PDF is f(x) = \\left \\{ \\begin{array}{ll} \\frac{1}{b-a} & if\\ ax) \\\\ &= P(X_1>x,...,X_n>x)\\\\ &= \\prod_{i=1}^n P(X_i > x) = (1-x)^n \\end{align}From above, we use the survial function to calculate the expectation E(Y) = \\int_0^{\\infty} P(Y>x) dx = \\int_0^1(1-x)^ndx = \\int_0^1 x^n dx = \\frac{1}{n+1} NormalDefinition (Standard Normal Distribution) A c.r.v. $Z$ is said to have the standard Normal Distribution if its $PDF$ $\\varphi$ is given by: \\varphi(z) = \\frac{1}{\\sqrt(2\\pi)} e^{-z^2/2},-\\infty < z < \\inftyWe write this as $Z\\sim N(0,1)$, and $Z$ has mean 0 and variance 1, the CDF $\\phi$ is \\phi(z) = \\int_{-\\infty}^z \\varphi(t) dt = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}}e^{-t^2/2}dtDefinition (Normal Distribution) If $Z\\sim N(0,1)$ then X = \\mu + \\sigma Zis said to have the Normal Distribution with mean $\\mu$ and variance $\\sigma^2$, denote this by $X\\sim N(\\mu,\\sigma^2)$ Theorem (Normal CDF and PDF) Let $X\\sim N(\\mu, \\sigma^2)$,CDF of $X$ is F(x) = \\phi(\\frac{x-\\mu}{\\sigma})PDF of $X$ is f(x) = \\varphi (\\frac{x-\\mu}{\\sigma})\\frac{1}{\\sigma} ExponentialDefinition (Exponential Distribution) f(x) = \\lambda e ^{-\\lambda x}, x>0we denote this by $X\\sim Expo(\\lambda)$. The corresponding CDF is F(x) = 1-e^{-\\lambda x},x>0Theorem (Memoryless Property) P(X\\geq s+t | X\\geq s) = P(X\\geq t) If $X$ is a positive continuous random variable with memoryless property, then $X$ has an Exponential distribution Geometric Distribution is also Memoryless Exponential distribution as the “continuous counterpart” of the Geometric distribution Exponential \\&amp; Geometric via $\\delta$- StepsWe devide a unit of time into n pieces, each of size $\\delta = \\frac{1}{n}$, and the trial occurs every $\\delta$ time period and success with probability $\\lambda \\delta$. Denote $Y$ as the number of trials until first success, $\\hat{Y}$ as the time until first success under $Y$. Y\\sim FS(\\lambda \\delta)Thus we have F(\\hat{Y}) = E(Y)\\cdot \\delta = \\frac{1}{\\lambda\\delta}\\cdot \\delta=\\frac{1}{\\lambda}And \\begin{array} P(Y>t) &= P\\{ all\\ trials\\ up\\ to\\ time\\ t\\ has\\ been\\ failures \\} \\\\ &=P\\{ at\\ least\\ \\frac{t}{\\delta}\\ failures\\}\\\\ &=(1-p)^{\\frac{t}{\\delta}}=(1-\\lambda \\delta)^{\\frac{t}{\\delta}}\\\\ &=\\left[ (1-\\lambda\\delta)^{\\frac{1}{\\lambda\\delta}} \\right]^{\\lambda t} \\xrightarrow{\\delta \\rightarrow 0} e^{-\\lambda t} \\end{array}Theorem property of Exponential Given $X_1\\sim Expo(\\lambda_1)$, $X_2\\sim Expo(\\lambda_2)$, $X_1 \\bot X_2$ ($X_1$ and $X_2$ are independent), then P(X_1","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Generative-Adversarial-Nets","slug":"Generative-Adversarial-Nets","date":"2018-07-25T11:21:44.000Z","updated":"2018-10-10T07:23:09.465Z","comments":true,"path":"2018/07/25/Generative-Adversarial-Nets/","link":"","permalink":"http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/","excerpt":"","text":"$x$: Data we already have $z$: Data we want to generate $\\theta_g$: parameter for Generator $\\theta_d$: parameter for D $G$: Generator $D$: Discriminator target is training $G$ to minimize \\mathop{min}_G \\mathop{max}_D V(D,G) = E_{x\\sim p_{data(x)}}[\\mathop{log}D(x)] + E_{z\\sim p_z(z)}[\\mathop{log}(1-D(G(z)))]For player $D$, want V bigger by making more accurate estimate on real date x as $D(x)\\rightarrow 1$ discriminate the fake data by $D(G(z)) \\rightarrow 0$ equals to $(1 - D(G(z))) \\rightarrow 1$ more accurate the $D$ is ,the larger value of $V$ can be For player $G$, want V smaller by enlarge $D(G(z))\\rightarrow 1$ for $(1 - D(G(z))) \\rightarrow 0$ better fake of $G$, $G(z)\\rightarrow 1$, and less value of $V$ Algorithm for number of iterations do for $k$ steps do Sample $m$ noise samples $\\{ z^{(1)},…,z^{(m)} \\}$ from noise prior $p_g(z)$ Sample $m$ examples $\\{ x^{(1)},…,x^{(m)} \\}$ from data generating distribution $p_{data}(x)$ Update the discriminator by ascending its stochastic gradient :\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\left[ \\mathop{log}D(x^{(i)})+log\\left(1-D(G(z^{(i)}))\\right) \\right] end for Sample $m$ noise samples $\\{ z^{(1)},…,z^{(m)} \\}$ from noise prior $p_g(z)$ Update the generator by descending its stochastic gradient :\\nabla_{\\theta_g} \\frac{1}{m}\\sum_{i=1}^m\\mathop{log}\\left( 1-D(G(z^{(i)})) \\right) end for ew - q - q","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"http://yoursite.com/tags/DL/"}]},{"title":"Stochastic Process (Expectation)","slug":"Stochastic-Process-3","date":"2018-07-22T02:30:26.000Z","updated":"2018-10-10T07:19:20.159Z","comments":true,"path":"2018/07/22/Stochastic-Process-3/","link":"","permalink":"http://yoursite.com/2018/07/22/Stochastic-Process-3/","excerpt":"","text":"ExpectationDefinition (Expectation of R.V.) E(X) = \\sum_{x}\\underbrace{x}_{value} \\underbrace{P(X=x)}_{PMF\\ at\\ x}Theorem (Monotonicity): $X$ and $Y$ are r.v.s. such that $X&gt;Y$ with probability 1.Then $E(X)\\geq E(Y)$ Theorem (Expectation via Survial Function): Let $X$ be a nonnegative r.v. Let $F$ be the CDF of $X$, and define survial function of $X$ named $G$ as $G(x) = 1-F(x) = P(X&gt;x)$, Then E(x) = \\sum_{n=0}^{\\infty} G(x)Theorem (Low Of The Unconscious Statistician(LOTUS)): If $X$ is discrete r.v. and $g$ is a function from $R$ to $R$, then E(g(x)) = \\sum_x g(x)P(X=x)Propertise of Expectation $E(X+Y) = E(X) + E(Y)$ $E(cX) = c E(x)$ If $X$ and $Y$ are independent, $E(XY) = E(X) E(Y)$ Inequalities of Expectation Cauchy–Bunyakovsky–Schwarz inequalityE[XY]^2\\leq E[X^2] E[Y^2] https://en.wikipedia.org/wiki/Expected_value#Inequalities … VarianceDefinition (Variance and Standard Deviation) variance of an r.v. $X$ is Var(X) = E(X-EX)^2Square root of the variance is standard deviation (SD): SD(X) = \\sqrt{Var(X)}Propertise of Variance For any r.v. $X$, $Var(X) = E(X^2) - (EX)^2$ $Var(X + c ) = Var(X)$ $Var(c X ) = c^2Var(X)$ If $X$ and $Y$ are independent, then $Var(X+Y) = Var(X) + Var(Y)$ Geometric and Negative BinomialDefinition (Geometric Distribution): Consider a sequence of independent Bernoulli trials, each with the same success probability $p\\in (0,1)$, trails performed until a success occurs. Let $X$ be the number of the failures before the first successful trail. Then $X$ has the Geometric Distributions, denote by $X\\sim Geom(p)$ Theorem (Geometric PMF): If $X\\sim Geom(p)$, then the PMF of $X$ is P(X=k) = (1-p)^kpTheorem (Memoryless Property): If $X\\sim Geom(p)$, then for positive integer n P(X\\geq n+k | X \\geq k ) = P(X\\geq n)Definition (First Success Distribution): very similay to Geometric $X$, Let it be $Y$, and $X+1 = Y$ …. , we denote it by $FS(p)$ Definition (Negative Binomial Distribution): In a sequence of independent Bernoulli trails with p, if $X$ is the number of failures before $r^{th}$ success, then $X$ is the Negative Binomial Distribution with $r$ and $p$, denoted by $X\\sim NBin(r, p)$ Theorem (Negative Binomial PMF): If $X\\sim NBin(r,p)$, then the PMF of $X$ is P(X=n) = \\left ( \\begin{array}{c} n+r-1 \\\\ r-1 \\end{array} \\right )p^r(1-p)^nTheorem (Geometric &amp; Negative Binomial): Let $X\\sim NBin(r,p)$, and $X_i$ are $i.i.d. Geom(p)$ , Then we have $X= X_1+\\dotsb + X_r$ Indicator R.V.Definition (Indicator R.V.) I_A = \\left \\{ \\begin{array}{ll} 1 & if\\ A\\ occurs \\\\ 0 & otherwise \\end{array} \\right .Propertise of Indicator R.V. $(I_A)^k = I_A$ $I_{A^c} = 1- I_A$ $I_{A\\cap B} = I_A I_B$ $I_{A\\cup B} = I_A + I_B - I_A I_B$ Theorem (Bridge between Probability &amp; Expectation) P(A) = E(I_A)Example 1: Au urn contain R G B three balls, r g b is probability draw a ball from it (r+g+b = 1), whats the expected number of different colors of ball before getting the first R ball ?Let $I_g$ be the $1$ if G is obtained before R, and define the $I_b$ similarly. Then E(I_g) = P(green\\ before\\ red) = \\frac{g}{g+r}since “green before red” means that first non-blue ball is green , so probability is $frac{g}{g+r}$, then, the final result is E(I_g+I_b) = \\frac{g}{g+r} + \\frac{b}{b+r}Moments &amp; IndicatorsGiven n events $A_1,\\dotsb, A_n$ and indicators $I_j, j = 1, \\dotsb, n$ $X = \\sum_{j=1}^n I_j$: the number of events occur \\left( \\begin{array}{c} X \\\\ 2 \\end{array} \\right) = \\sum_{i","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Neural Style","slug":"Neural-Style","date":"2018-07-19T02:25:16.000Z","updated":"2018-10-10T07:21:23.126Z","comments":true,"path":"2018/07/19/Neural-Style/","link":"","permalink":"http://yoursite.com/2018/07/19/Neural-Style/","excerpt":"","text":"Nerual StyleThere are two aspect for a image, one is the content of the image, which can be descriped as elements or object in the image, another is the style of the image, it might be abstract, and usually revealed by the painting skill or technique. Shortly, we have two image, one for style while the other for content. now, we want to combine the style in image1 and the content in image2 together, and it can be achieved from deep neural net work, and we call it Neural Style. Moreover we simply define the loss function care both style and content L_{total} = \\alpha L_{content}+\\beta L_{style}Now let’s have a look about what neural network can do here, and analysis the affect of $L_{content}$ and $L_{style}$ independently. Suppose we have the content image, and send it to the neural network, it will have the responses in each layer by filters, we also construct a white noisy image, filter it in the same way, and define a loss $L_{content}$ between filtered content and filtered noisy, we take the noisy image as input,and it can update iterativly. The image above show the reconstruction result between different layers, and reconstruction from lower layers(a,b,c) is alomost perfect, the style reconstruction may be more realistic in the deeper layer. Let’s get familiar with some notion of the formulation first( suppse we are in the $l^{th}$ level of the net ): $\\vec{p}$: Original content image (input) $P^l$: Content feature representation in layer $l$ respect to $\\vec{p}$ $\\vec{a}$: Original style image (input) $A^l$: Style feature representation in layer $l$ respect to $\\vec{a}$ $\\vec{x}$: Target image (output) $F^l$: Content feature representation in layer $l$ respect to $\\vec{x}$ $G^l$: Style feature representation in layer $l$ respect to $\\vec{x}$ $F_{ij}^l$: Element of $i^{th}$ filter at $j^{th}$ position in layer $l$ $N_l$: The number of the filters in the $l^{th}$ level $M_l$: The size of a feature map produced by a filter,usually it equals to $height \\times weight$ The squared-error loss between two content feature representations is: L_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2} \\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2In each layer, build a style representation compute the correlations between the different filter responses, which is called Gram Matrix $G^l\\in R^{N_l \\times N_l}$, and $G_{ij}^l$ is the inner product between the vectorized feature map between $i$ and $j$ in layer $l$ G_{ij}^l = \\sum_k F_{ik}^l F_{jk}^lAlso we have A_{ij}^l = \\sum_k P_{ik}^l P_{jk}^lThe contribution of the layer to the total loss is E_l = \\frac{1}{4N_l^2 M_l^2 }\\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2And the total loss is L_{style}(\\vec{a},\\vec{x}) = \\sum_{l=0}^L w_lE_lLet’s focus more on the detail about the gradient of the loss: The derivative of content loss respect to activations in layer l equals \\frac{\\partial L_{content}}{\\partial F_{ij}^l} = \\left \\{ \\begin{array}{ll} (F^l - P^l)_{ij} & if\\ F_{ij}^l > 0 \\\\ 0 & if\\ F_{ij}^l < 0 \\end{array} \\right .The derivative of style loss respect to activations in layer l equals \\frac{\\partial E_l}{\\partial F_{ij}^l} = \\left \\{ \\begin{array}{ll} \\frac{1}{N_l^2M_l^2}((F^l)^T(G^l - A^l))_{ij}& if\\ F_{ij}^l > 0 \\\\ 0 & if\\ F_{ij}^l < 0 \\end{array} \\right .The final loss function we want to minimize is L_{total}(\\vec{p},\\vec{a},\\vec{x}) = \\alpha L_{content}(\\vec{p},\\vec{x}) + \\beta L_{style}(\\vec{a},\\vec{x}) Fast Neural Style FastNet","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"}]},{"title":"Stochastic Process (Random Variable)","slug":"Stochastic-Process-1","date":"2018-07-07T00:21:04.000Z","updated":"2018-10-10T07:19:13.105Z","comments":true,"path":"2018/07/07/Stochastic-Process-1/","link":"","permalink":"http://yoursite.com/2018/07/07/Stochastic-Process-1/","excerpt":"","text":"There are some notation occationals Definition (Discrete Random Variable) A variable $X$ is discrete if there is a finite list of value $a_1,a_2,…,a_n$ that $P(X=a_j) = 1$, $P(X=x)&gt;0$ is the support of $X$ Definition (Probability Mass Function) The probability mass function (PMF) of a discrete r.v. $X$ is the function $p_X$ given by $p_X(x) = P(X=x)$. Bernoulli &amp; BinomialDefinition (Bernoulli Distribution) shortly, $P(X=1)=p$ and $P(X=0) = 1 - p$, and write as $X\\sim Bern(p)$ Definition (Indicator Random Variable) The indicator random variable of an event $A$ is the r.v. equals 1 if $A$ occurs and 0 otherwise, We denote the indicator of $A$ by $I_A$ or $I(A)$. Note $I_A \\sim Bern(p)$ with p = P(A) Theorem (Binomial PMF) Binomial Distribution is the repeatation of Bernoulli Distribution. If $X\\sim Bin(n, p)$ then the PMF of X is P(X=k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k}Hypergeometricurn Model A box is fiiled with $w$ white and $b$ black balls, then drawing n balls With replacement: $Bin(n,w/(w+b))$ for the number of white balls Without replacement : Hypergeometric distribution $HGeom(w,b,n)$ Theorem (Hypergeometric PMF) If $X \\sim HGeom(w,b,n)$, then the PMF of $X$ is P(X=k) = \\frac {\\left ( \\begin{array}{c} w \\\\ k \\end{array} \\right ) \\left( \\begin{array}{c} b \\\\ n-k \\end{array} \\right)} {\\left( \\begin{array}{c} w+b \\\\ n \\end{array} \\right)}Zipf Distribution If $X\\sim Zipf(\\alpha &gt; 0)$, then PMF of $X$ is: P(X=k) = \\frac{\\frac{1}{k^{\\alpha + 1}}} {\\sum_{j=1}^{\\infty}(\\frac{1}{j})^{\\alpha + 1}} Zipf Distribution can measure the Word Frequency Cumulative Distribution FunctionsDefinition (Cumulative Distribution Function) The cumulative distribution function(CDF) os an r.v. $X$ is the function $F_X$ given by $F_X(x) = P(X\\leq x)$ Theorem (Valid CDFs) CDF has the following properties Increasing: If $x_1 &lt; x_2$, then $F(x_1) &lt; F(x_2)$ Right-Continuous: $F(a) = lim_{x\\rightarrow a^+} F(x)$ Convergence to $0$ and $1$: $lim_{x\\rightarrow - \\infty} F(x) = 0$ and $lim_{x \\rightarrow \\infty} F(x) = 1$ Functions of Random Variable:Definition (Function of an r.v.) An experiment with sample space S, an r.v. $X$, and a function $g$, also the $g(X)$ is the variable that maps $s$ to $g(X(s))$, for all $s\\in S$ Theorem (PMF of $g(X)$) for all y in the support of $g(X)$ P(g(X) = y) = \\sum_{x:g(x)=y} P(X=x)The function of r.v. map the sample space into real number, which is easy for us calculate in mathematic. Independence of R.V.sDefinition (Independence of two R.V.s) Random variables $X$ and $Y$ are said to be independent P(X\\leq x,Y\\leq y) = P(X\\leq x)P(Y\\leq y)for all $x,y\\in R$,In the discrete case, equivalent to : P(X=x,Y=y) = P(X=x)P(Y=y)Definition (Independence of many R.V.s) Random variables $X_1,…,X_n$ are independent if P(X_1 \\leq x_1,\\dotsb , X_n \\leq x_n) = P(X_1 \\leq x_1) \\dotsb P(X_n \\leq x_n)for all $x_1,\\dotsb,x_n \\in R$ Definition (i.i.d) We call some r.v. that are independent and have the same distribution independent and identicallly distributed or i.i.d for short Independent: r.v.s provide no information about each others Identically distributed: r.v.s have the same PMF Theorem If $X\\sim Bin(n,p)$ , $Y \\sim Bin(m,p)$, and $X$ is independent of $Y$, then $X+Y \\sim Bin(n+m,p)$ Definition (Conditional Independence of two R.V.s) P(X\\leq x, Y \\leq y| Z= z) = P(X\\leq x |Z =z) P(Y\\leq y|Z=z)w","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic Process (Conditional Probability)","slug":"Stochastic-Process-2","date":"2018-07-06T00:24:34.000Z","updated":"2018-10-10T07:19:16.625Z","comments":true,"path":"2018/07/06/Stochastic-Process-2/","link":"","permalink":"http://yoursite.com/2018/07/06/Stochastic-Process-2/","excerpt":"","text":"Defination of Conditonal ProbabilityDefination Two events $A$ and $B$, with $P(B)&gt;0$, the conditional probability of $A$ given $B$ , denoted by $P(A|B)$, is defined as : P(A|B)=\\frac{P(AB)}{P(B)} $P(A)$: prior probability $P(A|B)$: posterior probability Bayes’ Rule &amp; LOTPChain Rule chain rule P(A_1,...,A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1,...,A_{n-1})Bayes’ RuleP(A|B) = \\frac{P(B|A)P(A)}{P(B)}LOTP (Law of Total Probability) Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_1)&gt;0$, Then : P(B) = \\sum_i^n P(B|A_i)P(A_i)Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, for any event $B$ such that $P(B) &gt; 0$, we have : P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\\dotsb+P(A_n)P(B|A_n)} Conditional ProbabilitiyConditional Probability is also the probability, so it inherent the property of probability (suppose the sample space is $S$): $P(S|E) = 1$ and $P(\\emptyset|E) = 0$ if events $A_1,…$ are disjoint, then $P(\\cup_{j=1}^{\\infty}A_j|E) = \\sum_{j=1}^{\\infty}P(A_j|E)$ $P(A^c|E) = 1 - P(A|E)$ Inclusion-Exclusion : $P(A\\cup B|E) = P(A|E) + P(B|E) - P(A\\cap B|E)$ Bayes’ Rule with Extra Condition:Theorem: Provided that $P(A\\cap E)&gt;0$ and $P(B\\cap E)&gt;0$, we have: P(A|B,E) = \\frac{P(B|A,E)P(A|E)}{P(B|E)}LOTP with Extra Condition:Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_i \\cap E) &gt;0$, Then: P(B|E) = \\sum_{i=1}^n P(B|A_i,E)P(A_i|E)Approaches for $P(A|B,C)$ P(A|B,C) = \\frac{P(A,B,C)}{P(B,C)} P(A|B,C) = \\frac{P(B|A,C)P(A|C)}{P(B|C)} P(A|B,C) = \\frac{P(C|A,B)P(A|B)}{P(A|C)} Independence of EventsIndependence of Two EventsDefination: Events $A$ and $B$ are independent if P(A\\cap B) = P(A) P(B) \\Leftrightarrow P(A|B) = P(A) , P(B|A) = P(B)Independence vs Disjointness $A,B$ is disjoint : $P(A\\cap B) = 0$ $A,B$ is independent : $P(A) = 0, P(B) = 0$ Conditional IndependenceDefination: Events $A$ and $B$ are conditionally independent given E if: P(A\\cap B|E) = P(A|E)P(B|E)Contitional\\ Independence \\nRightarrow IndependenceIndependence \\nRightarrow Contitional\\ Independence","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Robust PCA","slug":"Robust-PCA","date":"2018-07-04T14:47:11.000Z","updated":"2018-10-10T07:18:58.281Z","comments":true,"path":"2018/07/04/Robust-PCA/","link":"","permalink":"http://yoursite.com/2018/07/04/Robust-PCA/","excerpt":"","text":"Introduction to RPCAThe data we collect usually have the low rank property, but the property will vanished when the data is collected causing the noisy, but we can still decomposite the matrix into low-rank matrix and spares error matrix from the corruped data. D=\\underbrace{A}_{\\text{low rank matrix}}+\\underbrace{E}_{\\text{sparse matrix}} \\notagTraditional approach for solving this problem is using PCA (Principal Components Analysis), there are many interpretation to PCA, one relate to rank is despiting the low value singular value as this componets contribute less to the data. Thus, it can be considered as the noisy. So we take the $k^{th}$ largest singular value and drop the rest , this can be represnt as the following formulation : \\mathop{min}_{A,E} \\|E \\|_F, \\ \\ \\ \\ \\text{subject to } \\ rank(A)\\leq r, D = A + E \\notagPCA \bhas a shortage that it is not robust to the outliers, then the RPCA (Robust Principal Components Analysis) came out, RPCA could making the matrix recovery whether the noisy is large or not only if the sparse property is confirmed, the original form of the RPCA can be written as : \\mathop{min}_{A,E} rank(A) + \\|E\\|_0, \\ \\ \\ \\ \\text{subject to } \\ D = A + E \\notagThe optimization formulation above is non-convex and is hard to get the solution, we can use the convex relax technology apply on it, then it turn out into the most used and the most efficient from: \\mathop{min}_{A,E} \\|A\\|_* + \\|E\\|_1, \\ \\ \\ \\ \\text{subject to } \\ D = A + E \\notag$| \\cdot |_*$ is the unclear norm, which is the sum of the all singular values : $\\sum_i^n\\sigma_i$, $l_1$ norm of matrix $| \\cdot |_1$ is the sum of absolute value of all the element : $\\sum_i^n \\sum_j^n |D_{ij}|$ . Algorithm of RPCABefore introducting the Algorithm, we first introducing the two operators Singular Value ThresholdingThe optimal solution to the optimization problem : $\\frac{1}{2} | X- Y |_F^2 + \\tau |X|_*$ with the variable $X$ is thresholing the singular value of $X$ \\begin{align} \\mathcal{D}_{\\tau}(X) := U \\mathcal{D}_{\\tau} (\\Sigma) V^{\\prime} , \\ \\ \\mathcal{D}_{\\tau}(\\Sigma) = diag ( \\{ \\sigma_i - \\tau \\} ) \\notag \\\\ \\mathcal{D}_{\\tau}(Y) = \\mathop{arg} \\mathop{min}_{X} \\left \\{ \\frac{1}{2} \\| X- Y \\|_F^2 + \\tau \\|X\\|_* \\right \\} \\notag \\end{align}Soft ThresholdingAs same as the $l_1$ norm in vector, thresholding the absolute value of all the element in $X$. \\begin{align} \\psi_{st}(Y) = \\mathop{arg} \\mathop{min}_{X} \\left \\{ \\frac{1}{2} \\| X- Y \\|_F^2 + \\tau \\|X\\|_1 \\right \\} \\notag \\end{align}There are various methods to solving the RPCA problem, the most successful one is slove the Augmented Lagrangian function of the original problem which we called ALM algorithm, the Augmented Lagrangian function is: \\begin{align} L(A,E,Y,\\mu) = \\|A\\|_* + \\lambda\\|E\\|_1+ \\langle Y,D-A-E \\rangle + \\frac{\\mu}{2} \\| D- A -E \\|_F^2 \\notag \\end{align}Usually, we use ADMM to slove the ALM problems : \\begin{align} A_{k+1} &= SVT_{1/\\mu_k}(D-E_k + \\mu_k^{-1} Y_k) \\notag \\\\ E_{k+1} &= ST_{\\lambda/\\mu_k} (D - A_{k+1} + \\mu_k^{-1} Y_k) \\notag \\\\ Y_{k+1} &= Y_k + \\mu_k ( D - A_{k+1} - E_{k+1} ) \\notag \\end{align}","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]},{"title":"Stochastic-Process （Abbreviate & Notation）","slug":"Stochastic-Process-0","date":"2018-07-02T08:54:34.000Z","updated":"2018-10-10T07:19:10.001Z","comments":true,"path":"2018/07/02/Stochastic-Process-0/","link":"","permalink":"http://yoursite.com/2018/07/02/Stochastic-Process-0/","excerpt":"","text":"r.v.s.: random variable sequence i.i.d: individual identical distribution w.p.: with probability PMF (Probability Mass Function) $P(X=x)$ CDF (Cumulative Distribution Function) $P(X \\leq x)$ PDF (Probability Density Function) derivate of CDF PGF (Probability Generating Function) $E(t^X) = \\sum_{k=0}^{\\infty} p_k t^k$ MGF (Moment Generating Function) $M(t) = E(e^{tX})$ Distributions Binomial Distribution $X\\sim B(n,p)$: number of success in n trails HyperGeometric Distribution $X\\sim HGeom(w,b,n)$: draw n balls between w white and b black Geometric Distribution $X\\sim Geom(p)$: number of the Bernoulli trails before success (First Success Distribution) Negative Binomial Distribution $X\\sim NBin(r,p)$: number of the Bernoulli trails before $r^{th}$ success Poisson Distribution $X\\sim Pois(\\lambda)$: number of times an event occurs in an interval of time or space Uniform Distribution $U\\sim Unif(a,b)$: Distribution on the interval $(a,b)$ Standard Normal Distribution $X\\sim N(0,1)$ Normal Distribution $X\\sim N(\\mu,\\sigma^2)$ Beta Distribution $X\\sim Beta(a,b)$ Multinomial Distribution $\\mathbf{X}\\sim Mult_k(n,\\mathbf{p})$","categories":[],"tags":[{"name":"MATH","slug":"MATH","permalink":"http://yoursite.com/tags/MATH/"}]}]}