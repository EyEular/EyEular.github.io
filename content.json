{"meta":{"title":"EyEular","subtitle":null,"description":null,"author":"Eulring","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-07-22T13:04:20.969Z","updated":"2018-07-22T13:04:20.969Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Indigo","date":"2018-07-23T03:30:34.473Z","updated":"2018-07-23T02:32:35.989Z","comments":true,"path":"custom/index.html","permalink":"http://yoursite.com/custom/index.html","excerpt":"","text":"Image image Blockquote 当blockquote、img、pre、figure为第一级内容时，在page布局中拥有card阴影，所有标题居中展示。 Content@card{ 目前的想法是预定义一系列内容模块，通过像输入 Markdown 标记一样来简单调用。好在 Markdown 没有把所有便于输入的符号占用，最终我定义了@moduleName{ ... }这种标记格式。如果你使用过Asp.Net MVC，一定会很熟悉这种用法，没错，就是razor。 page布局中的title和subtitle对应 Markdown 中的title和description。 基本的内容容器还是card，你可以这样使用card： 12345@card&#123;在`page`页中，建议把内容都放到`card`中。&#125; 需要注意的是：标记与内容之间必须空一行隔开。至于为何要这样，看到最后就明白了。 } Column@column-2{ @card{ 左与card标记类似，分栏的标记是这样的： 123456789101112131415@column-2&#123;@card&#123;# 左&#125;@card&#123;# 右&#125;&#125; 为了移动端观感，当屏幕宽度小于 480 时，column将换行显示。 } @card{ 右column中的每一列具有等宽、等高的特点，最多支持三栏： 123456789101112131415161718192021@column-3&#123;@card&#123;左&#125;@card&#123;中&#125;@card&#123;右&#125;&#125; } } Three columns@column-3{ @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } } Timeline@card{ 在timeline模块中，你的 5 号标题#####和六号标题######将被“征用”，用作时间线上的标记点： 123456789101112@timeline&#123;##### 2016@item&#123;###### 11月6日为 Card theme 添加 page layout。&#125;&#125; @item中多行内容可以换行输入，目前不允许隔行： 12345678910111213141516171819202122@timeline&#123;##### 2016@item&#123;###### 11月6日第一行 第二行 /* ok */&#125;@item&#123;###### 11月6日第一行第二行 /* error */&#125;&#125; } @timeline{ 2016@item{ 11月6日为 Card theme 添加 page layout。加快绿化空间好看 } @item{ 10月31日本地化多说。 } @item{ 10月24日为 Indigo 主题创建 Card 分支。 } 2015@item{ 2月24日发布 Indigo 主题到 hexo.io。 } @item{ 1月22日创建 Indigo 主题。 } } CodeBlock12345// 自定义内容块实现page.content.replace(/&lt;p&gt;&#125;&lt;\\/p&gt;/g, '&lt;/div&gt;') .replace(/&lt;p&gt;@([\\w-]+)&#123;&lt;\\/p&gt;/g, function(match, $1)&#123; return '&lt;div class=\"'+ $1 +'\"&gt;' &#125;) @card{ 这里可以解释，为什么标记之间必须要隔一行了。 当你在 Markdown 中隔行输入时，会形成新的段落，而如果一个段落中的内容仅仅是我们约定的标记，就可以用很容易的用正则匹配到替换为对应的模块容器。 } End@card{ 为了解决 Hexo 自定义页面slug为空不能很好的使用多说评论这个问题，现在已经给每个自定义页面自动生成了hexo-page-path这种格式的slug。本来准备用date做格式的最后一节，测试中发现 page 中的date值为修改时间，是动态的。综合考虑使用了路径path。 以后可以根据需要添加更多模块支持。 打赏和评论默认开启，可根据需要在 Markdown 头部定义是否关闭。 }"},{"title":"","date":"2018-07-22T13:03:46.410Z","updated":"2018-07-22T13:03:46.410Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"links","date":"2018-07-24T14:57:03.515Z","updated":"2018-07-24T14:57:03.515Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":"There some web links might be usefulMa Yi Lab @column-3{ @card{&gt;} @card{ right} @card{ right} }"}],"posts":[{"title":"PGM-2","slug":"PGM-2","date":"2018-08-14T02:46:32.000Z","updated":"2018-08-17T03:40:56.392Z","comments":true,"path":"2018/08/14/PGM-2/","link":"","permalink":"http://yoursite.com/2018/08/14/PGM-2/","excerpt":"","text":"DAG -&gt; Undirected Tree -&gt; Undirected Graph Variable EliminationProbabilistic InferenceThere are two important tasks : Querying: Computing the likelihood of certain variables, optionally conditioned on another set of variables. This is generally called Probabilistic Inference Estimation: When the model itself is unknown, estimating a plausible model M from data D. This process is called Learning. Likelihood : Likelihood is calculated to get the conditional probability of a different subset of variables conditioned based on Evidence $\\mathbf{E} = \\{ X_{k+1},…,X_n \\}$, Evidence is the unknown variables, so we need eliminate the unsure variables to get the specific lieklihood. P(\\mathbf{e}) = \\sum_{x_1}\\cdots \\sum_{x_k} P(x_1,...,x_k,\\mathbf{e})Conditional Probability : The conditional probability distribution of some query nodes conditioned on anevidence. P(X|\\mathbf{e}) = \\frac{P(X,\\mathbf{e})}{P(\\mathbf{e})}= \\frac{P(X,\\mathbf{e})}{\\sum_{x}P(X=x,\\mathbf{e})}Let $\\mathbf{Y}$ be a subset of all domain variables $\\mathbf{X} = \\{ \\mathbf{Y},\\mathbf{Z} \\}$, $\\mathbf{Z}$ is the set of variables under elimination. P(\\mathbf{Y}|\\mathbf{e}) = \\sum_{z}P(\\mathbf{Y},\\mathbf{Z}=z|\\mathbf{e})Most Probable Assignment : In this query, we are interested in finding only one set of values for the query variables that maximize the given conditional probability instead of finding the entire distribution. MPA(\\mathbf{Y}|\\mathbf{e})=\\mathop{arg}\\mathop{max}_{y\\in \\mathbf{Y}}P(y|\\mathbf{e})=\\mathop{arg}\\mathop{max}_{y\\in \\mathbf{Y}}\\sum_{z} P(y,z|\\mathbf{e})EliminationDirected ChainSuppose the graph is $A\\rightarrow B\\rightarrow C\\rightarrow D\\rightarrow E$ \\begin{align} P(e) &= \\sum_{a,b,c,d} p(a,b,c,d) \\\\ &= \\sum_{a,b,c,d} P (a)P (b|a)P (c|b)P (d|c)P (e|d) \\\\ &=\\sum_{d,c,b} P(c|b) P(d|c) P(e|d) \\sum_a P(a) P(b|a) \\\\ &\\text{this is an one variable elimination cost } k^2\\\\ &= \\sum_{d,c,b} P(c|b) P(d|c) P(e|d) p(b) \\\\ &\\ \\cdots \\\\ &= \\sum_d P(e|d) p(d) \\end{align}Complexity: Eliminate: costs $O(k^2 n)$ _the iter cost k and probability cost k_ Naive: cost $O(k^n)$ Undirected ChainSuppose the graph is $A - B - C - D - E$ \\begin{align} P(e) &= \\sum_{a,b,c,d} \\frac{1}{Z} \\phi(b,a) \\phi(c,b) \\phi(d,c) \\phi(e,d) \\\\ &\\propto \\sum_{a,b,c,d} \\phi(b,a) \\phi(c,b) \\phi(d,c) \\phi(e,d) \\\\ &= \\sum_{a,b,c,d} \\phi(c,b) \\phi(d,c) \\phi(e,d)\\sum_a \\phi(b,a) \\\\ &= \\sum_{a,b,c,d} \\phi(c,b) \\phi(d,c) \\phi(e,d) m_a(b) \\\\ &\\ \\cdots \\\\ &= m_d(e) \\end{align}Finally we normalize to obtain a proper probability: P(e) = \\frac{m_d(e)}{\\sum_e m_d(e)}General Variable EliminationFor different evidence, we may need construct different formulation use the method above, we want the computer can automatically calculate the elimination, more general form is needed design. Let $X$ be set of all random variables Let $F$ denote the set of factors and then for each $\\phi \\in F$,$Scope[\\phi] \\in X$ There three type of variables in Elimination Model Let $Y\\subset X$ be a set of query variables Let $Z = X - Y$ would be the set of variables to be eliminated Let $\\mathcal{E}$ be the known variables, and $\\bar{e}_i$ is the assignment The core operation can be view as the form of, we can extend it to general form by import \bevidence potential \\tau(Y) = \\sum_z \\prod_{\\phi \\in F} \\phi The evidence potantial: \\begin{align} \\delta(\\mathcal{E}) = \\left \\{ \\begin{array}{ll} 1& if\\ \\mathcal{E_i} \\equiv \\bar{e}_i \\\\ 0 & if\\ \\mathcal{E_i} \\neq \\bar{e}_i \\end{array} \\right . \\end{align} Total evidence potential: \\begin{align} \\delta(\\mathbf{\\mathcal{E}},\\mathbf{\\bar{e}})= \\prod_{i\\in I_{\\mathcal{E}}} \\delta (\\mathcal{E}_i,\\bar{e}_i) \\end{align} Introducing evidence:\\tau(\\mathbf{Y},\\mathbf{\\bar{e}}) = \\sum_{z,e}\\prod_{\\phi \\in F} \\phi \\times \\delta(\\mathbf{\\mathcal{E}},\\bar{\\mathbf{e}}) The elimination algorithm… … Complexity of Variable Elimination\\tau(Y) = \\sum_z \\prod_{\\phi \\in F} \\phiHere $y$ is the unknow variable waited of elimination, $x$ is under the eliminating, $m$ is the factor during the elimination Product:m_x^{\\prime} (x,y_1,...,y_k) = \\prod_{i=1}^k m_i(x,y_{c_i}) Sum:m_x(y_1,...,y_k) = \\sum_x m^{\\prime}_x (x,y_1,...,y_k) The multiplications is $k\\cdot | Var(X) |\\cdot \\prod_i |Var(Y_{C_i})|$ Graph Elimination Begin with the graph and an elimination ordering. For each variable in the ordering, connect all neighbors of the variable. Eliminate the variable. Belief PropagationThe Chapter 1 we talk about the DAG. In this chapter we want to know elimination in Undirected graph, let’s see how it can be accomplished by seeing a specific case of equivalence in directed and undirected trees. Any undirected tree can be converted to a directed tree by choosing a root node and directing all edges away from it Undirected Tree: p(x) = \\frac{1}{Z}\\left( \\prod_{i\\in V} \\psi(x_i) \\prod_{(i,j)\\in E} \\psi (x_i,x_j) \\right) Directed Tree: p(x) = p(x_r) \\prod_{(i,j)\\in E} p(x_i|x_j) Equivalence: \\psi (x_r) = p(x_r);\\ \\psi(x_i,x_j)=p(x_j|x_i);\\ Z=1,\\psi(x_i) = 1 Elimination on treeElimination operation can be regarded as a message passing process.Let $m_{ji}(x_i)$ is the factor resulting from eliminating variables from below up to $i$, which is a function of $x_i$ : m_{ji}(x_i) = \\sum_{x_j} \\left( \\psi(x_j)\\psi(x_i,x_j) \\prod_{k\\in N(j)\\setminus i} m_{kj}(x_j) \\right)message from $i$ to $j$ only related with opposite side nodes of the arrow, then the probability is : p(x_i)\\propto \\psi(x_i) \\prod_{e\\in N(i)} m_{ei}(x_i)when calculate the different value of $p(x_i)$ we find the $m_{ij}(x_i)$ are re-used, so we can store the value of $m$ Naively this complexity of message passing is $O(NC)$ (where N=nodes, C=complexity of one complete passing/clique bottleneck). However, actually, you can get it in two passes: 2C, or $O(C)$ belief propagation is only valid on trees Factor TreesLet’s define a transformation of any GM named Factor Tree, every factors(clique) in distribution represent a new node in new graph, below is an example, a property is that $f$ nodes only connect $x$ nodes. There are not only one transformation for a graph. The above graph are still a non-tree, we need get a tree like example 3. There are two kinds of variables $\\nu$\b : from variables to factors\\nu_{is}(x_i) = \\prod_{t\\in N(i)\\setminus s} \\mu_{ti}(x_i) $\\mu$ : from factors to variables\\mu_{si}(x_i) = \\sum_{x_{N(s)}\\setminus i}\\left( f_s(x_{N(s)}) \\prod_{j\\in N(s)\\setminus i} \\nu_{js}(x_j)\\right) _the first $\\sum$ operation of the above function it the iters of all possible value of the variables._ Factor tree is broder than a non-tree, which means .","categories":[],"tags":[{"name":"PGM","slug":"PGM","permalink":"http://yoursite.com/tags/PGM/"}]},{"title":"Random-Record","slug":"Random-Record","date":"2018-08-09T10:55:56.000Z","updated":"2018-08-09T10:55:56.584Z","comments":true,"path":"2018/08/09/Random-Record/","link":"","permalink":"http://yoursite.com/2018/08/09/Random-Record/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"PGM-Representation","slug":"PGM-1","date":"2018-08-08T12:51:00.000Z","updated":"2018-08-14T02:46:12.948Z","comments":true,"path":"2018/08/08/PGM-1/","link":"","permalink":"http://yoursite.com/2018/08/08/PGM-1/","excerpt":"","text":"Introduction of PGMWhy Using PGMFor a joint distribution $P(X_1,X_2,…,X_n)$, can be written in two ways All Dependent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2|X_1)P(X_2|X_1,X_2)…P(X_n|X_1,…,X_{n-1})$ pro: the formular are correct in any case con: require huge probability table $O(k^n)$ Independent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2)…P(X_n)$ pro: probability table only take $O(kn)$ memory con: independent are restrictive hypothese The PGM are target a middle-ground between two extremes Two types of GMs Directed edges give causality relationships (Bayesian Network or Directed Graphical Model) Undirected edges simply give correlations between variables (Markov Random Field or Undirected Graphical model) Bayesian NetworkFirstly the Bayesian Network is DAG (Directed Acyclic Graph) Factorization Theorem : The probability of specific BN P(X_1,X_2,...,X_n) = \\prod_{i=1:n} P(X_i|Parents(X_i))Graph model and BayesianSuppose the distribution is $A\\leftarrow B \\rightarrow C$ Bayesian Interpretation: $P(ABC) = P(B) P(A|B) P(C|B)$ Graph Model Interpretation: $I(G) = \\{ A\\bot C |B \\}$now we want to prove $I(P(ABC)) = I(\\{ A\\bot C |B \\})$ \\begin{align} P(AC|B) &= P(A|B)P(C|B) \\leftarrow \\{ A\\bot C |B \\}\\\\ P(AC|B) &= \\frac{P(ABC)}{P(B)} = \\frac{ P(B) P(A|B) P(C|B)}{P(B)} \\leftarrow P(ABC) \\end{align} conditional Independence :for $X\\leftarrow Z \\rightarrow Y$, $Z$ represent the height of father, $X,Y$ are the brother, the relation between $X$ and $Y$ : Dependent : we dont know the height of father, but we know the height of $Y$, so $Y$ may effect the distribution of $Z$, at the same time also influence the $X$ Independent : we already know the height of father, so whether $Y$ is dont influence $X$ local Markov assumption :each node $X_i$ is independent of its nondescendants given its parents._when you confirm the parents, you only dependent with your childs_ I_l(G):\\{ X_i\\bot NonDescendants_{X_i} | Pa_{X_i} : \\forall i \\}Independencies (Three Basic Model) a: Cascade $Y$ observed $X,Z$ are independent b: Common parent $Y$ observed $X,Z$ are independent $Y$ unknow $X,Z$ are dependent c: V-structure $Y$ observed $X,Z$ are dependent $Y$ unknow $X,Z$ are independent Let $P$ be a distribution of $X$, $I(P)$ is the set of independence assertions of the form $(X \\bot Y | Z)$ that hold in $P$. $I(G)$ is the sub-set of the $I(P)$, We say that $K$ is an $I$-map for a _set_ of independencies $I$ if $I(K) ⊆ I$ Active trail Causal Trail $X → Z → Y$ : active iff $Z$ is not observed. Evidential Trail $X ← Z ← Y$ : active iff $Z$ is not observed. Common Cause $X ← Z → Y$: active iff $Z$ is not observed. Common Effect $X → Z ← Y$ : active iff $Z$ (or any of its descendents) is observed._here active means the dependent relation estibilish_ Definition : Let $\\textbf{X}, \\textbf{Y} , \\textbf{Z}$ be three sets of nodes in $G$. We say that $\\textbf{X}$ and $\\textbf{Y}$ are d-separated given $\\textbf{Z}$, denoted $d\bsep_G(\\textbf{X};\\textbf{Y}|\\textbf{Z})$, if there is no active trail between any node $X \\in \\textbf{X}$ and $Y \\in \\textbf{Y}$ given $\\textbf{Z}$ Definition: $I(G)=$ all independence properties that correspond to d- separation: I(G) = {X \\bot Y | Z : dsep_G(X \\bot Y | Z)} Undirected GMAn undirected graphical model represents a distribution $P(X1,…Xn)$ defined by an undirected graph H, and a set of positive-valued potential functions $\\psi_c$ corresponding to each clique $c \\in C$ of $H$ such that: \\begin{align} P(X_1,...,X_n) = \\frac{1}{Z} \\prod_{c\\in C} \\psi_c(X_c) \\\\ Z = \\sum_{X_1,...,X_n} \\prod_{c\\in C} \\psi_c(X_c) \\end{align}potential function can be joint and conditional probability function, or even a table of values Clique Example max-cliques = $\\{A,B,D\\}, \\{B,C,D\\}$, sub-cliques = $\\{A,B\\}, \\{C,D\\}, …\\text{ all edges and single point}$ \\begin{align} P^{\\prime}(x_1,x_2,x_3,x_4) = \\frac{1}{Z} \\psi_c(X_{124}) \\times \\psi_c(X_{234}) \\\\ Z = \\sum_{x_1,x_2,x_3,x_4} \\psi_c(X_{124}) \\end{align}Independence: global Markov independencies $I(H)$ $= \\{ A\\bot C|B:sep_H(A,C|B) \\}$ any disjoint A,B,C in distribution, B separates A and C, A is independent of C given B. local Markov independencies $I_l(H)$ $=\\{X_i \\bot V \\setminus (X\\cup MB_{X_i})|MB_{X_i} :\\forall i \\}$ Independent with node that dont near it. pairwise Markov independencies $I_p(H)$ $=\\{X \\bot Y|V \\setminus \\{X,Y\\}:\\{X,Y\\}\\notin E\\}$ Independent when no shared edge. Markov blanket of $X_i$ denoted $MB_{X_i}$, is the neighbors of $X_i$ in graph B separates A and C if every path from A to C through B: $sep_H(A,C|B)$ relation of local and global Thm: $P\\models I(H) \\Rightarrow P \\models I_l(H) \\Rightarrow P \\models I_p(H)$ Corollary: For a positive distribution P, global, local, and pairwise indepedencies are equivalent Exponential Model\bForm discussed above, we need find a clique potential can ensure the positive distribution, one form is negative exponential: $\\Psi (\\mathbf{x}_c) = exp\\{ -\\phi_c(\\mathbf{x_c}) \\}$ The exponential form of the distribution structure is: p(\\mathbf{x}) = \\frac{1}{Z}\\mathop{exp}\\left\\{ - \\sum_{c\\in C} \\phi_c(\\mathbf{x_c}) \\right\\}=\\frac{1}{Z} \\mathop{exp}\\{ -H(\\mathbf{x}) \\}$H(\\mathbf{x})$ is the “free energy”. Boltzmann MachinesA Boltzmann Machine is a fully connected graph with pairwise potentials on binary-valued nodes. The energy function for this is expressed in sub-clique form, which comes from the physics tradition. \\begin{align} P(\\mathbf{x}) &= \\frac{1}{Z} \\mathop{exp} \\left\\{ \\sum_{i,j} \\phi_{ij}(x_i,x_j) \\right\\} \\\\ &= \\frac{1}{Z} \\mathop{exp} \\left\\{ \\sum_{i,j} \\theta_{ij}x_ix_j + \\sum_i \\alpha_i x_i + C \\right\\} \\\\ &= \\frac{1}{Z} \\mathop{exp} \\left\\{ (x-\\mu)^T \\Theta (x-\\mu) \\right\\} \\end{align}Restricted Boltzmann MachinesThis is inspired by the Boltzmann Machine and is responsible for much of the deep learning craze. An RBM consists of many layers. Within each layer, there are two sublayers: one of hidden units (factors, $h_j$), and one of visible units ($x_i$). The probability function for an RBM is p(x,h|\\theta) = \\mathop{exp} \\left\\{ \\sum_i \\theta_i \\phi_i (x_i) + \\sum_j \\theta_j \\phi_j(h_j) + \\sum_{i,j} \\theta_{i,j} \\phi_{i,j} (x_i,h_j) - A(\\theta) \\right\\}Conditional Random Fields… .","categories":[],"tags":[{"name":"PGM","slug":"PGM","permalink":"http://yoursite.com/tags/PGM/"}]},{"title":"Stochastic-Process-11","slug":"Stochastic-Process-11","date":"2018-08-04T08:05:19.000Z","updated":"2018-08-06T01:27:43.065Z","comments":true,"path":"2018/08/04/Stochastic-Process-11/","link":"","permalink":"http://yoursite.com/2018/08/04/Stochastic-Process-11/","excerpt":"","text":"i.i.d sequence of random variables: too restrictive assumption completely dependent among random variables: hard to analysis balance between complete independence &amp; complete dependence Classification of Markov Process Discrete-Time Markov Chain: Discrete $S$ &amp; Discrete $T$ Continuous-Time Markov Chain: Discrete $S$ &amp; Continuous $T$ Discrete Markov Chain: Continuous $S$ &amp; Discrete $T$ Continuous Markov Chain: Continuous $S$ &amp; Continuous $T$ Definition (Markov Chain)A sequence of random variables $X_0,X_1,X_2,…$ taking values in the state space $\\{1,2,…,M\\}$ is called Markov Chain, the event $X_i+1$ only influenced by $X_i$ P(X_{n+1}=j|X_n = i,X_{n-1}=i_{n-1},...,X_0=i_0) = P(X_{n+1}=j|X_n=i)Definition (Transition Matrix)Let $X_0,X_1,X_2,…$ be a Markov Chain with state space $\\{1,2,…,M\\}$, and let$q_{ij} = P(X_{n+1}=j|X_N=i)$ be the transition probability from state $i$ to state $j$. The $M \\times M$ matrix $Q=(q_{ij})$ is called transition matrix of the chain. Definition (n-step Transition Probability)The n-step transition probability from $i$ to $j$ is the probability of being at $j$ exactly $n$ steps after being at $i$. Denote this by $q^{(n)}_{ij}$ q^{(n)}_{ij} = P(X_n=j|X_0=i)=\\sum_{k\\in S} q^{(1)}_{kj} q^{(n-1)}_{ik}which implies Q^n = Q^{n-1}\\cdot QTheorem (Chapman-Kolmogorov Equation)q_{ij}^{(n+m)} = \\sum_{k\\in S} q^{(n)}_{ik} q^{(m)}_{kj}First Step AnalysisExample (Toss A Coin till HH Appear)This problem can be formulated as a 3-state markov chainThe Transition graph is equivalent toLet $e_s = E[\\text{waiting time for HH|initial state = s}]$, then we have \\begin{align} e_{Null} &= \\frac{1}{2} (1+e_{Null})+\\frac{1}{2} (1+e_H) \\\\ e_H &= \\frac{1}{2} (1+e_{HH}) + \\frac{1}{2} (1+e_{Null})\\\\ e_{HH} &= 0 \\end{align}Example (Toss A Coin till HTHT Appear)Let see a more complicate case , this can be done by establish a linear equation Classification of StatesDefinition (Recurrent and Transition States) Recurrent State $i$ of Markov chain have the probability of $1$ eventually return to $i$ Transient Other-wise, the state is Transient Definition (Irreducible &amp; Reducible Chain) Irreducible any state $i$ and $j$, possible to go from $i$ to $j$ in a finite number of steps. Reducible not irreducible Theorem (Irreducible Implies All States Recurrent)In an irreducible Markov Chain with a finite state space, all states are recurrent Example (Coupon Collector)We want to collect all $C$ types coupons, Let $X_n$ be the number of distinct coupon types in our collection after $n$ attempts. Then $X_0,X_1,…$ is a Markov Chain on the state space $\\{ 0,1,…,C \\}$ Definition (Period)The period of state $i$ in a markov chain is the gcd of the possible numbers of steps can return to $i$ when starting at $i$. A state is called aperiodic if its period equals 1, and periodic otherwise. .","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process-10","slug":"Stochastic-Process-10","date":"2018-08-04T08:05:13.000Z","updated":"2018-08-05T08:29:32.564Z","comments":true,"path":"2018/08/04/Stochastic-Process-10/","link":"","permalink":"http://yoursite.com/2018/08/04/Stochastic-Process-10/","excerpt":"","text":"Bayesian InferenceBayesian Inference FrameworkWe aim to extract information about $\\Theta$, based on observing a collection $X = (X_1,…,X_n)$ Unknow $\\Theta$ treated as a random variable prior distribution $p_\\Theta$ or $f_\\Theta$ Observation $X$ observation model $p_X|\\Theta$ or $f_X|\\Theta$ Use appropriate version of the bayes rule to find $p_{\\Theta|X}(\\cdot|X=x)$ Principal Bayesian Estimation Method Maximum a posterior probability (MAP) rule Select the possible parameter with maximum conditional/posterior probability given the data Least mean squares (LMS) estimation Select an estimator/function of the data that minimizes the mean squared error between the parameterand its estimate $p_{\\Theta|X}(\\theta^*|x)=\\mathop{max}_{\\theta}p_{\\Theta|X}(\\theta|x)$ Example (Inferring the Unknown Bias of A Coin)We wish to estimate the probability of heads, denoted by $p$, suppose the prior is a beta density with $a,b$, that is , $p \\sim Beta(a,b)$. We consider n independent tosses and let $X$ be the number of heads observed MAP Estimate The posterior PDF of $p$ has the form \\begin{align} f(p|X=k) &= \\frac{P(X=k|p)f(p)}{P(X=k)}=\\frac{\\left( \\begin{array}{c} n\\\\k \\end{array} \\right) p^k(1-p)^{n-k}\\frac{1}{\\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)} \\\\ &= c\\cdot p^{a+k-1} (1-p)^{b+(n-k)-1} \\end{align}Hence the posterior density is beta with parameters $a+k$ and $b+(n-k)$ By MAP rule, we select the eatimator as \\hat{p}_{MAP} = \\mathop{arg}\\mathop{max}_p f(p|X=k) = \\mathop{arg}\\mathop{max}_p p^{a+k-1}(1-p)^{b+(n-k)-1}Let $g(p) = p^{a+k-1}(1-p)^{b+(n-k)-1}$,then we have log(g(p)) = (a+k-1)\\mathop{log}p + (b+(n-k)-1)\\mathop{log}(1-p)To find $p^*$ let \\partial \\frac{ \\mathop{log}(g(p))} {\\partial p}|_{p=p^*} = 0which yields \\hat{p}_{MAP} = \\frac{a+k-1}{a+b+n-2}When the prior distribution of $p$ is $Unif(0,1)$, that is $a=0,b=0$, the estimator under MAP rule is $\\hat{p}_{MAP} = \\frac{k}{n}$ LMS Estimate By Beta-Binomial conjugacy, $f(p|X=k) \\sim Beta(a+k,b+n-k)$, the expectation of random variable $Y\\sim Beta(a,b)$ is $E(Y) = \\frac{a}{a+b}$, we have \\hat{p}_{LMS} = E(p|X=k) = \\frac{a+k}{(a+k)+(b+n-k)} = \\frac{a+k}{a+b+k}When the prior distribution of $p$ id $Unif(0,1)$, that is $a = 1,b = 1$, the estimator under MAP rule is $\\hat{p}_{MAP} = \\frac{k+1}{n+2}$ Classical Inference Classical Statistics: unknown constant $\\theta$ also for vectors $X$ and $\\theta$ : $p_{X_1,…,X_n}(x_1,…,x_n; \\theta_1,…,\\theta_m)$ $p_X(x;\\theta)$ are NOT conditional probabilities; $\\theta$ is not random mathematically: many models, one for each possible value of $\\theta$ For example, the data observation model is $X\\sim Binomial(n,\\theta)$, Then under each possible value of $\\theta$, the candidate model is p_X(x;\\theta) = P(X=x;\\theta) = \\left( \\begin{array}{c} n\\\\k \\end{array} \\right) \\theta^x (1-\\theta)^{n-x}Classical Inference use the maximum likelihood to estimate the $\\theta$ Sampling MomentsDefinition (Moments)Let $X$ be an r.v. with mean $\\mu$ and variance $\\sigma^2$, The $n^{th}$ moment of $X$ is $E(X^n)$, the $n^{th}$ central moment is $E((X-\\mu)^n)$, and the $n^{th}$ standardized moment is $E((\\frac{X-\\mu}{\\sigma})^n)$ Definition (Sample Moments)Let $X_1,…,X_n$ be i.i.d. random variables, the $k^{th}$ sample moment is the M_k = \\frac{1}{n} \\sum_{j=1}^n (X_j)^kThe sample mean $\\bar{X}_n$ is the first sample moment: \\bar{X}_n = \\frac{1}{n} \\sum_{j=1}^n X_jTheorem (Mean and Var of Sample Mean)Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\\mu$ and variance $\\sigma^2$. Then the sample mean $\\bar{X}_n$ is unbiased for estimating $\\mu$. That is E(\\bar{X}_n) = \\muThe variance is Var(\\bar{X}_n) = \\frac{\\sigma^2}{n}Definition (Sample Variance)Let $X_1,…,X_n$ be i.i.d. random variables. The sample variance is the r.v. S_n^2 = \\frac{1}{n-1} \\sum_{j=1}^n (X_j-\\bar{X}_n)^2Theorem (Unbiaseness of Sample Var)Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\\mu$ and variance $\\sigma^2$. The Sample Var $S_n^2$ is unbiased for estimating $\\sigma^2$ E(S_n^2) = \\sigma^2Definition (Convergence with Probability)Let $X_1,X_2,…$ be random variables. $X_n$ converges almost surely (a.s.) to the random variable $X$ as $n\\rightarrow \\infty$ and only if P(\\left\\{ \\omega : X_n(\\omega) \\rightarrow X(\\omega)\\ as\\ n\\rightarrow \\infty \\right\\}) = 1Notation: $X_n \\xrightarrow{a.s.} X \\text{ as } n\\rightarrow \\infty$ Example Definition (Convergence in Probability)Let $X_1,X_2,…$ be random variables. $X_n$ converges in probability to the random variable $X$ as $n\\rightarrow \\infty$ if and only if for every $\\epsilon &gt;0$ P(|X_n-X|>\\epsilon) \\rightarrow0 \\ as \\ n\\rightarrow \\inftyNotation: $X_n \\xrightarrow{P} X \\text{ as } n\\rightarrow \\infty$ Example Law of large NumbersDefinitionLet $X_1,…,X_n$ be i.i.d. r.v. with finite mean $\\mu$ and finite variance $\\sigma^2$. The samplw mean $\\bar{X}_n$ is defined as \\bar{X}_n = \\frac{1}{n}\\sum_{j=1}^n X_jThe Sample mean $\\bar{X}_n$ is itself an r.v. with mean $\\mu$ and variance $\\sigma^2/n$ Theorem (Strong Law of Large Numbers)The event $\\bar{X}_n \\rightarrow \\mu$ has probability $1$ Theorem (Weak Law of Large Numbers)For all $\\epsilon &gt;0, P(|\\bar{X}_n - \\mu&gt;\\epsilon) \\rightarrow 0$ as $n\\rightarrow \\infty$ Definition (Time Average)\\bar{N}^{Time\\ Average}(\\omega)=\\mathop{lim}_{t\\rightarrow \\infty} \\frac{\\int_0^t N(v,\\omega)dv}{t}Definition (Ensemble Average)\\bar{N}^{Ensemble}(\\omega)=\\mathop{lim}_{t\\rightarrow\\infty} E[N(t)]=\\sum_{i=0}^{\\infty} i p_i Central Limit TheoremTheorem (Central Limit)As $n\\rightarrow \\infty$ \\sqrt{n}\\left( \\frac{\\bar{X}_n - \\mu}{\\sigma} \\right) \\rightarrow N(0,1)CLT Approximation For a large $n$, the distribution od $\\bar{X}_n$ is approximately $N(\\mu,\\sigma^2/n)$ Example (CLT Example) Poisson Convergence to Normal Let $Y\\sim Pois(n)$. Consider $Y$ as sum of $n$ i.i.d. $Pois(1)$ r.v.s. For large $n$:Y\\sim N(n,n) Gamma Convergence to Normal Let $Y\\sim Gamma(n,\\lambda)$. Consider $Y$ as sum of $n$ i.i.d. $Expo(\\lambda)$ r.v.s. For large $n$:Y\\sim N(\\frac{n}{\\lambda},\\frac{n}{\\lambda^2}) Binomial Convergence to Normal Let $Y\\sim Bin(n,p)$. Consider $Y$ as sum of $n$ i.i.d. $Bern(p)$ r.v.s. For large $n$:Y\\sim N(np,np(1-p)) De Moivre-Laplace Approximation\\begin{align} P(Y=k) &= P(k-\\frac{1}{2} < Y < k + \\frac{1}{2}) \\\\ &\\approx \\Phi\\left( \\frac{k+\\frac{1}{2} - np}{\\sqrt{np(1-p)}} \\right) -\\Phi\\left( \\frac{k-\\frac{1}{2} - np}{\\sqrt{np(1-p)}} \\right) \\end{align} Possion Approximation When $n$ is large and $p$ is samll Normal Approximation When $n$ is large and $p$ is around $1/2$ InequalityBasic Inequalities Cauchy-Schwarz Inequality |E(XY)| \\leq \\sqrt{E(X^2)E(Y^2)} Jensen’s Inequality If $g$ is a convex function and $X$ is a r.v. thenE(g(x)) \\geq g(E(X)) If $g$ is a concave functionE(g(x)) \\leq g(E(X)) Markov’s Inequality P(|X|\\geq a) \\leq \\frac{E|X|}{a} Chebyshev’s Inequality Let $X$ have mean $\\mu$ and variance $\\sigma^2$ P(|X-\\mu|\\geq a) \\leq \\frac{\\sigma^2}{a^2} Chernoff’s Inequality P(X\\geq a) \\leq \\frac{E(e^{tX})}{e^{ta}} Concentration InequalitiesHoeffding Lemma Let r.v. $X$ satisfy $E(X) = 0$ and $X\\leq b$, Then for any $h&gt;0$ E(e^{hX}) \\leq e^{\\frac{1}{8}h^2 (b-a)^2}Hoeffding Inequality Let the r.v. $X_1,X_2,…,X_n$ be independent, with $x_k\\leq X_k\\leq b_k$ for each $k$, Let $S_n = \\sum_{k=1}^n X_k$. Then P(|S_n- \\mu|\\geq t) \\leq 2e^{-\\frac{2t^2}{\\sum_{k=1}^n(b_k-a_k)^2}}.","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process (Martingale)","slug":"Stochastic-Process-9","date":"2018-08-04T07:16:54.000Z","updated":"2018-08-04T07:46:59.326Z","comments":true,"path":"2018/08/04/Stochastic-Process-9/","link":"","permalink":"http://yoursite.com/2018/08/04/Stochastic-Process-9/","excerpt":"","text":"Stopping TimeTheorem (Mean of Random Sum is Sum of Means)Let $X_1,X_2,…,X_n$ be i.i.d. random variables and $N\\geq 0$ independent of all $X_n$ Then E\\left( \\sum_{n=1}^N X_n \\right) = E(N) E(X_1) Proof using Adam’s Law E\\left( \\sum_{n=1}^N X_n \\right) = E\\left( E\\left( \\sum_{n=1}^N X_n|N \\right) \\right) = E(NE(X_1|N)) Since $N$ is independent of $X_1$, we have E\\left( \\sum_{n=1}^N X_n \\right) = E(NE(X_1)) = E(X_1)E(N)","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process (Conditional Expectation)","slug":"Stochastic-Process-8","date":"2018-08-03T12:33:40.000Z","updated":"2018-08-04T03:18:47.805Z","comments":true,"path":"2018/08/03/Stochastic-Process-8/","link":"","permalink":"http://yoursite.com/2018/08/03/Stochastic-Process-8/","excerpt":"","text":"Conditional Expectation Given An EventIf $Y$ is a discrete r.v. E(Y|A) = \\sum_y P(Y=y|A)If $Y$ is continuous r.v. E(Y|A) = \\int_{-\\infty}^{\\infty} yf(y|A)dyApproximationImage a large number of $n$ of replication of experiments $y_1,…,y_n$ E(Y)\\approx \\frac{1}{n}\\sum_{j=1}^n y_jIf $I_j$ is the indicator of $A$ occurring E(Y|A) \\approx \\frac{\\sum_{j=1}^n y_j I_j}{\\sum_{j=1}^n I_j}Example (Life Expectation)Yang is 24 years old, he hear average life expectancy is $80$, Should he conclude he has 50 years of life left ? Of Course not, cause he already live $24$ years and some people may die less than $24$ E(T) < E(T|T\\geq 30)Law of Total ExpectationLet $A_1,…,A_n$ be partition of a sample space, $Y$ be a random variable on sample space. Then E(Y) = \\sum_{i=1}^n E(Y|A_i) P(A_i)Example (Geometric Expectation Redux)Let $X\\sim Geom(p)$, as the number of Tails before the first Heads in a sequence of coin flips with p. $p$ of head. To get $E(X)$ from sum of series, it also can be obtained in another way. We condition on the outcome of the first toss: if it lands heads, then $X$ is $0$ and we’re done ; if it lands Tails, then we wasted one toss and back to where we started by memorylessness Therefore \\begin{align} E(X) &= E(X|\\text{first toss }H)\\cdot p + E(X|\\text{first toss }T)\\cdot q \\\\ &= 0 \\cdot p + (1+E(X)) \\cdot q \\end{align}which gives $E(X) = q/p$ Example (Time until HH vs. HT)You toss a fair coin repeatedly. What is the expected number of tosses until the pattern HT/HH appears for the first times ? Times until HT $W_{HT}$: number of tosses untill HT appears $W_1$: waiting time for first H $W_2$: additional waiting time for the first T Then $W_1\\sim Fs(\\frac{1}{2}),E[W_1] = 2$ $W_2\\sim Fs(\\frac{1}{2}),E[W_2] = 2$ E[W_{HT}] = E[W_1+W_2]=E[W_1] + E[W_2] = 4 Times until HH E[W_{HH}] = E[W_HH|\\text{first toss }H]\\cdot \\frac{1}{2} + E[W_HH|\\text{first toss }T]\\cdot \\frac{1}{2} where E[W_{HH}|\\text{first toss }T] = 1 + E[W_{HH}] and \\begin{align} E[W_{HH}|\\text{first toss }H] =& E[W_{HH}|\\text{first toss }H, \\text{second toss }H]\\cdot \\frac{1}{2} \\\\ &+ E[W_{HH}|\\text{first toss }H, \\text{second toss }T]\\cdot \\frac{1}{2} \\\\ =& 2\\cdot \\frac{1}{2} + (E[W_{HH}]+2)\\cdot \\frac{1}{2} \\end{align} Thus we get $E[W_{HH}] = 6$ As we can see the above example use the memorylessness property of the Conditional Expectation of distribution, and construct target in both side to calculate the target Conditional Expectation Given An R.V.Let $g(x) = E(Y|X=x)$ Then the conditional expectation of $Y$ given $X$, denoted $E(Y|E)$ is defined to be the random variable $g(X)$ Example (Stick Length)Suppose we have a stick of length $1$ and break the stick at a point $X$ chosen uniformly at random. Given that $X=x$, we then choose another breakpoint $Y$ uniformly on the interval $[0,x]$, find $E(Y|X)$, and its mean and variance E(Y|X) = X/2E(E(Y|X)) = E(X/2) = \\frac{1}{4}Var(E(Y|X)) = Var(X/2) = \\frac{1}{48} Properties of Conditional ExpectationTheorem (Dropping independent)If $X$ and $Y$ are independent, then $E(Y|X)=E(Y)$ Taking Out What’s KnownE(h(X)Y|X) = h(X) E(Y|X)Theorem (Linearity)E(Y_1+Y_2|X) = E(Y_1|X) + E(Y_2|X)Theorem (Adam’s Law)For any r.v.s $X$ and $Y$ E(E(Y|X)) = E(Y)Proof by LOTP For $X$ discrete E(Y) = \\sum_x E(Y|X=x) P(X=x) We let $E(Y|X=x) = g(x)$, then E(E(Y|X)) = E(g(X)) = \\sum_x g(x) P(X=x) = \\sum_x E(Y|X=x) P(X=x) So E(E(Y|X)) = E(Y)Theorem (Adam’s Law with Extra Conditioning)For any r.v.s $X,Y,Z$ E(E(Y|X,Z)|Z) = E(Y|Z)E(E(X|Z,Y)|Y) = E(X|Y)Definition (Conditional Variance)Var(Y|X) = E((Y-E(Y|X))^2|X)this equivalent to Var(Y|X) = E(Y^2|X) - (E(Y|X))^2Theorem (Eve’s Low)Var(Y) = E(Var(Y|X)) + Var(E(Y|X))Example (Random Sum)A store receives $N$ customers a day, $N$ is an r.v. with finite mean and variance. Let $X_j$ be the amount spend by the $j^{th}$ customer, $X_j$ has the mean $\\mu$ and variance $\\sigma^2$, $N$ and $X_j$ are independent of one another. Find the random sum $X = \\sum_{j=1}^N X_j$ in terms of $\\mu,\\sigma^2,E(N),Var(N)$ For E(X) E(X|N) = E\\left( \\sum_{j=1}^N X_j|N \\right) = \\sum_{j=1}^N E(X_j|N)= \\sum_{j=1}^N E(X_j) = N\\mu Finally, by Adam’s Law E(X) = E(E(X|N)) = E(N\\mu) =\\mu E(N)For Var(X) We conditon on $N$ get $Var(X|N)$ Var(X|N) = Var\\left( \\sum_{j=1}^N X_j|N \\right) = \\sum_{j=1}^N Var(X_j|N) = \\sum_{j=1}^N Var(X_j) = N\\sigma^2 Eve’s Law give the unconditional variance of $X$ \\begin{align} Var(X) =& E(Var(X|N)) + Var(E(X|N))\\\\ =& E(N\\sigma^2) + Var(N\\mu) \\\\ =& \\sigma^2 E(N) + \\mu^2 Var(N) \\end{align} Prediction and EstimationTheorem (Projection Interpretation)For any function $h$, the r.v. $Y-E(Y|X)$ is Uncorrelated with $h(X)$: $Cov(Y-E(Y|X),h(X)) = 0$, equivalently E((Y-E(Y|X))h(X)) = 0.","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process (Conjugacy & Bayesian)","slug":"Stochastic-Process-7","date":"2018-07-31T02:35:18.000Z","updated":"2018-08-03T05:40:01.666Z","comments":true,"path":"2018/07/31/Stochastic-Process-7/","link":"","permalink":"http://yoursite.com/2018/07/31/Stochastic-Process-7/","excerpt":"","text":"Beta-Binomial DistributionDefinition (Beta Distribution)An r.v. $X$ is said to have Beta distribution with parameters $a$ and $b$, if its PDF is f(x) = \\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1},0","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process (Multivariate)","slug":"Stochastic-Process-6","date":"2018-07-29T00:41:09.000Z","updated":"2018-08-02T01:39:52.453Z","comments":true,"path":"2018/07/29/Stochastic-Process-6/","link":"","permalink":"http://yoursite.com/2018/07/29/Stochastic-Process-6/","excerpt":"","text":"Discrete Multivariate R.V.sDefinition (Joint CDF) The Joint CDf of r.v.s $X$ and $Y$ is the function $F_{X,Y}$ given by F_{X,Y}(x,y) = P(X\\leq x,Y\\leq y)Definition (Joint PMF) The Joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X,Y}$ given by p_{X,Y}(x,y) = P(X=x,Y=y)Definition (Marginal PMF) For discrete r.v.s $X$ and $Y$, Marginal PMF of $X$ is P(X=x) = \\sum_y P(X=x,Y=y)Definition (Conditional PMF) For discrete r.v.s $X$ and $Y$, the Conditional PMF of $X$ given $Y=y$ is P_{X|Y}(x|y) = P(X=x|Y=y)=\\frac{P(X=x,Y=y)}{P(Y=y)}Definition (Independence of Discrete R.V.s) Random variables $X$ and $Y$ are independent if for all x and y F_{X,Y}(x,y) = F_X(x) F_Y(y)for all x and y also equivalent to the condition P(Y=y|X=x) = P(Y=y) Continuous Multivariate R.V.sDefinition (Joint PDF) If $X$ and $Y$ are continuous with joint CDF $F_{X,Y}$ then f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)Definition (Marginal PDF) If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ then f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) dyDefinition (Conditional PDF) For continuous r.v.s. $X$ and $Y$ with joint PDF $f_{X,Y}$ the Conditional PDF of $Y$ given $X=x$ is f_{Y|X}(y|x)= \\frac{f_{X,Y}(x,y)}{f_X(x)}Definition (Independence of Continuous R.V.s) Random variables $X$ and $Y$ are independent if for all x and y F_{X,Y}(x,y) = F_X(x)F_Y(y)If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ f_{X,Y}(x,y) = f_X(x) f_Y(y)Theorem (2D LOTUS) Let g be a function from $R^2$ to $R$ If $X$ and $Y$ are discrete E(g(X,Y)) = \\sum_x \\sum_y g(x,y) P(X=x,Y=y)If $X$ and $Y$ are continuous E(g(X,Y)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{X,Y}(x,y) dxdyGeneral Bayes’ Rule Convariance and CorrelationCovariance Measure a tendency of two r.v.s $X\\&amp;Y$ to go up or down together Positive Covariance: $X$ go up, $Y$ tends go up Negative Covariance: $X$ go up, $Y$ tends go down Definition (Covariance) The covariance between r.v.s $X$ and $Y$ is Cov(X,Y) = E((X-EX)(Y-EY))=E(XY)-E(X)E(Y)Theorem (Uncorrelated) If $X$ and $Y$ are independent, then they are Uncorrelated($Cov(X,Y)=0$) Properties of Covariance $Cov(X,X) = Var(X)$ $Cov(X,Y) = Cov(Y,X)$ $Cov(X,c) = 0$ $Cov(a\\cdot X,Y) = a\\cdot Cov(X,Y)$ $Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$ $Cov(X+Y,W+Z) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$ $Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$ For n r.v.s $X_1,\\dotsb ,X_n$ Var(X_1+\\dotsb +X_n)=Var(X_a)+\\dotsb+Var(X_n)+2\\sum_{i x)\\dotsb P(X_n> x) \\\\ &=1-[1-F(x)]^n \\end{align}The result here can be rewrite as $\\sum_{k=1}^n\\left( \\begin{array}{c} n\\\\k \\end{array} \\right) F(x)^k (1-F(x))^{n-k}$ This result can be obtained by expand $[F(x) + 1 -F(x)]^n$ Finally, let’s consider more general case where $1&lt;j&lt;n, X_{(j)}\\leq x$, this means at least $j$ of $\\{X_i \\}$ fall to the left of $x$ Denote $N$ as the nunber of $X_i$ landing to the left of $x$. $X_i$ lands to the left of $x$ w.p. $P(X_i\\leq x) = F(x)$. Then $N\\sim Bin(n,F(x))$ P(X_{(j)}\\leq x) = P(N\\geq j=\\sum_{k=j}^n \\left( \\begin{array}{c} n\\\\k \\end{array} \\right) F(x)^k(1-F(x))^{n-k} Theorem (PDF of Order Statistic) Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with CDF $F$ and PDF $f$. Then the marginal PDF of $j^{th}$ order statistic $X_{(j)}$ is f_{X_{(j)}} (x) = n \\left( \\begin{array}{c} n-1\\\\j-1 \\end{array} \\right) f(x) F(x)^{j-1} (1-F(x))^{n-j}Theorem (Joint PDF) Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with PDF $f$, Then the joint PDF of all order statistics is f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n) = n! \\prod_{i=1}^n f(x_i), x_1","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Topology-(引论)","slug":"Topology-1","date":"2018-07-28T12:19:29.000Z","updated":"2018-07-29T00:40:25.323Z","comments":true,"path":"2018/07/28/Topology-1/","link":"","permalink":"http://yoursite.com/2018/07/28/Topology-1/","excerpt":"","text":"Eular 定理对于一个多面体 P，我们定义 v：定点数 e：棱边数 f：面数 Eular 定理：$v+f-e = 2$ 但是满足这个定理的多面体是有条件的： P 的任何两个顶点可以用一串棱相连接 反例：中空的立方体 P 上任意由直线段构成的圈，把 P 分割成两片 反例：螺帽柱状体 Eular 定理证明：我们首先来看看树形，这个在图论里面常常出现，树有个性质就是 $v-e=1$，我们可以尝试用一棵树 T 来表示一个多面体，表示的方法是，树中的点就是 P 中的点（树 T 中的点囊括了所有 P 的点），树中的边就是 P 中的棱（边只是一部分的棱哦）。 然后我们来构造 T 的一种对偶，称为 $\\Gamma$，$\\Gamma$ 也是一颗树后面会证明，只不过这棵树的点由 P 中面的中心点来表示（也就是用来表示面的数量），这样面与面之间的边在多面体中是可以有一个曲折的，可以想象一下。。 上面采用这个形式只是因为这样构造能囊括所有的面，下面来证明一下这个 $\\Gamma$ 是树，而且曲折所在的棱刚好是 T 的边对于 P 中棱的补集： 连通性：如果 $\\Gamma$ 的某两个顶点不能用 $\\Gamma$ 内的一串棱连接，则它们必然被一个圈分开。由于 T 不含任何圈，$\\Gamma$ 必然联通。 无圈：如果 $\\Gamma$ 有圈，那么就会把顶点分开成两份，T 中的棱想要连接所有顶点就不可避免的要触碰到这个圈，所以 $\\Gamma$ 无圈 $T,\\Gamma$ 包含所有棱：假设一条棱没有被用着，这个棱本可以这样被用：棱两侧的面中点相连（$\\Gamma$），或者棱两端的点相连（T），但是都没用着，这样 $\\Gamma ,T$ 就会在后面相交。。（这是我的数学直觉，书上并没有这个的证明，我自己补的。。。不是很严谨。。。） 最后我们有 $v(T) - e(T) = 1$, $v(\\Gamma) - e(\\Gamma) = 1$, 加起来有 v(T) - [e(T)+e(\\Gamma)] + v(\\Gamma) = 2同时，根据构造有 v(T) = v, e(T) + e(\\Gamma)+e, v(\\Gamma) = f其他的证明方式可以用数学归纳法拓扑等价我们考虑一个正四面体未冲气的气球，我们把它吹胖，吹成了一个圆形。 这样多面体的点和球面的点之间的对应就是拓扑等价或同胚的一个例子，确切的说就是一对一的连续满映射","categories":[],"tags":[{"name":"Topology","slug":"Topology","permalink":"http://yoursite.com/tags/Topology/"}]},{"title":"Stochastic-Process (Generating Function)","slug":"Stochastic-Process-5","date":"2018-07-27T02:23:34.000Z","updated":"2018-07-28T09:06:43.720Z","comments":true,"path":"2018/07/27/Stochastic-Process-5/","link":"","permalink":"http://yoursite.com/2018/07/27/Stochastic-Process-5/","excerpt":"","text":"Generating FunctionThree kinds of generating functions Probability Generating Function (PGF) : related to Z-transform Moment Generating Function (MGF) : related to Laplace transform Characteristic Function (CF) : related to Fourier transform Motivation PGF: handling non-negative integral random variables MGF: handling general random variables CF: equally useful with MGF Application Easy to characterizing the distribution of the sum of independent random variables Play a central role in the study of branching processes Provide a bridge between complex analysis and probability …… Moment Generating FunctionDefinition (Moment Generating Function) MGF of an r.v. $X$ is $M(t) = E(e^{tX})$, as a function of $t$ (different t denote different valued moment) and this must finite on some open interval (-a,a) containing 0 or dont exist. Why we need MGF MGF encodes the moments of an r.v. MGF of an r.v. Determines its distribution, like CDF and PMF/PDF MFG make it easy to find the distribution of a sum of i.r.v.s. Theorem (Moments via Derivatives of the MGF) $E(X^n) = M^{(n)}(0)$Using Taylor expansion of $M(t)$ at 0 M(t) = \\sum_{n=0}^{\\infty} M^{(n)}(0) \\frac{t^n}{n!}Using Taylor expansion of $E(X)$ M(t) = E(e^{tX}) = E\\left( \\sum_{n=0}^{\\infty} X^n \\frac{t^n}{n!} \\right)=\\sum_{n=0}^{\\infty} E(X^n) \\frac{t^n}{n!}Matching the coefficients of two expansions, we get $E(X^n) = M^{(n)}(0)$ MGF of DistributionTheorem (MGF Determines the Distribution) Two r.v. have the same MGF have the same distribution, more strictly, if there is even a tiny interval containing 0 on which the MGF are equal, the the r.v.s must have same distribution. Example 1 (Bernoulli MGF) MGF of $X\\sim Bern(p)$$e^{tX}=e^t$ with probability $p$, and $1$ with probability $q$, so $M(t) = E(e^{tX})=pe^t + q$ Example 2 (Geometric MGF) MGF of $X\\sim Geom(p)$ M(t) = E(e^{tX})=\\sum_{k=0}^{\\infty} e^{tk}q^kp=p\\sum_{k=0}^{\\infty} (qe^t)^k=\\frac{p}{1-qe^t}t in $(-\\infty, log(1/p))$ Example 3 (Uniform MGF) MGF of $U\\sim Unif(a,b)$ M(t) = E(e^{tU}) = \\int_a^b e^{tu}\\frac{1}{b-a} du = \\frac{e^{tb}-e^{ta}}{t(b-a)}and $M(0) = 1$ Example 4 (Binomial MGF) $Bin(n,p)$ M(t) = (pe^t+q)^nExample 5 (Negative Binomial) $NBin(r,p)$ M(t) = \\left( \\frac{p}{1-qe^t} \\right)^rTheorem (MGF of Location-Scale Transformation) If $X$ has MGF $M(t)$, then MGF of $a+bX$ is E(e^{t(a+bX)})=e^{at}E(e^{btX})=e^{at}M(bt)Example 6 (Normal MGF) MGF of $(X = \\mu + \\sigma Z) \\sim N(\\mu,\\sigma^2)$ M_Z(t) = E(e^{tZ})=\\int_{-\\infty}^{\\infty}e^{tz}\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}dz=e^{t^2/2}Use the Theorem above then M_X(t) = e^{\\mu t}M_Z(\\sigma t) = e^{\\mu t}e^{(\\sigma t)^2/2} = e^{\\mu t + \\frac{1}{2} \\sigma^2 t^2}Sum of Independent DistributionsTheorem (MGF of A Sum of Independent R.V.s) If $X$ and $Y$ are independent, Then M_{X+Y} (t) = M_X(t) M_Y(t)Example 1 (Sum of Poissons) $X\\sim Pois(\\lambda), Y\\sim Pois(\\mu)$, $X$ and $Y$ are independent. Then $X+Y \\sim Pois(\\lambda + \\mu)$The MGF of $X$ is E(e^{tX}) = \\sum_{k=0}^{\\infty}e^{tk}\\frac{e^{-\\lambda}\\lambda^k}{k!}=e^{-\\lambda}\\sum_{k=0}^{\\infty}\\frac{(\\lambda e^t)^k}{k!}=e^{-\\lambda}e^{\\lambda e^t}=e^{\\lambda(e^t-1)}The MGF of $X+Y$ is E(e^{tX})E(e^{tY}) = e^{\\lambda (e^t-1)} e^{\\mu (e^t-1)} = e^{(\\lambda + \\mu)(e^t-1)}Which is the $Pois(\\lambda + \\mu)$, so $X+Y\\sum Pois(\\lambda+\\mu)$ Example 2 (Sum of Normals) $X_1\\sim N(\\mu_1,\\sigma_1^2)$ and $X_2 \\sim N(\\mu_2,\\sigma_2^2)$, $X_1+X_2 = ?$MGF of $X_1+X_2$ is M_{X_1+X_2}(t)= M_{X_1}(t)M_{X_2}(t)= e^{\\mu_1t+\\frac{1}{2}\\sigma^2_1t^2}\\cdot e^{\\mu_2 t+\\frac{1}{2}\\sigma_2^2 t^2}=e^{(\\mu_1+\\mu_2)t+\\frac{1}{2}(\\sigma_1^2+\\sigma_2^2)t^2}Which is the N(\\mu_1 + \\mu_2, \\sigma_2^2 + \\sigma_1^2) MGF. Probability Generating FunctionDefinition (PGF) PGF of a nonnegative integer-valued r.v. $X$ with PMF $p_k = P(X=k)$ is the generating function of the PMF, By LOTUS , this is E(t^X) = \\sum_{k=0}^{\\infty} p_k t^kExample 1 (Generating Dice Probabilities) Let $X$ be the sum from rolling 6 pair dice, $X_1,…,X_6$ be the individual rolls, what is $P(X=18)$ ?The PGF of $X_1$ is E(t^{X_1}) = \\frac{1}{6}(t+t^2+\\dotsb+t^6)The PGF of $X$ is E(t^X) = E(t^{X_1}\\dotsb t^{X_6}) = E(t^{X_1})\\dotsb E(t^{X_6})=\\frac{t^6}{6^6}(1+t+\\dotsb +t^5)^6The coefficient of $t^{18}$ in the PGF is $P(X=18)$, so P(X=18) = \\frac{3421}{6^6}Theorem (PMF \\&amp; PGF) P(X=k) = \\frac{g_{X}^{(k)}(0)}{k!} Characteristic FunctionDefinition CF The Characteristic function of a random variable $X$ is the function $\\phi : R \\rightarrow C$ defined by \\phi(t) = E(e^{itX}), i = \\sqrt{-1}","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic-Process (Continuous Random Variable)","slug":"Stochastic-Process-4","date":"2018-07-25T12:04:57.000Z","updated":"2018-07-28T12:05:50.907Z","comments":true,"path":"2018/07/25/Stochastic-Process-4/","link":"","permalink":"http://yoursite.com/2018/07/25/Stochastic-Process-4/","excerpt":"","text":"Probability Density FunctionDefinition (Probability Density Function): For a continuous r.v. $X$ with CDF $F$, the probability density function (PDF) of $X$ is the derivative $f$ of the $F$ P(a\\leq X \\leq b) = \\int_a^b f_X(x)dx = F(b) - F(a)P(X\\in [x,x+\\delta]) \\approx f_X(x) \\cdot \\deltaRelation between PDF &amp; PMF: The PDF is the analogous to the PMF in many ways. But, the PDF $f(x)$ is not a probability. Relation between PDF &amp; CDF: Let $X$ be a continuous r.v. with PDF $f$. Then the CDF of $X$ is given by F(x) = \\int_{-\\infty}^x f(x) dtCDF of Logistic Distribution F(x) = \\frac{e^x}{1+e^x}, x\\in RCDF of Rayleigh Distribution F(x) = 1 - e^{-x^2/2}, x>0Theorem (Expectation of Continuous R.V.) E(X) = \\int_{-\\infty}^{\\infty} x f(x) dxTheorem (Expectation via Survial Function) Let $G$ be the survial function of $X$, Then E(X) = \\int_0^{\\infty} G(x) dxTheorem (LOTUS: Continuous) E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f(x) dx Uniform DistributionDefinition (Uniform Distribution) Distribution on the interval$(a,b)$, and its PDF is f(x) = \\left \\{ \\begin{array}{ll} \\frac{1}{b-a} & if\\ ax) \\\\ &= P(X_1>x,...,X_n>x)\\\\ &= \\prod_{i=1}^n P(X_i > x) = (1-x)^n \\end{align}From above, we use the survial function to calculate the expectation E(Y) = \\int_0^{\\infty} P(Y>x) dx = \\int_0^1(1-x)^ndx = \\int_0^1 x^n dx = \\frac{1}{n+1} NormalDefinition (Standard Normal Distribution) A c.r.v. $Z$ is said to have the standard Normal Distribution if its $PDF$ $\\varphi$ is given by: \\varphi(z) = \\frac{1}{\\sqrt(2\\pi)} e^{-z^2/2},-\\infty < z < \\inftyWe write this as $Z\\sim N(0,1)$, and $Z$ has mean 0 and variance 1, the CDF $\\phi$ is \\phi(z) = \\int_{-\\infty}^z \\varphi(t) dt = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}}e^{-t^2/2}dtDefinition (Normal Distribution) If $Z\\sim N(0,1)$ then X = \\mu + \\sigma Zis said to have the Normal Distribution with mean $\\mu$ and variance $\\sigma^2$, denote this by $X\\sim N(\\mu,\\sigma^2)$ Theorem (Normal CDF and PDF) Let $X\\sim N(\\mu, \\sigma^2)$,CDF of $X$ is F(x) = \\phi(\\frac{x-\\mu}{\\sigma})PDF of $X$ is f(x) = \\varphi (\\frac{x-\\mu}{\\sigma})\\frac{1}{\\sigma} ExponentialDefinition (Exponential Distribution) f(x) = \\lambda e ^{-\\lambda x}, x>0we denote this by $X\\sim Expo(\\lambda)$. The corresponding CDF is F(x) = 1-e^{-\\lambda x},x>0Theorem (Memoryless Property) P(X\\geq s+t | X\\geq s) = P(X\\geq t) If $X$ is a positive continuous random variable with memoryless property, then $X$ has an Exponential distribution Geometric Distribution is also Memoryless Exponential distribution as the “continuous counterpart” of the Geometric distribution Exponential \\&amp; Geometric via $\\delta$- StepsWe devide a unit of time into n pieces, each of size $\\delta = \\frac{1}{n}$, and the trial occurs every $\\delta$ time period and success with probability $\\lambda \\delta$. Denote $Y$ as the number of trials until first success, $\\hat{Y}$ as the time until first success under $Y$. Y\\sim FS(\\lambda \\delta)Thus we have F(\\hat{Y}) = E(Y)\\cdot \\delta = \\frac{1}{\\lambda\\delta}\\cdot \\delta=\\frac{1}{\\lambda}And \\begin{array} P(Y>t) &= P\\{ all\\ trials\\ up\\ to\\ time\\ t\\ has\\ been\\ failures \\} \\\\ &=P\\{ at\\ least\\ \\frac{t}{\\delta}\\ failures\\}\\\\ &=(1-p)^{\\frac{t}{\\delta}}=(1-\\lambda \\delta)^{\\frac{t}{\\delta}}\\\\ &=\\left[ (1-\\lambda\\delta)^{\\frac{1}{\\lambda\\delta}} \\right]^{\\lambda t} \\xrightarrow{\\delta \\rightarrow 0} e^{-\\lambda t} \\end{array}Theorem property of Exponential Given $X_1\\sim Expo(\\lambda_1)$, $X_2\\sim Expo(\\lambda_2)$, $X_1 \\bot X_2$ ($X_1$ and $X_2$ are independent), then P(X_1","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Generative-Adversarial-Nets","slug":"Generative-Adversarial-Nets","date":"2018-07-25T11:21:44.000Z","updated":"2018-08-10T01:04:48.720Z","comments":true,"path":"2018/07/25/Generative-Adversarial-Nets/","link":"","permalink":"http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/","excerpt":"","text":"$x$: Data we already have $z$: Data we want to generate $\\theta_g$: parameter for Generator $\\theta_d$: parameter for D $G$: Generator $D$: Discriminator target is training $G$ to minimize \\mathop{min}_G \\mathop{max}_D V(D,G) = E_{x\\sim p_{data(x)}}[\\mathop{log}D(x)] + E_{z\\sim p_z(z)}[\\mathop{log}(1-D(G(z)))]For player $D$, want V bigger by making more accurate estimate on real date x as $D(x)\\rightarrow 1$ discriminate the fake data by $D(G(z)) \\rightarrow 0$ equals to $(1 - D(G(z))) \\rightarrow 1$ more accurate the $D$ is ,the larger value of $V$ can be For player $G$, want V smaller by enlarge $D(G(z))\\rightarrow 1$ for $(1 - D(G(z))) \\rightarrow 0$ better fake of $G$, $G(z)\\rightarrow 1$, and less value of $V$ Algorithm for number of iterations do for $k$ steps do Sample $m$ noise samples $\\{ z^{(1)},…,z^{(m)} \\}$ from noise prior $p_g(z)$ Sample $m$ examples $\\{ x^{(1)},…,x^{(m)} \\}$ from data generating distribution $p_{data}(x)$ Update the discriminator by ascending its stochastic gradient :\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\left[ \\mathop{log}D(x^{(i)})+log\\left(1-D(G(z^{(i)}))\\right) \\right] end for Sample $m$ noise samples $\\{ z^{(1)},…,z^{(m)} \\}$ from noise prior $p_g(z)$ Update the generator by descending its stochastic gradient :\\nabla_{\\theta_g} \\frac{1}{m}\\sum_{i=1}^m\\mathop{log}\\left( 1-D(G(z^{(i)})) \\right) end for ew - q - q","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"}]},{"title":"Stochastic Process (Expectation)","slug":"Stochastic-Process-3","date":"2018-07-22T02:30:26.000Z","updated":"2018-08-14T02:44:03.525Z","comments":true,"path":"2018/07/22/Stochastic-Process-3/","link":"","permalink":"http://yoursite.com/2018/07/22/Stochastic-Process-3/","excerpt":"","text":"ExpectationDefinition (Expectation of R.V.) E(X) = \\sum_{x}\\underbrace{x}_{value} \\underbrace{P(X=x)}_{PMF\\ at\\ x}Theorem (Monotonicity): $X$ and $Y$ are r.v.s. such that $X&gt;Y$ with probability 1.Then $E(X)\\geq E(Y)$ Theorem (Expectation via Survial Function): Let $X$ be a nonnegative r.v. Let $F$ be the CDF of $X$, and define survial function of $X$ named $G$ as $G(x) = 1-F(x) = P(X&gt;x)$, Then E(x) = \\sum_{n=0}^{\\infty} G(x)Theorem (Low Of The Unconscious Statistician(LOTUS)): If $X$ is discrete r.v. and $g$ is a function from $R$ to $R$, then E(g(x)) = \\sum_x g(x)P(X=x)Propertise of Expectation $E(X+Y) = E(X) + E(Y)$ $E(cX) = c E(x)$ If $X$ and $Y$ are independent, $E(XY) = E(X) E(Y)$ Inequalities of Expectation Cauchy–Bunyakovsky–Schwarz inequalityE[XY]^2\\leq E[X^2] E[Y^2] https://en.wikipedia.org/wiki/Expected_value#Inequalities … VarianceDefinition (Variance and Standard Deviation) variance of an r.v. $X$ is Var(X) = E(X-EX)^2Square root of the variance is standard deviation (SD): SD(X) = \\sqrt{Var(X)}Propertise of Variance For any r.v. $X$, $Var(X) = E(X^2) - (EX)^2$ $Var(X + c ) = Var(X)$ $Var(c X ) = c^2Var(X)$ If $X$ and $Y$ are independent, then $Var(X+Y) = Var(X) + Var(Y)$ Geometric and Negative BinomialDefinition (Geometric Distribution): Consider a sequence of independent Bernoulli trials, each with the same success probability $p\\in (0,1)$, trails performed until a success occurs. Let $X$ be the number of the failures before the first successful trail. Then $X$ has the Geometric Distributions, denote by $X\\sim Geom(p)$ Theorem (Geometric PMF): If $X\\sim Geom(p)$, then the PMF of $X$ is P(X=k) = (1-p)^kpTheorem (Memoryless Property): If $X\\sim Geom(p)$, then for positive integer n P(X\\geq n+k | X \\geq k ) = P(X\\geq n)Definition (First Success Distribution): very similay to Geometric $X$, Let it be $Y$, and $X+1 = Y$ …. , we denote it by $FS(p)$ Definition (Negative Binomial Distribution): In a sequence of independent Bernoulli trails with p, if $X$ is the number of failures before $r^{th}$ success, then $X$ is the Negative Binomial Distribution with $r$ and $p$, denoted by $X\\sim NBin(r, p)$ Theorem (Negative Binomial PMF): If $X\\sim NBin(r,p)$, then the PMF of $X$ is P(X=n) = \\left ( \\begin{array}{c} n+r-1 \\\\ r-1 \\end{array} \\right )p^r(1-p)^nTheorem (Geometric &amp; Negative Binomial): Let $X\\sim NBin(r,p)$, and $X_i$ are $i.i.d. Geom(p)$ , Then we have $X= X_1+\\dotsb + X_r$ Indicator R.V.Definition (Indicator R.V.) I_A = \\left \\{ \\begin{array}{ll} 1 & if\\ A\\ occurs \\\\ 0 & otherwise \\end{array} \\right .Propertise of Indicator R.V. $(I_A)^k = I_A$ $I_{A^c} = 1- I_A$ $I_{A\\cap B} = I_A I_B$ $I_{A\\cup B} = I_A + I_B - I_A I_B$ Theorem (Bridge between Probability &amp; Expectation) P(A) = E(I_A)Example 1: Au urn contain R G B three balls, r g b is probability draw a ball from it (r+g+b = 1), whats the expected number of different colors of ball before getting the first R ball ?Let $I_g$ be the $1$ if G is obtained before R, and define the $I_b$ similarly. Then E(I_g) = P(green\\ before\\ red) = \\frac{g}{g+r}since “green before red” means that first non-blue ball is green , so probability is $frac{g}{g+r}$, then, the final result is E(I_g+I_b) = \\frac{g}{g+r} + \\frac{b}{b+r}Moments &amp; IndicatorsGiven n events $A_1,\\dotsb, A_n$ and indicators $I_j, j = 1, \\dotsb, n$ $X = \\sum_{j=1}^n I_j$: the number of events occur \\left( \\begin{array}{c} X \\\\ 2 \\end{array} \\right) = \\sum_{i","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Neural Style","slug":"Neural-Style","date":"2018-07-19T02:25:16.000Z","updated":"2018-07-22T02:31:09.823Z","comments":true,"path":"2018/07/19/Neural-Style/","link":"","permalink":"http://yoursite.com/2018/07/19/Neural-Style/","excerpt":"","text":"Nerual StyleThere are two aspect for a image, one is the content of the image, which can be descriped as elements or object in the image, another is the style of the image, it might be abstract, and usually revealed by the painting skill or technique. Shortly, we have two image, one for style while the other for content. now, we want to combine the style in image1 and the content in image2 together, and it can be achieved from deep neural net work, and we call it Neural Style. Moreover we simply define the loss function care both style and content L_{total} = \\alpha L_{content}+\\beta L_{style}Now let’s have a look about what neural network can do here, and analysis the affect of $L_{content}$ and $L_{style}$ independently. Suppose we have the content image, and send it to the neural network, it will have the responses in each layer by filters, we also construct a white noisy image, filter it in the same way, and define a loss $L_{content}$ between filtered content and filtered noisy, we take the noisy image as input,and it can update iterativly. The image above show the reconstruction result between different layers, and reconstruction from lower layers(a,b,c) is alomost perfect, the style reconstruction may be more realistic in the deeper layer. Let’s get familiar with some notion of the formulation first( suppse we are in the $l^{th}$ level of the net ): $\\vec{p}$: Original content image (input) $P^l$: Content feature representation in layer $l$ respect to $\\vec{p}$ $\\vec{a}$: Original style image (input) $A^l$: Style feature representation in layer $l$ respect to $\\vec{a}$ $\\vec{x}$: Target image (output) $F^l$: Content feature representation in layer $l$ respect to $\\vec{x}$ $G^l$: Style feature representation in layer $l$ respect to $\\vec{x}$ $F_{ij}^l$: Element of $i^{th}$ filter at $j^{th}$ position in layer $l$ $N_l$: The number of the filters in the $l^{th}$ level $M_l$: The size of a feature map produced by a filter,usually it equals to $height \\times weight$ The squared-error loss between two content feature representations is: L_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2} \\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2In each layer, build a style representation compute the correlations between the different filter responses, which is called Gram Matrix $G^l\\in R^{N_l \\times N_l}$, and $G_{ij}^l$ is the inner product between the vectorized feature map between $i$ and $j$ in layer $l$ G_{ij}^l = \\sum_k F_{ik}^l F_{jk}^lAlso we have A_{ij}^l = \\sum_k P_{ik}^l P_{jk}^lThe contribution of the layer to the total loss is E_l = \\frac{1}{4N_l^2 M_l^2 }\\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2And the total loss is L_{style}(\\vec{a},\\vec{x}) = \\sum_{l=0}^L w_lE_lLet’s focus more on the detail about the gradient of the loss: The derivative of content loss respect to activations in layer l equals \\frac{\\partial L_{content}}{\\partial F_{ij}^l} = \\left \\{ \\begin{array}{ll} (F^l - P^l)_{ij} & if\\ F_{ij}^l > 0 \\\\ 0 & if\\ F_{ij}^l < 0 \\end{array} \\right .The derivative of style loss respect to activations in layer l equals \\frac{\\partial E_l}{\\partial F_{ij}^l} = \\left \\{ \\begin{array}{ll} \\frac{1}{N_l^2M_l^2}((F^l)^T(G^l - A^l))_{ij}& if\\ F_{ij}^l > 0 \\\\ 0 & if\\ F_{ij}^l < 0 \\end{array} \\right .The final loss function we want to minimize is L_{total}(\\vec{p},\\vec{a},\\vec{x}) = \\alpha L_{content}(\\vec{p},\\vec{x}) + \\beta L_{style}(\\vec{a},\\vec{x}) Fast Neural Style FastNet","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]},{"title":"Stochastic Process (Random Variable)","slug":"Stochastic-Process-1","date":"2018-07-07T00:21:04.000Z","updated":"2018-08-14T02:44:07.456Z","comments":true,"path":"2018/07/07/Stochastic-Process-1/","link":"","permalink":"http://yoursite.com/2018/07/07/Stochastic-Process-1/","excerpt":"","text":"There are some notation occationals Definition (Discrete Random Variable) A variable $X$ is discrete if there is a finite list of value $a_1,a_2,…,a_n$ that $P(X=a_j) = 1$, $P(X=x)&gt;0$ is the support of $X$ Definition (Probability Mass Function) The probability mass function (PMF) of a discrete r.v. $X$ is the function $p_X$ given by $p_X(x) = P(X=x)$. Bernoulli &amp; BinomialDefinition (Bernoulli Distribution) shortly, $P(X=1)=p$ and $P(X=0) = 1 - p$, and write as $X\\sim Bern(p)$ Definition (Indicator Random Variable) The indicator random variable of an event $A$ is the r.v. equals 1 if $A$ occurs and 0 otherwise, We denote the indicator of $A$ by $I_A$ or $I(A)$. Note $I_A \\sim Bern(p)$ with p = P(A) Theorem (Binomial PMF) Binomial Distribution is the repeatation of Bernoulli Distribution. If $X\\sim Bin(n, p)$ then the PMF of X is P(X=k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k}Hypergeometricurn Model A box is fiiled with $w$ white and $b$ black balls, then drawing n balls With replacement: $Bin(n,w/(w+b))$ for the number of white balls Without replacement : Hypergeometric distribution $HGeom(w,b,n)$ Theorem (Hypergeometric PMF) If $X \\sim HGeom(w,b,n)$, then the PMF of $X$ is P(X=k) = \\frac {\\left ( \\begin{array}{c} w \\\\ k \\end{array} \\right ) \\left( \\begin{array}{c} b \\\\ n-k \\end{array} \\right)} {\\left( \\begin{array}{c} w+b \\\\ n \\end{array} \\right)}Zipf Distribution If $X\\sim Zipf(\\alpha &gt; 0)$, then PMF of $X$ is: P(X=k) = \\frac{\\frac{1}{k^{\\alpha + 1}}} {\\sum_{j=1}^{\\infty}(\\frac{1}{j})^{\\alpha + 1}} Zipf Distribution can measure the Word Frequency Cumulative Distribution FunctionsDefinition (Cumulative Distribution Function) The cumulative distribution function(CDF) os an r.v. $X$ is the function $F_X$ given by $F_X(x) = P(X\\leq x)$ Theorem (Valid CDFs) CDF has the following properties Increasing: If $x_1 &lt; x_2$, then $F(x_1) &lt; F(x_2)$ Right-Continuous: $F(a) = lim_{x\\rightarrow a^+} F(x)$ Convergence to $0$ and $1$: $lim_{x\\rightarrow - \\infty} F(x) = 0$ and $lim_{x \\rightarrow \\infty} F(x) = 1$ Functions of Random Variable:Definition (Function of an r.v.) An experiment with sample space S, an r.v. $X$, and a function $g$, also the $g(X)$ is the variable that maps $s$ to $g(X(s))$, for all $s\\in S$ Theorem (PMF of $g(X)$) for all y in the support of $g(X)$ P(g(X) = y) = \\sum_{x:g(x)=y} P(X=x)The function of r.v. map the sample space into real number, which is easy for us calculate in mathematic. Independence of R.V.sDefinition (Independence of two R.V.s) Random variables $X$ and $Y$ are said to be independent P(X\\leq x,Y\\leq y) = P(X\\leq x)P(Y\\leq y)for all $x,y\\in R$,In the discrete case, equivalent to : P(X=x,Y=y) = P(X=x)P(Y=y)Definition (Independence of many R.V.s) Random variables $X_1,…,X_n$ are independent if P(X_1 \\leq x_1,\\dotsb , X_n \\leq x_n) = P(X_1 \\leq x_1) \\dotsb P(X_n \\leq x_n)for all $x_1,\\dotsb,x_n \\in R$ Definition (i.i.d) We call some r.v. that are independent and have the same distribution independent and identicallly distributed or i.i.d for short Independent: r.v.s provide no information about each others Identically distributed: r.v.s have the same PMF Theorem If $X\\sim Bin(n,p)$ , $Y \\sim Bin(m,p)$, and $X$ is independent of $Y$, then $X+Y \\sim Bin(n+m,p)$ Definition (Conditional Independence of two R.V.s) P(X\\leq x, Y \\leq y| Z= z) = P(X\\leq x |Z =z) P(Y\\leq y|Z=z)w","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Stochastic Process (Conditional Probability)","slug":"Stochastic-Process-2","date":"2018-07-06T00:24:34.000Z","updated":"2018-08-14T02:44:10.103Z","comments":true,"path":"2018/07/06/Stochastic-Process-2/","link":"","permalink":"http://yoursite.com/2018/07/06/Stochastic-Process-2/","excerpt":"","text":"Defination of Conditonal ProbabilityDefination Two events $A$ and $B$, with $P(B)&gt;0$, the conditional probability of $A$ given $B$ , denoted by $P(A|B)$, is defined as : P(A|B)=\\frac{P(AB)}{P(B)} $P(A)$: prior probability $P(A|B)$: posterior probability Bayes’ Rule &amp; LOTPChain Rule chain rule P(A_1,...,A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1,...,A_{n-1})Bayes’ RuleP(A|B) = \\frac{P(B|A)P(A)}{P(B)}LOTP (Law of Total Probability) Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_1)&gt;0$, Then : P(B) = \\sum_i^n P(B|A_i)P(A_i)Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, for any event $B$ such that $P(B) &gt; 0$, we have : P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\\dotsb+P(A_n)P(B|A_n)} Conditional ProbabilitiyConditional Probability is also the probability, so it inherent the property of probability (suppose the sample space is $S$): $P(S|E) = 1$ and $P(\\emptyset|E) = 0$ if events $A_1,…$ are disjoint, then $P(\\cup_{j=1}^{\\infty}A_j|E) = \\sum_{j=1}^{\\infty}P(A_j|E)$ $P(A^c|E) = 1 - P(A|E)$ Inclusion-Exclusion : $P(A\\cup B|E) = P(A|E) + P(B|E) - P(A\\cap B|E)$ Bayes’ Rule with Extra Condition:Theorem: Provided that $P(A\\cap E)&gt;0$ and $P(B\\cap E)&gt;0$, we have: P(A|B,E) = \\frac{P(B|A,E)P(A|E)}{P(B|E)}LOTP with Extra Condition:Theorem: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_i \\cap E) &gt;0$, Then: P(B|E) = \\sum_{i=1}^n P(B|A_i,E)P(A_i|E)Approaches for $P(A|B,C)$ P(A|B,C) = \\frac{P(A,B,C)}{P(B,C)} P(A|B,C) = \\frac{P(B|A,C)P(A|C)}{P(B|C)} P(A|B,C) = \\frac{P(C|A,B)P(A|B)}{P(A|C)} Independence of EventsIndependence of Two EventsDefination: Events $A$ and $B$ are independent if P(A\\cap B) = P(A) P(B) \\Leftrightarrow P(A|B) = P(A) , P(B|A) = P(B)Independence vs Disjointness $A,B$ is disjoint : $P(A\\cap B) = 0$ $A,B$ is independent : $P(A) = 0, P(B) = 0$ Conditional IndependenceDefination: Events $A$ and $B$ are conditionally independent given E if: P(A\\cap B|E) = P(A|E)P(B|E)Contitional\\ Independence \\nRightarrow IndependenceIndependence \\nRightarrow Contitional\\ Independence","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]},{"title":"Robust PCA","slug":"Robust-PCA","date":"2018-07-04T14:47:11.000Z","updated":"2018-07-25T11:29:24.589Z","comments":true,"path":"2018/07/04/Robust-PCA/","link":"","permalink":"http://yoursite.com/2018/07/04/Robust-PCA/","excerpt":"","text":"Introduction to RPCAThe data we collect usually have the low rank property, but the property will vanished when the data is collected causing the noisy, but we can still decomposite the matrix into low-rank matrix and spares error matrix from the corruped data. D=\\underbrace{A}_{\\text{low rank matrix}}+\\underbrace{E}_{\\text{sparse matrix}} \\notagTraditional approach for solving this problem is using PCA (Principal Components Analysis), there are many interpretation to PCA, one relate to rank is despiting the low value singular value as this componets contribute less to the data. Thus, it can be considered as the noisy. So we take the $k^{th}$ largest singular value and drop the rest , this can be represnt as the following formulation : \\mathop{min}_{A,E} \\|E \\|_F, \\ \\ \\ \\ \\text{subject to } \\ rank(A)\\leq r, D = A + E \\notagPCA \bhas a shortage that it is not robust to the outliers, then the RPCA (Robust Principal Components Analysis) came out, RPCA could making the matrix recovery whether the noisy is large or not only if the sparse property is confirmed, the original form of the RPCA can be written as : \\mathop{min}_{A,E} rank(A) + \\|E\\|_0, \\ \\ \\ \\ \\text{subject to } \\ D = A + E \\notagThe optimization formulation above is non-convex and is hard to get the solution, we can use the convex relax technology apply on it, then it turn out into the most used and the most efficient from: \\mathop{min}_{A,E} \\|A\\|_* + \\|E\\|_1, \\ \\ \\ \\ \\text{subject to } \\ D = A + E \\notag$| \\cdot |_*$ is the unclear norm, which is the sum of the all singular values : $\\sum_i^n\\sigma_i$, $l_1$ norm of matrix $| \\cdot |_1$ is the sum of absolute value of all the element : $\\sum_i^n \\sum_j^n |D_{ij}|$ . Algorithm of RPCABefore introducting the Algorithm, we first introducing the two operators Singular Value ThresholdingThe optimal solution to the optimization problem : $\\frac{1}{2} | X- Y |_F^2 + \\tau |X|_*$ with the variable $X$ is thresholing the singular value of $X$ \\begin{align} \\mathcal{D}_{\\tau}(X) := U \\mathcal{D}_{\\tau} (\\Sigma) V^{\\prime} , \\ \\ \\mathcal{D}_{\\tau}(\\Sigma) = diag ( \\{ \\sigma_i - \\tau \\} ) \\notag \\\\ \\mathcal{D}_{\\tau}(Y) = \\mathop{arg} \\mathop{min}_{X} \\left \\{ \\frac{1}{2} \\| X- Y \\|_F^2 + \\tau \\|X\\|_* \\right \\} \\notag \\end{align}Soft ThresholdingAs same as the $l_1$ norm in vector, thresholding the absolute value of all the element in $X$. \\begin{align} \\psi_{st}(Y) = \\mathop{arg} \\mathop{min}_{X} \\left \\{ \\frac{1}{2} \\| X- Y \\|_F^2 + \\tau \\|X\\|_1 \\right \\} \\notag \\end{align}There are various methods to solving the RPCA problem, the most successful one is slove the Augmented Lagrangian function of the original problem which we called ALM algorithm, the Augmented Lagrangian function is: \\begin{align} L(A,E,Y,\\mu) = \\|A\\|_* + \\lambda\\|E\\|_1+ \\langle Y,D-A-E \\rangle + \\frac{\\mu}{2} \\| D- A -E \\|_F^2 \\notag \\end{align}Usually, we use ADMM to slove the ALM problems : \\begin{align} A_{k+1} &= SVT_{1/\\mu_k}(D-E_k + \\mu_k^{-1} Y_k) \\notag \\\\ E_{k+1} &= ST_{\\lambda/\\mu_k} (D - A_{k+1} + \\mu_k^{-1} Y_k) \\notag \\\\ Y_{k+1} &= Y_k + \\mu_k ( D - A_{k+1} - E_{k+1} ) \\notag \\end{align}","categories":[],"tags":[{"name":"Low-Rank","slug":"Low-Rank","permalink":"http://yoursite.com/tags/Low-Rank/"}]},{"title":"Stochastic-Process （Abbreviate & Notation）","slug":"Stochastic-Process-0","date":"2018-07-02T08:54:34.000Z","updated":"2018-08-14T02:44:12.387Z","comments":true,"path":"2018/07/02/Stochastic-Process-0/","link":"","permalink":"http://yoursite.com/2018/07/02/Stochastic-Process-0/","excerpt":"","text":"r.v.s.: random variable sequence i.i.d: individual identical distribution w.p.: with probability PMF (Probability Mass Function) $P(X=x)$ CDF (Cumulative Distribution Function) $P(X \\leq x)$ PDF (Probability Density Function) derivate of CDF PGF (Probability Generating Function) $E(t^X) = \\sum_{k=0}^{\\infty} p_k t^k$ MGF (Moment Generating Function) $M(t) = E(e^{tX})$ Distributions Binomial Distribution $X\\sim B(n,p)$: number of success in n trails HyperGeometric Distribution $X\\sim HGeom(w,b,n)$: draw n balls between w white and b black Geometric Distribution $X\\sim Geom(p)$: number of the Bernoulli trails before success (First Success Distribution) Negative Binomial Distribution $X\\sim NBin(r,p)$: number of the Bernoulli trails before $r^{th}$ success Poisson Distribution $X\\sim Pois(\\lambda)$: number of times an event occurs in an interval of time or space Uniform Distribution $U\\sim Unif(a,b)$: Distribution on the interval $(a,b)$ Standard Normal Distribution $X\\sim N(0,1)$ Normal Distribution $X\\sim N(\\mu,\\sigma^2)$ Beta Distribution $X\\sim Beta(a,b)$ Multinomial Distribution $\\mathbf{X}\\sim Mult_k(n,\\mathbf{p})$","categories":[],"tags":[{"name":"Probability","slug":"Probability","permalink":"http://yoursite.com/tags/Probability/"}]}]}