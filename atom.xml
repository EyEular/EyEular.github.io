<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EyEular</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-16T08:14:42.834Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Eulring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MatplotLib-1</title>
    <link href="http://yoursite.com/2018/09/16/MatplotLib-1/"/>
    <id>http://yoursite.com/2018/09/16/MatplotLib-1/</id>
    <published>2018-09-16T08:14:42.000Z</published>
    <updated>2018-09-16T08:14:42.834Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP-1</title>
    <link href="http://yoursite.com/2018/09/14/NLP-1/"/>
    <id>http://yoursite.com/2018/09/14/NLP-1/</id>
    <published>2018-09-14T12:01:35.000Z</published>
    <updated>2018-09-16T08:13:31.827Z</updated>
    
    <content type="html"><![CDATA[<p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;.&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PGM-Approximate Inference</title>
    <link href="http://yoursite.com/2018/09/11/PGM-4/"/>
    <id>http://yoursite.com/2018/09/11/PGM-4/</id>
    <published>2018-09-11T00:50:05.000Z</published>
    <updated>2018-09-14T11:36:35.442Z</updated>
    
    <content type="html"><![CDATA[<p>概率的推断就是计算 conditional 和 marginal，之前我们学习了 exact inference 也就是准确的推断概率图概率，我们学习message passing 算法、sum-product、</p><p>inference is answer a query</p><p>Approximate inference 就是对于 inference 的一个数值估计，不一定最后的结果要在 0-1 之内</p><h1 id="Exact-Inference-Revisit"><a href="#Exact-Inference-Revisit" class="headerlink" title="Exact Inference Revisit"></a>Exact Inference Revisit</h1><h3 id="Sum-Product"><a href="#Sum-Product" class="headerlink" title="Sum-Product"></a>Sum-Product</h3><p><img src="/2018/09/11/PGM-4/pgm4img3.jpg" align="justify"></p><p><img src="/2018/09/11/PGM-4/pgm4img1.jpg" align="justify"></p><h3 id="Factor-Graph"><a href="#Factor-Graph" class="headerlink" title="Factor Graph"></a>Factor Graph</h3><p><img src="/2018/09/11/PGM-4/pgm4img4.jpg" align="justify"></p><h3 id="Junction-Tree"><a href="#Junction-Tree" class="headerlink" title="Junction Tree"></a>Junction Tree</h3><p><img src="/2018/09/11/PGM-4/pgm4img2.jpg" align="justify"></p><blockquote><p>在 Junction Tree 中 local Consistency 等价于 global Consistency</p></blockquote><h1 id="Loopy-Belief-Propagation"><a href="#Loopy-Belief-Propagation" class="headerlink" title="Loopy Belief Propagation"></a>Loopy Belief Propagation</h1><p>Junction Tree 虽然可以处理所有的 graph，但是只适用于树形结构，在密集结构中用 Junction Tree 复杂度依然会比较高，假设树变成了 gird，我们不打算用 Junction Tree 算法来计算出这个图的 inference 的精确解。<br><br><img src="/2018/09/11/PGM-4/pgm4img5.jpg" align="justify"></p><h3 id="LBP-The-Algorithm"><a href="#LBP-The-Algorithm" class="headerlink" title="LBP : The Algorithm"></a>LBP : The Algorithm</h3><p>我们在这个图上运行直接做 Belief Propagation，在树形结构上，Belief Propagation 只要来回传递两次就能够得到精确解了，但是在这个图上，我们需要多次的运行 BP ，最终可能会收敛，也有可能会呈现出周期性的数值变化，也就是不收敛。</p><p>一般来讲好的 近似可以通过以下的方式</p><ul><li>在固定的迭代次数后停止</li><li>如果结果没有明显变化，就停止</li><li>如果在数值上没有震荡，并且收敛了，那么通常就是接近真实了</li></ul><p><img src="/2018/09/11/PGM-4/pgm4img6.jpg" align="justify"></p><h3 id="LBP-The-Bethe-Approximation"><a href="#LBP-The-Bethe-Approximation" class="headerlink" title="LBP : The Bethe Approximation"></a>LBP : The Bethe Approximation</h3><p>我们对于 LBP 算法的正确性做一个分析：</p><p>一般来讲，真实的分布 P 是这样子的：</p><script type="math/tex; mode=display">P(X) = \frac{1}{Z} \prod_{f_a \in F} f_a(X_a)</script><p>但是这种分布的计算很困难。。</p><p>假设我们有另一个分布 Q ，在后面，Q 是我们近似得到的分布，我们希望来评价 Q 和 P 的相似度，对于某一事件不同概率的衡量，最常用的就是相对熵 KL Divergence ：</p><script type="math/tex; mode=display">KL(Q_1\Vert Q_2) = \sum_X Q_1(X) log(\frac{Q_1(X)}{Q_2(X)})</script><p>相对熵是来衡量两个取值为正的函数或者概率分布之间的差异的，有以下的特性：</p><ul><li>$KL(Q_1\Vert Q_2) \geq 0$</li><li>$KL(Q_1\Vert Q_2) = 0$ iff $Q_1=Q_2$</li></ul><p>相对熵还可以写成 <strong>信息熵</strong> 减去 <strong>交叉熵</strong>：</p><script type="math/tex; mode=display">\begin{align}KL(Q||P) &= \sum_X Q(X)log Q(X) - \sum_X Q(X) log P(X) \\&= -H_Q(X) -E_Q logP(X)\end{align}</script><p>我们把真实分布带入到 $P(X)$ 中，可以得到：</p><script type="math/tex; mode=display">\begin{align}  KL(Q||P) &= -H_Q(X) - E_Q log (\frac{1}{Z} \prod_{f_a \in F} f_a(X_a)) \\  &= -H_Q(X) - E_Q\sum_{f_a \in F}log f_a(X_a) + E_Q log Z\end{align}</script><p>我们定义一下 free-energy 为前两项：</p><script type="math/tex; mode=display">F(P,Q) = -H_Q(X) - \sum_{f_a \in F} E_Q log f_a(X_a)</script><p>对于 Energy Functional：</p><ul><li>$\sum_{f_a \in F} E_Q log f_a(X_a)$ 的计算比较方便。。。</li><li>$H_Q$ 的计算会比较复杂，因为我们需要遍历所有 $X$ 的取值再做计算<br>-</li></ul><p>d</p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率的推断就是计算 conditional 和 marginal，之前我们学习了 exact inference 也就是准确的推断概率图概率，我们学习message passing 算法、sum-product、&lt;/p&gt;
&lt;p&gt;inference is answer a 
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>NLP-Word-Vector</title>
    <link href="http://yoursite.com/2018/09/09/NLP-0/"/>
    <id>http://yoursite.com/2018/09/09/NLP-0/</id>
    <published>2018-09-09T07:50:49.000Z</published>
    <updated>2018-09-14T02:23:40.457Z</updated>
    
    <content type="html"><![CDATA[<p>Word Vector 也就是词向量可以分为两种</p><ul><li>Count Based：这种方法是通过统计完全局的信息最后来做特征提取</li><li>Direct Prediction：这种方法只选取来局部的统计，但是能够直接进行计算</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img5.jpg" align="justify"></p><p>有一种方法能够统一上面两种性质称为 GloVe</p><p>那么我们来介绍一下几种常见的方法</p><ul><li>SVD</li><li>CBOW</li><li>SkipGram</li><li>GloVe</li></ul><p>词向量也就是把单词转换成为向量的表示，这样方便计算机进行计算<br>运算</p><p>首先，英语单词的数量很多有将近 13m，同时我们定义所有的单词集合为 $V$，以及单词的数量为 $|V|$</p><p>最简单的词向量是 <strong>one-hot Vector</strong></p><hr><p>在讲词向量时，我们先来引出共现矩阵 co-occurrence matrix 的概念，我们假设共现矩阵为 X ，其元素 $X_{i,j}$ 表示单词 i 和单词 j 在同一个窗口一起出现的次数的统计，这里的统计是对于某个数据库下的统计。<br>共现矩阵在很多的算法中都会出现，为了让共现矩阵更加完善，我们会作出一下修改和限制，比如，我们会抑制共现矩阵中出现的较大元素让他们 $\leq 100$ ，比如一些常见的单词 ‘the’ ‘he’ 等等造成的统计<br>使用 ramp window 也就是共现矩阵中的元素更新不再是 +1 而是根据离中心词的距离加权考虑</p><hr><h3 id="SVD-Method"><a href="#SVD-Method" class="headerlink" title="SVD Method"></a>SVD Method</h3><p>直接对于共现矩阵做 SVD 分解，分解后选取前 k 大的特征值对应的特征向量来作为词向量</p><p><img src="/2018/09/09/NLP-0/nlp0img1.jpg" align="justify"></p><p>这个方法其实是有很多缺点的</p><ul><li>矩阵是稀疏的，因为很多次是不会一起出现的</li><li>矩阵的维度很大，所以做 SVD 很花费时间</li></ul><hr><h3 id="CBOW-（Continuous-Bag-of-words-Models）"><a href="#CBOW-（Continuous-Bag-of-words-Models）" class="headerlink" title="CBOW （Continuous Bag of words Models）"></a>CBOW （Continuous Bag of words Models）</h3><p>CBOW 其实就是计算以某个单词为中心，固定一个窗口，计算周围单词出现的概率乘积，然后对于这个乘积就是这个事件的概率，与此同时在训练的过程中，我们</p><p>首先我们来看看这个问题的几个参数</p><ul><li>$w_i$ : 在字典 $|V|$ 中的单词 i</li><li>$V (n\times |V|)$ : context 词向量矩阵</li><li>$U (n\times |V|)$ : center 词向量矩阵</li><li>$v_i (n\times 1)$ : V 矩阵的某一列，也就是 $w_i$ 的上下文词向量</li><li>$u_i (n\times 1)$ : U 矩阵的某一列，也就是 $w_i$ 的中心词向量</li><li>$m$ : 窗口的大小</li></ul><h5 id="CBOW-Algorithm"><a href="#CBOW-Algorithm" class="headerlink" title="CBOW Algorithm"></a>CBOW Algorithm</h5><ul><li>对于 context 单词，我们生成 2m 个的 one-hot 向量 $[ x^{(c-m)},…,x^{(c-1)},x^{(c+1)},x^{(c+m)} ]$</li><li>用 context 词向量矩阵乘以 one-hot vector 从而得到 context 单词对应的 context 词向量 $v_i = Vx^i$</li><li>将这些得到的 context 词向量取均值得到 $\hat{v} = \frac{v_{c-m}+…+v_{c+m}}{2m}$</li><li>用 center 词向量矩阵去乘以上面得到的 context 均值词向量矩阵得到：所有单词以中心词向量表示于所有 context 的乘积，结果就是对于每个单词的一个 score : $z = U\hat{v}$</li><li>我们将 score 转换成概率，用 softmax 来实现：$\hat{y} = softmax(z)$</li><li>首先我们是知道真实的分布的也就是中心单词的 one-hot vector ，为 $y$ , 这样我们就可以用各种 loss function 来优化了，一般是 cross entropy 。。</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img2.jpg" align="justify"></p><p>上面这张图 $W$ 相当于 context matrix ，$W^{\prime}$ 相当于 center matrix</p><hr><h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><h5 id="Skip-Gram-Algorithm"><a href="#Skip-Gram-Algorithm" class="headerlink" title="Skip-Gram Algorithm"></a>Skip-Gram Algorithm</h5><ul><li>对于 center 单词，生成它的 one-hot 向量</li><li>然后再获取中心单词的词向量 $v_c = V x$</li><li>用上面的中心词向量乘以 context 词向量矩阵得到一个 $|V|$ 维向量</li><li>对于这一个向量，我们取不同的位置的值做 softmax 预测，从而生成对应的 loss function</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img4.jpg" align="justify"></p><p>上面一张图是 CBOW 和 Skip-Gram 算法之间的比较，总的来说</p><ul><li>CBOW ：根据周围单词，来估计中心单词出现的概率</li><li>Skip-Gram ：根据中心单词，来估计周围单词出现的概率</li></ul><hr><h3 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h3><p>直接写出 Glove 模型的优化函数吧：</p><script type="math/tex; mode=display">J(\theta)  = \frac{1}{2} \sum_{i,j=1}^w f(X_{ij}) (u_i^Tv_j - log X_{ij})^2</script><blockquote><p>对于上面这个公式，f 其实是一个权重函数，$\theta$ 是所有的变量 $U,V$</p></blockquote><p>是</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Word Vector 也就是词向量可以分为两种&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Count Based：这种方法是通过统计完全局的信息最后来做特征提取&lt;/li&gt;
&lt;li&gt;Direct Prediction：这种方法只选取来局部的统计，但是能够直接进行计算&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Sparse-Coding</title>
    <link href="http://yoursite.com/2018/09/09/Sparse-Coding/"/>
    <id>http://yoursite.com/2018/09/09/Sparse-Coding/</id>
    <published>2018-09-09T01:21:38.000Z</published>
    <updated>2018-09-09T07:48:19.909Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经学启发"><a href="#神经学启发" class="headerlink" title="神经学启发"></a>神经学启发</h1><p>稀疏编码的概念来自于神经生物学。生物学家提出，哺乳类动物在长期的进化中，生成了能够快速，准确，低代价地表示自然图像的视觉神经方面的能力。我们直观地可以想象，我们的眼睛每看到的一副画面都是上亿像素的，而每一副图像我们都只用很少的代价重建与存储。我们把它叫做稀疏编码，即Sparse Coding。</p><p>从上可以看出稀疏编码的目的是：在大量的数据集中，选取很小部分作为元素来重建新的数据。</p><hr><h1 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h1><p>稀疏编码是一种 unsupervised learning，我们希望找到一组 over-complete 的 基向量 basic vector 来表示我们的数据，也就是数据 $\textbf{x}$ 可以表示为这些基向量的线性组合：</p><script type="math/tex; mode=display">  \textbf{x} = \sum_{i=1}^k a_i \phi_i</script><p>对于基向量的学习，我们一般有一组训练数据，另外，基向量的大小规定为 $\textbf{x}\in R^n$，同时，也是数据的大小。一般来讲，$n$ 维的数据最多只需要 $n$ 个线性不相关的基向量就可以了（使用 PCA），但是就 n 个基向量的话，可能有一个现象就是，没个数据可能都需要接近 n 个基向量来表示，我们希望 稀疏编码 能有一个优势，就是组成数据的基向量个数尽可能的小一些，也就是上面的 $\mathbf{a}$ 是稀疏的。。。</p><p>为了达成这个目的，我们可以增加基向量的个数 $k  &gt; n$，也就是说，一个数据，可能由多种基向量来表示了，在所有的表示中，我们可以尽可能的选取稀疏的表示方法。。</p><p>Sparse Coding 可以分为两个部分，一个是 Training 阶段，一个是 Coding阶段</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>给定一些列样本数据 $[ x_1,x_2,… x_m]$ 我们希望学到一组基 $[ \phi_1,\phi_2,…,\phi_k  ]$ 来表示前面的数据，训练的 objective function 如下：</p><script type="math/tex; mode=display">\mathop{min}_{a,\phi} \sum_{i=1}^m \left \Vert x_i-\sum_{j=1}^k a_{i,j}\phi_j \right \Vert^2 + \lambda \sum_{i=1}^m \sum_{j=1}^k |a_{i,j}|</script><p>优化的迭代分为两部（都可以用凸优化来求解）</p><ul><li>固定字典 $\phi$ 更新 $a$，这个问题其实就是一个 Lasso 问题</li><li>固定表达 $a$ 更新 $\phi$，这个其实就是一个 QP 问题</li></ul><h3 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h3><p>给定一个新的数据，获得它关于字典的表达，这一步，其实就是上面的优化的第二步</p><script type="math/tex; mode=display">\mathop{min}_{a} \sum_{i=1}^m \left \Vert x_i-\sum_{j=1}^k a_{i,j}\phi_j \right \Vert^2 + \lambda \sum_{i=1}^m \sum_{j=1}^k |a_{i,j}|</script><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/aixueshuqian/p/3936892.html" target="_blank" rel="noopener">https://www.cnblogs.com/aixueshuqian/p/3936892.html</a><br><a href="https://www.cnblogs.com/caocan702/p/5666175.html" target="_blank" rel="noopener">https://www.cnblogs.com/caocan702/p/5666175.html</a><br>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;神经学启发&quot;&gt;&lt;a href=&quot;#神经学启发&quot; class=&quot;headerlink&quot; title=&quot;神经学启发&quot;&gt;&lt;/a&gt;神经学启发&lt;/h1&gt;&lt;p&gt;稀疏编码的概念来自于神经生物学。生物学家提出，哺乳类动物在长期的进化中，生成了能够快速，准确，低代价地表示自然图像的
      
    
    </summary>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>PGM-3</title>
    <link href="http://yoursite.com/2018/08/18/PGM-3/"/>
    <id>http://yoursite.com/2018/08/18/PGM-3/</id>
    <published>2018-08-18T13:21:46.000Z</published>
    <updated>2018-09-11T00:50:11.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Exponential-Family"><a href="#The-Exponential-Family" class="headerlink" title="The Exponential Family"></a>The Exponential Family</h1><p>random variable $\mathbf{X}$ is in the exponential family</p><script type="math/tex; mode=display">\begin{align}P(\mathbf{X}=x;\eta) &= h(x)\mathop{exp}\{ \eta^T\mathbf{T}(x) - A(\eta) \} \\&= \frac{1}{Z(\eta)} h(x) exp \{ \eta^T T(x) \}\end{align}</script><ul><li>$\eta$ :  vector of natural parameters</li><li>$\mathbf{T}$ :  vector of sufficient statistics</li><li>$\mathbf{A}$ : log partition function</li><li>$log Z(\eta) = A(\eta)$</li></ul><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><h5 id="Multivariate-Gaussian"><a href="#Multivariate-Gaussian" class="headerlink" title="Multivariate Gaussian"></a>Multivariate Gaussian</h5><script type="math/tex; mode=display">\begin{align}  P(\mathbf{x};\mu;\Sigma) &=  \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \text{exp}\left(  -\frac{1}{2} (\mathbf{x}-\mu)^T \Sigma^{-1} (\mathbf{x}-\mu) \right) \\  &= \frac{1}{(2\pi)^{p/2}} \text{exp} \left( -\frac{1}{2}(\text{tr } \mathbf{x}^T\Sigma^{-1} \mathbf{x} +\mu^T \Sigma^{-1} \mu - 2 \mu^T \Sigma^{-1} \mathbf{x} +ln |\Sigma| )\right) \\  &=  \underbrace{ \frac{1}{(2\pi)^{p/2}} }_{h(\mathbf{x})} \text{exp} \left( -\frac{1}{2} \underbrace{\text{tr } \Sigma^{-1}\mathbf{x}\mathbf{x}^T}_{\text{vec}(\Sigma^{-1})^T \text{vec}(\mathbf{x}\mathbf{x}^T)} + \mu^T\Sigma^{-1}\mathbf{x} - \underbrace{\frac{1}{2} \mu^T \Sigma^{-1} \mu - \frac{1}{2} \text{ln}|\Sigma|   }_{A(\eta)} \right)\end{align}</script><p>This implies that:</p><ul><li>$\eta = \left( \Sigma^{-1} \mu, -\frac{1}{2} vec (\Sigma^{-1}) \right)$</li><li>$\mathbf{T}(\mathbf{x}) = (\mathbf{x}, vec(\mathbf{x}\mathbf{x}^T))$</li><li>$A(\eta) = \frac{1}{2} (\mu^T \Sigma^{-1} \mu + \text{ln} |\Sigma|)$</li><li>$h(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}}$</li></ul><h5 id="Bernoulli"><a href="#Bernoulli" class="headerlink" title="Bernoulli"></a>Bernoulli</h5><script type="math/tex; mode=display">\begin{align}P(x;p) &= p^x (1-p)^{1-x} \\\text{ln} P(x;p) &= x\text{ln}(p)+(1−x)\text{ln}(1−p) \\ &= x\text{ln}(p)−x\text{ln}(1−p)+\text{ln}(1−p) \\ &= x(\text{ln}(p)−\text{ln}(1−p))+\text{ln}(1−p) \\ &= x\text{ln} (\frac{p}{1-p}) + \text{ln} (1-p) \\ \text{exp} (\text{ln} P(x;p)) &= \text{exp} \left( x\text{ln} (\frac{p}{1-p}) + \text{ln} (1-p)  \right)\end{align}</script><p>This implies that:</p><ul><li>$\eta = \text{ln} (\frac{p}{1-p})$</li><li>$T(x) = x$</li><li>$A(\eta) = -\text{ln}(1-p)$</li><li>$h(x) = 1$</li></ul><h5 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h5><p>the univariate Gaussian, Poisson, gamma, multinomial, linear regression, Ising model, restricted Boltzmann machines, and conditional random fields (CRFs) are all in the exponential family</p><h3 id="Why-Exponential-Family"><a href="#Why-Exponential-Family" class="headerlink" title="Why Exponential Family"></a>Why Exponential Family</h3><h5 id="Moment-generating-property"><a href="#Moment-generating-property" class="headerlink" title="Moment generating property"></a>Moment generating property</h5><script type="math/tex; mode=display">\begin{align}  \int P(x,\eta) dx = \int h(x) e^{\eta^T T(x) - A(\eta)} dx &= 1 \\  \int h(x) e^{\eta^T T(x)} &= Z(\eta)\end{align}</script><p>-</p><script type="math/tex; mode=display">\begin{align}  \frac{dA}{d\eta} &= \frac{d}{d\eta} log (Z(\eta)) = \frac{1}{Z(\eta)} \frac{d}{d\eta} Z(\eta) \\  &= \frac{1}{Z(\eta)} \frac{d}{d\eta} \int h(x) e^{\eta^T T(x)}dx \\  &= \int T(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} = E[T(x)]\end{align}</script><p> -</p><script type="math/tex; mode=display">\begin{align}\frac{d^2 A}{d^2 \eta} &= \int T^2(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} dx - \int T(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} dx \frac{1}{Z(\eta)} \frac{d}{d \eta} Z(\eta) \\&= E[T^2(x)] - E^2[T(x)] \\&= Var[T(x)]\end{align}</script><ul><li><p>$A(\eta)$ is convex  since</p><script type="math/tex; mode=display">\frac{d^2 A(\eta)}{d \eta^2} = Var[T(x)] > 0</script></li><li><p>specific $\eta$ map to mean $\mu$, so we define an invert $\psi (\mu) = \eta$</p></li></ul><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;The-Exponential-Family&quot;&gt;&lt;a href=&quot;#The-Exponential-Family&quot; class=&quot;headerlink&quot; title=&quot;The Exponential Family&quot;&gt;&lt;/a&gt;The Exponential Fami
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>PGM-Exact Inference</title>
    <link href="http://yoursite.com/2018/08/14/PGM-2/"/>
    <id>http://yoursite.com/2018/08/14/PGM-2/</id>
    <published>2018-08-14T02:46:32.000Z</published>
    <updated>2018-09-13T01:24:04.963Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h1><p>某个确定了的概率图，它的推断可以看作是一个关于所有变量的函数，我们要求的是这个函数的具体值是多少，从概率的角度上消除变量，其实就是做这个函数的边缘化 marginalization，<br>我们尽量争取每次计算(消除)的变量比较少，这样总的复杂度不会高。。<br>Elimination 似乎可以适用于所有结构的 graph 。</p><h3 id="Directed-Chain"><a href="#Directed-Chain" class="headerlink" title="Directed Chain"></a>Directed Chain</h3><p>假设我们的图的结构是这样的 $A\rightarrow B\rightarrow C\rightarrow D\rightarrow E$</p><script type="math/tex; mode=display">\begin{align}P(e) &= \sum_{a,b,c,d} p(a,b,c,d) \\    &= \sum_{a,b,c,d} P (a)P (b|a)P (c|b)P (d|c)P (e|d) \\    &=\sum_{d,c,b} P(c|b) P(d|c) P(e|d) \sum_a P(a) P(b|a) \\    &\text{this is an one variable elimination cost } k^2\\    &= \sum_{d,c,b} P(c|b) P(d|c) P(e|d) p(b) \\    &\ \cdots \\    &= \sum_d P(e|d) p(d)\end{align}</script><p>复杂度 Complexity:</p><ul><li>Eliminate 方法: costs $O(k^2 n)$ _这里面的 $k^2$ 表示迭代 k 次，每次计算概率也要 k_</li><li>Naive 方法: cost $O(k^n)$</li></ul><h3 id="Undirected-Chain"><a href="#Undirected-Chain" class="headerlink" title="Undirected Chain"></a>Undirected Chain</h3><p>假设我们的图是这样的无向图 $A - B - C - D - E$</p><script type="math/tex; mode=display">\begin{align}P(e) &= \sum_{a,b,c,d} \frac{1}{Z} \phi(b,a) \phi(c,b) \phi(d,c) \phi(e,d) \\ &\propto \sum_{a,b,c,d} \phi(b,a) \phi(c,b) \phi(d,c) \phi(e,d) \\ &= \sum_{a,b,c,d}  \phi(c,b) \phi(d,c) \phi(e,d)\sum_a \phi(b,a)  \\ &= \sum_{a,b,c,d}  \phi(c,b) \phi(d,c) \phi(e,d) m_a(b)  \\ &\ \cdots \\ &= m_d(e)\end{align}</script><p>这里是无向图，原始的势函数运算成为 m 的结果不是概率，所以这里我们要 normalize 一下:</p><script type="math/tex; mode=display">P(e) =  \frac{m_d(e)}{\sum_e m_d(e)}</script><h3 id="Graph-Elimination"><a href="#Graph-Elimination" class="headerlink" title="Graph Elimination"></a>Graph Elimination</h3><p><img src="/2018/08/14/PGM-2/pgm2img1.jpg" align="justify"><br>我们从一张图来看 Elimination 的每一次过程后，剩下的图的结构</p><ul><li>对于一张 graph 首先我们确认 elimination 的 order</li><li>对于每一个待消除的变量，它会连接一些变量，我们将这些变量两两相连</li><li>消除待消除的变量</li></ul><p><img src="/2018/08/14/PGM-2/pgm2img9.jpg" align="justify"></p><p>我们再来观察每次 Elimination 后，也就是边缘化操作后形成的函数，发现这些函数的变量在一起，刚好能组成这个概率图结构的 cliques 集合，如果我们考虑这些 cliques 组成的树，那么 elimination 操作其实就是在这颗树上进行 message passing。</p><p>Key insight 就是这些 message 其实是可以 reused 的，重复使用是指，当我们要进行多次 querying 的时候，信息的重复使用，所以我们希望设计好的算法，能够在 querying 的过程中保存下来这些信息，于是有了后面的 Sum-Product 算法。</p><h3 id="Complexity-of-Variable-Elimination"><a href="#Complexity-of-Variable-Elimination" class="headerlink" title="Complexity of Variable Elimination"></a>Complexity of Variable Elimination</h3><p>这里 $y$ 未知的，后面操作可能要消除的变量，$x$ 是正在消除的变量，$m$ 是当前要消除的乘子，sum-product 算法分为下面两部</p><ul><li>Sum:<script type="math/tex; mode=display">m_x(y_1,...,y_k) = \sum_x m^{\prime}_x (x,y_1,...,y_k)</script></li><li>Product:<script type="math/tex; mode=display">m_x^{\prime} (x,y_1,...,y_k) = \prod_{i=1}^k m_i(x,y_{c_i})</script></li></ul><p></p><p>乘起来就是一次 Elimination 的复杂度： $k\cdot  | Var(X) |\cdot \prod_i |Var(Y_{C_i})|$<br>也就是当前变量的状态乘上，乘子也就是对应的 clique 的所有变量的状态叉乘</p><p>我们发现整个算法的复杂度取决于最大的最大的 clique，我们称这个 clique 的变量数量 k 为 <strong>Tree-width</strong> , 同时要注意的是，不同的 elimination order 的 Tree-Width 是不同的，找到最优的 order 是 np-hard 问题。。。</p><hr><p></p><h1 id="Belief-Propagation"><a href="#Belief-Propagation" class="headerlink" title="Belief Propagation"></a>Belief Propagation</h1><p>asd</p><ul><li>Trees<ul><li>Two-pass Algorithm</li></ul></li><li>Factor Trees<ul><li>Message Passing on Factor Graph</li></ul></li><li>Non-Trees (General Graph)<ul><li>Junction Tree Algorithm</li></ul></li></ul><p>概率图中有向图模型其实是无向图的一种特例，从有向图到无向图的转换关系如下</p><ul><li><p>Undirected Tree:</p><script type="math/tex; mode=display">p(x) = \frac{1}{Z}\left( \prod_{i\in V} \psi(x_i) \prod_{(i,j)\in E} \psi (x_i,x_j) \right)</script></li><li><p>Directed Tree:</p><script type="math/tex; mode=display">p(x)  = p(x_r) \prod_{(i,j)\in E} p(x_i|x_j)</script></li><li><p>Equivalence:</p><script type="math/tex; mode=display">\psi (x_r) = p(x_r);\ \psi(x_i,x_j)=p(x_j|x_i);\ Z=1,\psi(x_i) = 1</script></li></ul><h3 id="Trees"><a href="#Trees" class="headerlink" title="Trees"></a>Trees</h3><p>Elimination 操作可以看作是 message passing.</p><p>令 $m_{ji}(x_i)$ 当作是从 i 那里变量消除后生成的乘子，同时，这就是 $x_i$ 的函数:</p><script type="math/tex; mode=display">m_{ji}(x_i) = \sum_{x_j} \left( \psi(x_j)\psi(x_i,x_j) \prod_{k\in N(j)\setminus i} m_{kj}(x_j) \right)</script><p>上面的公式可以理解为：从 $i$ 到 $j$ 的信息，只和传递信息箭头相反的范围内的那些节点相关</p><p>对于某个节点所对应的概率，我们可以这样表示：</p><script type="math/tex; mode=display">p(x_i)\propto \psi(x_i) \prod_{e\in N(i)} m_{ei}(x_i)</script><p>可以看到计算 $p(x_i)$ 的时候， $m_{ij}(x_i)$ 会被重复的使用, 所以我们可以存储 $m$ 的值</p><p>树的 Elimination 来做 querying 算法的复杂度是 $O(NC)$ (where N=nodes, C=complexity of one complete passing/clique bottleneck). 但是使用了 two path 算法（因为是无向图所以每条边有两个方向）以后，复杂度就变成了: 2C, or $O(C)$</p><p><strong>belief propagation is only valid on trees</strong></p><h3 id="Factor-Trees"><a href="#Factor-Trees" class="headerlink" title="Factor Trees"></a>Factor Trees</h3><p><br>首先，我们定义一个变换，这个变换把一个图变成了一个新的图，变换后的图称为 <strong>Factor Graph</strong>,如果变换后刚好是一颗树，那么我们也可以称之为 Factor Tree。 在新的 Factor Graph 中，每一个 factor (clique) 在图中表示一个节点 f, 下面是一个例子,  其中的一个性质就是 $f$ 节点只和 $x$ 节点相连，也就是说，x 的某个变量把它和其他变量的关系都托付给了 f 节点。</p><p><img src="/2018/08/14/PGM-2/pgm2img2.jpg" align="justify"></p><p>对于一个图，可能有好几种变换方式，我们希望变换后的结果就是一个树，和下面的 Example 3 一样。</p><p><img src="/2018/08/14/PGM-2/pgm2img3.jpg" align="justify"></p><p>另外变换后的图其实是一个二分图（bipartite），二分图每一侧都是一种类型的节点，所以信息传递策略于传统的方法有些不同。。有两种信息的传递方式</p><p><img src="/2018/08/14/PGM-2/pgm2img4.jpg" align="justify"></p><ul><li>$\nu$ : from variables to factors（左图）<script type="math/tex; mode=display">\nu_{is}(x_i) = \prod_{t\in N(i)\setminus s} \mu_{ti}(x_i)</script></li><li>$\mu$ : from factors to variables（右图）<script type="math/tex; mode=display">\mu_{si}(x_i) = \sum_{x_{N(s)}\setminus i}\left( f_s(x_{N(s)}) \prod_{j\in N(s)\setminus i} \nu_{js}(x_j)\right)</script><ul><li><strong>_上面的 $\sum$ 操作是遍历变量的赋值，$\sum$ 操作下面的 x 可以看作是一个向量，遍历向量里面所有的赋值._</strong></li></ul></li></ul><p>Factor Tree 算法只能够处理一些长得像树的概率图</p><h3 id="Junction-Trees"><a href="#Junction-Trees" class="headerlink" title="Junction Trees"></a>Junction Trees</h3><p>Junction tree data-structure for exact inference on general graphs</p><p><strong>Algorithm</strong></p><ul><li>Moralization</li><li>Triangulation</li><li>Junction tree</li><li>Message Propagation</li></ul><h5 id="Moral-Graph"><a href="#Moral-Graph" class="headerlink" title="Moral Graph"></a>Moral Graph</h5><p>因为我们要处理的是广泛结构的概率图模型，所以我们先把 BN 纳入到 MRF 的框架里面，这一步骤叫做 Moralization，我们知道 BN 中的 factor 是某些父变量对于指定变量的条件概率，我们不管哪些是条件变量，我们就把他们看成是一个整体的函数，我们的终极目的是生成一个 clique，clique 有要求是全联通的，于是我们就将这个变量的父节点两两配对相连，这样就形成了一个 clique，原来的 factor 就变成了势函数 potential。</p><p>在这里我们得到一个启发，就是增加一条边后，原本的 graph 是新的 graph 的一种特殊情况。</p><p><img src="/2018/08/14/PGM-2/pgm2img5.jpg" align="justify"></p><h5 id="Triangulation"><a href="#Triangulation" class="headerlink" title="Triangulation"></a>Triangulation</h5><p>对于三角化的操作，我们可以先看后面两个操作的介绍再回来，因为这是为了解决后面问题的而诞生的一个步骤</p><p>问题就是 Local Consistency 不能导出 Global Consistency，只有在三角化后的图中</p><p>三角化以后的图是没有大于 4 个节点以上的环的， 三角化的方法就是在大的环中添加额外边</p><h5 id="Clique-Tree"><a href="#Clique-Tree" class="headerlink" title="Clique Tree"></a>Clique Tree</h5><p><img src="/2018/08/14/PGM-2/pgm2img6.jpg" align="justify"></p><p>下面的推断可以知道，有向图条件概率乘积的表达形式，其实就是 clique tree 表达形式的一种特殊情况</p><script type="math/tex; mode=display">\begin{align}&P(X_1,X_2,X_3,X_4,X_5,X_6)\\& = P(X_1)P(X_2)P(X_3 | X_1,X_2)P(X_4 | X_3)P(X_5 | X_3)P(X_6 | X_4,X_5) \\& = P(X_1,X_2,X_3) \frac{P(X_3,X_4,X_5)}{P(X_3)} \frac{P(X_4,X_5,X_6)}{P(X_4,X_5)} \\& = \psi(X_1,X_2,X_3) \frac{\psi(X_3,X_4,X_5)}{\phi(X_3)} \frac{\psi(X_4,X_5,X_6)}{\phi(X_4,X_5)}\end{align}</script><p><strong>General Form :</strong> 之所以下面是要除以 cliques 之间的交集 S ，是因为交集的信息可能出现了多次</p><script type="math/tex; mode=display">P(\mathbf{X}) = \frac{\prod_{c} \psi_c(\mathbf{X_c})}{\prod_{s} \phi_s(\mathbf{X_s})}</script><h5 id="Message-Passing"><a href="#Message-Passing" class="headerlink" title="Message Passing"></a>Message Passing</h5><p><br>传递方式有两种，这两种方法算出来的结果应该是一样的，这是我们做出的假设称为 <strong>Local Consistency</strong></p><script type="math/tex; mode=display">P(S) = \sum_{V\setminus S} \psi(V) \qquad \qquad P(S) = \sum_{W\setminus S} \psi(W)</script><p>下面的第一行是 forward update，第二行是 backward update，其中 $\frac{\phi_S^*}{\phi_S}$ 是通过 Local Consistency 得出的，是建立起矩形节点两边沟通的桥梁</p><p><img src="/2018/08/14/PGM-2/pgm2img7.jpg" align="justify"><br><img src="/2018/08/14/PGM-2/pgm2img11.jpg" align="justify"></p><p>上面是 clique tree 信息传递的方式，</p><p><strong>Shafer-Shenoy algorithm</strong><br><img src="/2018/08/14/PGM-2/pgm2img8.jpg" align="justify"></p><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h3 id="General-Variable-Elimination"><a href="#General-Variable-Elimination" class="headerlink" title="General Variable Elimination"></a>General Variable Elimination</h3><p>为了让计算机能够自动的处理各种各样结构的概率图的 Elimination，我们可以设计一种更加 general 的形式，但是这个形式的设定，主要还是为了进行计算机的运算的。。</p><ul><li>Let $X$ be set of all random variables</li><li>Let $F$ denote the set of factors and then for each $\phi \in F$,$Scope[\phi] \in X$</li><li>There three type of variables in Elimination Model<ul><li>Let $Y\subset X$ be a set of <strong>query</strong> variables</li><li>Let $Z = X - Y$ would be the set of variables to be <strong>eliminated</strong></li><li>Let $\mathcal{E}$ be the <strong>known</strong> variables, and $\bar{e}_i$ is the assignment</li></ul></li></ul><p>The core operation can be view as the form of, we can extend it to general form by import evidence potential</p><script type="math/tex; mode=display">\tau(Y) =  \sum_z \prod_{\phi \in F} \phi</script><ul><li><p>The evidence potantial:</p><script type="math/tex; mode=display">\begin{align}  \delta(\mathcal{E}) = \left \{ \begin{array}{ll} 1& if\ \mathcal{E_i} \equiv \bar{e}_i \\ 0 & if\ \mathcal{E_i} \neq \bar{e}_i  \end{array}  \right .\end{align}</script></li><li><p>Total evidence potential:</p><script type="math/tex; mode=display">\begin{align}  \delta(\mathbf{\mathcal{E}},\mathbf{\bar{e}})= \prod_{i\in I_{\mathcal{E}}} \delta (\mathcal{E}_i,\bar{e}_i)\end{align}</script></li><li>Introducing evidence:<script type="math/tex; mode=display">\tau(\mathbf{Y},\mathbf{\bar{e}}) = \sum_{z,e}\prod_{\phi \in F} \phi \times \delta(\mathbf{\mathcal{E}},\bar{\mathbf{e}})</script></li></ul><h5 id="The-elimination-algorithm"><a href="#The-elimination-algorithm" class="headerlink" title="The elimination algorithm"></a>The elimination algorithm</h5><p>… …</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.jianshu.com/p/f90100680749" target="_blank" rel="noopener">https://www.jianshu.com/p/f90100680749</a></p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Variable-Elimination&quot;&gt;&lt;a href=&quot;#Variable-Elimination&quot; class=&quot;headerlink&quot; title=&quot;Variable Elimination&quot;&gt;&lt;/a&gt;Variable Elimination&lt;
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Random-Record</title>
    <link href="http://yoursite.com/2018/08/09/Random-Record/"/>
    <id>http://yoursite.com/2018/08/09/Random-Record/</id>
    <published>2018-08-09T10:55:56.000Z</published>
    <updated>2018-08-24T14:41:49.394Z</updated>
    
    <content type="html"><![CDATA[<p>如果一个变量在两个 clique 之间出现，那么这个变量一定在这两个 clique 的路径之间</p><p>learning is parameter estimation</p><p>sufficient statistics $T(x)$ means x self or some transformation of x</p><p>先验概率可理解为统计概率，后验概率可理解为条件概率</p><hr><p>In Bayesian probability theory, if the posterior distributions $p(\theta | x)$ are in the same probability distribution family as the prior probability distribution $p(\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function.</p><h2 id="For-example-the-Gaussian-family-is-conjugate-to-itself-or-self-conjugate-with-respect-to-a-Gaussian-likelihood-function-if-the-likelihood-function-is-Gaussian-choosing-a-Gaussian-prior-over-the-mean-will-ensure-that-the-posterior-distribution-is-also-Gaussian"><a href="#For-example-the-Gaussian-family-is-conjugate-to-itself-or-self-conjugate-with-respect-to-a-Gaussian-likelihood-function-if-the-likelihood-function-is-Gaussian-choosing-a-Gaussian-prior-over-the-mean-will-ensure-that-the-posterior-distribution-is-also-Gaussian" class="headerlink" title="For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian"></a>For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian</h2><p>Learning Graphical Models</p><p>The target is given the assignments and predict the best (most likely) structure of the network.</p><p>“Optimal” here means the employed algorithms guarantee to<br>return a structure that maximizes the objectives (e.g., LogLik)</p><script type="math/tex; mode=display">\begin{align}l(\theta_G,G;D) &= log p(D|\theta_G,G) \\&= log \prod_n \left(  \prod_i p(x_{n,i}|\mathbf{x}_{n,\pi_i(G)}, \theta_{i|\pi_i(G)})  \right)\end{align}</script><ul><li>$\prod_n$ means enum $n$ data</li><li>$\prod_i$ means enum all nodes</li><li>$\mathbf{x}_{n,\pi_i(G)}$ is the assignments of x’s parents</li></ul><p>M is the number of the state, add this we can turn the count function into probability representation</p><p>the right decomposited part is entropy !</p><p>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如果一个变量在两个 clique 之间出现，那么这个变量一定在这两个 clique 的路径之间&lt;/p&gt;
&lt;p&gt;learning is parameter estimation&lt;/p&gt;
&lt;p&gt;sufficient statistics $T(x)$ means x self
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PGM-Representation</title>
    <link href="http://yoursite.com/2018/08/08/PGM-1/"/>
    <id>http://yoursite.com/2018/08/08/PGM-1/</id>
    <published>2018-08-08T12:51:00.000Z</published>
    <updated>2018-09-10T06:37:07.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction-of-PGM"><a href="#Introduction-of-PGM" class="headerlink" title="Introduction of PGM"></a>Introduction of PGM</h1><h3 id="Why-Using-PGM"><a href="#Why-Using-PGM" class="headerlink" title="Why Using PGM"></a>Why Using PGM</h3><p>For a joint distribution $P(X_1,X_2,…,X_n)$, can be written in two ways</p><ul><li>All Dependent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2|X_1)P(X_2|X_1,X_2)…P(X_n|X_1,…,X_{n-1})$<ul><li>pro: the formular are correct in any case</li><li>con: require huge probability table $O(k^n)$</li></ul></li><li>Independent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2)…P(X_n)$<ul><li>pro: probability table only take $O(kn)$ memory</li><li>con: independent are restrictive hypothese</li></ul></li></ul><p>The PGM are target a middle-ground between two extremes</p><p>Two types of GMs</p><ul><li><strong>Directed edges</strong> give causality relationships (Bayesian Network or Directed Graphical Model)</li><li><strong>Undirected edges</strong> simply give correlations between variables (Markov Random Field or Undirected Graphical model)</li></ul><h1 id="Bayesian-Network"><a href="#Bayesian-Network" class="headerlink" title="Bayesian Network"></a>Bayesian Network</h1><p>Firstly the Bayesian Network is <strong>DAG</strong> (Directed Acyclic Graph)</p><p><strong>Factorization Theorem</strong> : The probability of specific BN</p><script type="math/tex; mode=display">P(X_1,X_2,...,X_n) = \prod_{i=1:n} P(X_i|Parents(X_i))</script><h3 id="Graph-model-and-Bayesian"><a href="#Graph-model-and-Bayesian" class="headerlink" title="Graph model and Bayesian"></a>Graph model and Bayesian</h3><p>Suppose the distribution is $A\leftarrow B \rightarrow C$</p><ul><li>Bayesian Interpretation: $P(ABC) = P(B) P(A|B) P(C|B)$</li><li><p>Graph Model Interpretation: $I(G) = \{ A\bot C |B \}$<br>now we want to prove $I(P(ABC)) = I(\{ A\bot C |B \})$</p><script type="math/tex; mode=display">\begin{align}  P(AC|B) &= P(A|B)P(C|B) \leftarrow \{ A\bot C |B \}\\  P(AC|B) &= \frac{P(ABC)}{P(B)} = \frac{ P(B) P(A|B) P(C|B)}{P(B)} \leftarrow P(ABC)\end{align}</script></li></ul><p><strong>conditional Independence :</strong><br>for $X\leftarrow Z \rightarrow Y$, $Z$ represent the height of father, $X,Y$ are the brother, the relation between $X$ and $Y$ :</p><ul><li>Dependent : we dont know the height of father, but we know the height of $Y$, so $Y$ may effect the distribution of $Z$, at the same time also influence the $X$</li><li>Independent : we already know the height of father, so whether $Y$ is dont influence $X$</li></ul><p><strong>local Markov assumption :</strong><br>each node $X_i$ is independent of its nondescendants given its parents.<br>_when you confirm the parents, you only dependent with your childs_</p><script type="math/tex; mode=display">I_l(G):\{ X_i\bot NonDescendants_{X_i} | Pa_{X_i} : \forall i \}</script><h3 id="Independencies-Three-Basic-Model"><a href="#Independencies-Three-Basic-Model" class="headerlink" title="Independencies (Three Basic Model)"></a>Independencies (Three Basic Model)</h3><p><img src="/2018/08/08/PGM-1/pgm1img2.jpg" align="justify"></p><ul><li><strong>a: Cascade</strong><ul><li>$Y$ observed $X,Z$ are independent</li></ul></li><li><strong>b: Common parent</strong><ul><li>$Y$ observed $X,Z$ are independent</li><li>$Y$ unknow $X,Z$ are dependent</li></ul></li><li><strong>c: V-structure</strong><ul><li>$Y$ observed $X,Z$ are dependent</li><li>$Y$ unknow $X,Z$ are independent</li></ul></li></ul><p>Let $P$ be a distribution of $X$, $I(P)$ is the set of independence assertions of the form $(X \bot Y | Z)$ that hold in $P$. $I(G)$ is the sub-set of the $I(P)$,</p><p>We say that $K$ is an $I$-map for a _set_ of independencies $I$ if $I(K) ⊆ I$</p><h3 id="Active-trail"><a href="#Active-trail" class="headerlink" title="Active trail"></a>Active trail</h3><ul><li>Causal Trail $X → Z → Y$ : active iff $Z$ is not observed.</li><li>Evidential Trail $X ← Z ← Y$ : active iff $Z$ is not observed.</li><li>Common Cause $X ← Z → Y$: active iff $Z$ is not observed.</li><li>Common Effect $X → Z ← Y$ : active iff $Z$ (or any of its descendents) is observed.<br>_here active means the dependent relation estibilish_</li></ul><p>Definition : Let $\textbf{X}, \textbf{Y} , \textbf{Z}$ be three sets of nodes in $G$. We say that $\textbf{X}$ and $\textbf{Y}$ are d-separated given $\textbf{Z}$, denoted $dsep_G(\textbf{X};\textbf{Y}|\textbf{Z})$, if there is no active trail between any node $X \in \textbf{X}$ and $Y \in \textbf{Y}$ given $\textbf{Z}$</p><p>Definition: $I(G)=$ all independence properties that correspond to d- separation:</p><script type="math/tex; mode=display">I(G) = {X \bot Y | Z : dsep_G(X \bot Y | Z)}</script><hr><h1 id="Undirected-GM"><a href="#Undirected-GM" class="headerlink" title="Undirected GM"></a>Undirected GM</h1><p>An undirected graphical model represents a distribution $P(X1,…Xn)$ defined by an undirected graph H, and a set of positive-valued potential functions $\psi_c$ corresponding to each clique $c \in C$ of $H$ such that:</p><script type="math/tex; mode=display">\begin{align}P(X_1,...,X_n) = \frac{1}{Z} \prod_{c\in C} \psi_c(X_c) \\Z = \sum_{X_1,...,X_n} \prod_{c\in C} \psi_c(X_c)\end{align}</script><p>potential function can be joint and conditional probability function, or even a table of values</p><p>Clique Example<br><img src="/2018/08/08/PGM-1/pgm1img4.jpg"></p><ul><li>max-cliques = $\{A,B,D\}, \{B,C,D\}$,</li><li>sub-cliques = $\{A,B\}, \{C,D\}, …\text{ all edges and single point}$</li></ul><script type="math/tex; mode=display">\begin{align}P^{\prime}(x_1,x_2,x_3,x_4) = \frac{1}{Z} \psi_c(X_{124}) \times \psi_c(X_{234}) \\Z = \sum_{x_1,x_2,x_3,x_4} \psi_c(X_{124})\end{align}</script><h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence:"></a>Independence:</h3><ul><li><strong>global Markov independencies $I(H)$</strong> $= \{ A\bot C|B:sep_H(A,C|B) \}$<ul><li>any disjoint A,B,C in distribution, B separates A and C, A is independent of C given B.</li></ul></li><li><strong>local Markov independencies $I_l(H)$</strong>   $=\{X_i \bot V \setminus (X\cup MB_{X_i})|MB_{X_i} :\forall i  \}$<ul><li>Independent with node that dont near it.</li></ul></li><li><strong>pairwise Markov independencies $I_p(H)$</strong> $=\{X \bot Y|V \setminus \{X,Y\}:\{X,Y\}\notin E\}$<ul><li>Independent when no shared edge.</li></ul></li><li><strong>Markov blanket</strong> of $X_i$ denoted $MB_{X_i}$, is the neighbors of $X_i$ in graph</li><li>B separates A and C if every path from A to C through B: $sep_H(A,C|B)$</li></ul><p>relation of local and global</p><ul><li>Thm: $P\models I(H) \Rightarrow P \models I_l(H) \Rightarrow P \models I_p(H)$</li><li>Corollary: For a positive distribution P, global, local, and pairwise indepedencies are equivalent</li></ul><h3 id="Exponential-Model"><a href="#Exponential-Model" class="headerlink" title="Exponential Model"></a>Exponential Model</h3><p>Form discussed above, we need find a clique potential can ensure the positive distribution, one form is negative exponential: $\Psi (\mathbf{x}_c) = exp\{ -\phi_c(\mathbf{x_c}) \}$</p><p>The exponential form of the distribution structure is:</p><script type="math/tex; mode=display">p(\mathbf{x}) = \frac{1}{Z}\mathop{exp}\left\{ - \sum_{c\in C} \phi_c(\mathbf{x_c}) \right\}=\frac{1}{Z} \mathop{exp}\{ -H(\mathbf{x}) \}</script><p>$H(\mathbf{x})$ is the “free energy”.</p><h5 id="Boltzmann-Machines"><a href="#Boltzmann-Machines" class="headerlink" title="Boltzmann Machines"></a>Boltzmann Machines</h5><p>A Boltzmann Machine is a fully connected graph with pairwise potentials on binary-valued nodes. The energy function for this is expressed in sub-clique form, which comes from the physics tradition.</p><script type="math/tex; mode=display">\begin{align}P(\mathbf{x}) &= \frac{1}{Z} \mathop{exp} \left\{ \sum_{i,j} \phi_{ij}(x_i,x_j) \right\} \\&= \frac{1}{Z} \mathop{exp} \left\{ \sum_{i,j} \theta_{ij}x_ix_j + \sum_i \alpha_i x_i + C \right\} \\&= \frac{1}{Z} \mathop{exp} \left\{ (x-\mu)^T \Theta (x-\mu) \right\}\end{align}</script><h5 id="Restricted-Boltzmann-Machines"><a href="#Restricted-Boltzmann-Machines" class="headerlink" title="Restricted Boltzmann Machines"></a>Restricted Boltzmann Machines</h5><p>This is inspired by the Boltzmann Machine and is responsible for much of the deep learning craze. An RBM consists of many layers. Within each layer, there are two sublayers: one of hidden units (factors, $h_j$), and one of visible units ($x_i$). The probability function for an RBM is</p><script type="math/tex; mode=display">p(x,h|\theta) = \mathop{exp} \left\{ \sum_i \theta_i \phi_i (x_i) + \sum_j \theta_j \phi_j(h_j) + \sum_{i,j} \theta_{i,j} \phi_{i,j} (x_i,h_j) - A(\theta) \right\}</script><h5 id="Conditional-Random-Fields"><a href="#Conditional-Random-Fields" class="headerlink" title="Conditional Random Fields"></a>Conditional Random Fields</h5><p>…</p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction-of-PGM&quot;&gt;&lt;a href=&quot;#Introduction-of-PGM&quot; class=&quot;headerlink&quot; title=&quot;Introduction of PGM&quot;&gt;&lt;/a&gt;Introduction of PGM&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process-11</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-11/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-11/</id>
    <published>2018-08-04T08:05:19.000Z</published>
    <updated>2018-08-06T01:27:43.065Z</updated>
    
    <content type="html"><![CDATA[<p>i.i.d sequence of random variables: too restrictive assumption</p><p>completely dependent among random variables: hard to analysis</p><p>balance between complete independence &amp; complete dependence</p><p>Classification of Markov Process</p><ul><li>Discrete-Time Markov Chain: Discrete $S$ &amp; Discrete $T$</li><li>Continuous-Time Markov Chain: Discrete $S$ &amp; Continuous $T$</li><li>Discrete Markov Chain: Continuous $S$ &amp; Discrete $T$</li><li>Continuous Markov Chain: Continuous $S$ &amp; Continuous $T$</li></ul><h3 id="Definition-Markov-Chain"><a href="#Definition-Markov-Chain" class="headerlink" title="Definition (Markov Chain)"></a>Definition (Markov Chain)</h3><p>A sequence of random variables $X_0,X_1,X_2,…$ taking values in the state space $\{1,2,…,M\}$ is called <strong>Markov Chain</strong>, the event $X_i+1$ only influenced by $X_i$</p><script type="math/tex; mode=display">P(X_{n+1}=j|X_n = i,X_{n-1}=i_{n-1},...,X_0=i_0) = P(X_{n+1}=j|X_n=i)</script><h3 id="Definition-Transition-Matrix"><a href="#Definition-Transition-Matrix" class="headerlink" title="Definition (Transition Matrix)"></a>Definition (Transition Matrix)</h3><p>Let $X_0,X_1,X_2,…$ be a Markov Chain with state space $\{1,2,…,M\}$, and let<br>$q_{ij} = P(X_{n+1}=j|X_N=i)$ be the transition probability from state $i$ to state $j$. The $M \times M$ matrix $Q=(q_{ij})$ is called transition matrix of the chain.</p><p><img src="/2018/08/04/Stochastic-Process-11/sp11img1.jpg" align="justify"></p><h3 id="Definition-n-step-Transition-Probability"><a href="#Definition-n-step-Transition-Probability" class="headerlink" title="Definition (n-step Transition Probability)"></a>Definition (n-step Transition Probability)</h3><p>The n-step transition probability from $i$ to $j$ is the probability of being at $j$ exactly $n$ steps after being at $i$. Denote this by $q^{(n)}_{ij}$</p><script type="math/tex; mode=display">q^{(n)}_{ij} = P(X_n=j|X_0=i)=\sum_{k\in S} q^{(1)}_{kj} q^{(n-1)}_{ik}</script><p>which implies</p><script type="math/tex; mode=display">Q^n = Q^{n-1}\cdot Q</script><h3 id="Theorem-Chapman-Kolmogorov-Equation"><a href="#Theorem-Chapman-Kolmogorov-Equation" class="headerlink" title="Theorem (Chapman-Kolmogorov Equation)"></a>Theorem (Chapman-Kolmogorov Equation)</h3><script type="math/tex; mode=display">q_{ij}^{(n+m)} = \sum_{k\in S} q^{(n)}_{ik} q^{(m)}_{kj}</script><h1 id="First-Step-Analysis"><a href="#First-Step-Analysis" class="headerlink" title="First Step Analysis"></a>First Step Analysis</h1><h3 id="Example-Toss-A-Coin-till-HH-Appear"><a href="#Example-Toss-A-Coin-till-HH-Appear" class="headerlink" title="Example (Toss A Coin till HH Appear)"></a>Example (Toss A Coin till HH Appear)</h3><p>This problem can be formulated as a 3-state markov chain<br><img src="/2018/08/04/Stochastic-Process-11/sp11img2.jpg" align="justify"><br>The Transition graph is equivalent to<br><img src="/2018/08/04/Stochastic-Process-11/sp11img3.jpg" align="justify"><br>Let $e_s = E[\text{waiting time for HH|initial state = s}]$, then we have</p><script type="math/tex; mode=display">\begin{align}e_{Null} &= \frac{1}{2} (1+e_{Null})+\frac{1}{2} (1+e_H) \\e_H &= \frac{1}{2} (1+e_{HH}) + \frac{1}{2} (1+e_{Null})\\e_{HH} &= 0\end{align}</script><h3 id="Example-Toss-A-Coin-till-HTHT-Appear"><a href="#Example-Toss-A-Coin-till-HTHT-Appear" class="headerlink" title="Example (Toss A Coin till HTHT Appear)"></a>Example (Toss A Coin till HTHT Appear)</h3><p>Let see a more complicate case , this can be done by establish a linear equation</p><p><img src="/2018/08/04/Stochastic-Process-11/sp11img4.jpg" align="justify"></p><h1 id="Classification-of-States"><a href="#Classification-of-States" class="headerlink" title="Classification of States"></a>Classification of States</h1><h3 id="Definition-Recurrent-and-Transition-States"><a href="#Definition-Recurrent-and-Transition-States" class="headerlink" title="Definition (Recurrent and Transition States)"></a>Definition (Recurrent and Transition States)</h3><ul><li><strong>Recurrent</strong> State $i$ of Markov chain have the probability of $1$ eventually return to $i$</li><li><strong>Transient</strong> Other-wise, the state is Transient</li></ul><p><img src="/2018/08/04/Stochastic-Process-11/sp11img5.jpg" align="justify"></p><h3 id="Definition-Irreducible-amp-Reducible-Chain"><a href="#Definition-Irreducible-amp-Reducible-Chain" class="headerlink" title="Definition (Irreducible &amp; Reducible Chain)"></a>Definition (Irreducible &amp; Reducible Chain)</h3><ul><li><strong>Irreducible</strong> any state $i$ and $j$, possible to go from $i$ to $j$ in a finite number of steps.</li><li><strong>Reducible</strong> not irreducible</li></ul><h3 id="Theorem-Irreducible-Implies-All-States-Recurrent"><a href="#Theorem-Irreducible-Implies-All-States-Recurrent" class="headerlink" title="Theorem (Irreducible Implies All States Recurrent)"></a>Theorem (Irreducible Implies All States Recurrent)</h3><p>In an irreducible Markov Chain with a finite state space, all states are recurrent</p><h3 id="Example-Coupon-Collector"><a href="#Example-Coupon-Collector" class="headerlink" title="Example (Coupon Collector)"></a>Example (Coupon Collector)</h3><p>We want to collect all $C$ types coupons, Let $X_n$ be the number of distinct coupon types in our collection after $n$ attempts. Then $X_0,X_1,…$ is a Markov Chain on the state space $\{ 0,1,…,C \}$<br><img src="/2018/08/04/Stochastic-Process-11/sp11img6.jpg" align="justify"></p><h3 id="Definition-Period"><a href="#Definition-Period" class="headerlink" title="Definition (Period)"></a>Definition (Period)</h3><p>The <strong>period</strong> of state $i$ in a markov chain is the gcd of the possible numbers of steps can return to $i$ when starting at $i$. A state is called <strong>aperiodic</strong> if its period equals 1, and periodic otherwise.</p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;i.i.d sequence of random variables: too restrictive assumption&lt;/p&gt;
&lt;p&gt;completely dependent among random variables: hard to analysis&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process-10</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-10/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-10/</id>
    <published>2018-08-04T08:05:13.000Z</published>
    <updated>2018-08-05T08:29:32.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Inference"><a href="#Bayesian-Inference" class="headerlink" title="Bayesian Inference"></a>Bayesian Inference</h1><h3 id="Bayesian-Inference-Framework"><a href="#Bayesian-Inference-Framework" class="headerlink" title="Bayesian Inference Framework"></a>Bayesian Inference Framework</h3><p>We aim to extract information about $\Theta$, based on observing a collection $X = (X_1,…,X_n)$</p><ul><li>Unknow $\Theta$<ul><li>treated as a random variable</li><li>prior distribution $p_\Theta$ or $f_\Theta$</li></ul></li><li>Observation $X$<ul><li>observation model $p_X|\Theta$ or $f_X|\Theta$</li></ul></li><li>Use appropriate version of the bayes rule to find $p_{\Theta|X}(\cdot|X=x)$</li></ul><h3 id="Principal-Bayesian-Estimation-Method"><a href="#Principal-Bayesian-Estimation-Method" class="headerlink" title="Principal Bayesian Estimation Method"></a>Principal Bayesian Estimation Method</h3><ul><li><strong>Maximum a posterior probability (MAP) rule</strong> Select the possible parameter with maximum conditional/posterior probability given the data</li><li><strong>Least mean squares (LMS) estimation</strong> Select an estimator/function of the data that minimizes the mean squared error between the parameterand its estimate<ul><li>$p_{\Theta|X}(\theta^*|x)=\mathop{max}_{\theta}p_{\Theta|X}(\theta|x)$</li></ul></li></ul><h3 id="Example-Inferring-the-Unknown-Bias-of-A-Coin"><a href="#Example-Inferring-the-Unknown-Bias-of-A-Coin" class="headerlink" title="Example (Inferring the Unknown Bias of A Coin)"></a>Example (Inferring the Unknown Bias of A Coin)</h3><p>We wish to estimate the probability of heads, denoted by $p$, suppose the prior is a beta density with $a,b$, that is , $p \sim Beta(a,b)$. We consider n independent tosses and let $X$ be the number of heads observed</p><p><strong>MAP Estimate</strong> The posterior PDF of $p$ has the form</p><script type="math/tex; mode=display">\begin{align}f(p|X=k) &= \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)} \\&= c\cdot p^{a+k-1} (1-p)^{b+(n-k)-1}\end{align}</script><p>Hence the posterior density is beta with parameters $a+k$ and $b+(n-k)$</p><p>By MAP rule, we select the eatimator as</p><script type="math/tex; mode=display">\hat{p}_{MAP} = \mathop{arg}\mathop{max}_p f(p|X=k) = \mathop{arg}\mathop{max}_p p^{a+k-1}(1-p)^{b+(n-k)-1}</script><p>Let $g(p) = p^{a+k-1}(1-p)^{b+(n-k)-1}$,then we have</p><script type="math/tex; mode=display">log(g(p)) = (a+k-1)\mathop{log}p + (b+(n-k)-1)\mathop{log}(1-p)</script><p>To find $p^*$ let</p><script type="math/tex; mode=display">\partial \frac{  \mathop{log}(g(p))} {\partial p}|_{p=p^*}  = 0</script><p>which yields</p><script type="math/tex; mode=display">\hat{p}_{MAP}  = \frac{a+k-1}{a+b+n-2}</script><p>When the prior distribution of $p$ is $Unif(0,1)$, that is $a=0,b=0$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k}{n}$</p><p><strong>LMS Estimate</strong> By Beta-Binomial conjugacy, $f(p|X=k) \sim Beta(a+k,b+n-k)$, the expectation of random variable $Y\sim Beta(a,b)$ is $E(Y) = \frac{a}{a+b}$, we have</p><script type="math/tex; mode=display">\hat{p}_{LMS} = E(p|X=k) = \frac{a+k}{(a+k)+(b+n-k)} = \frac{a+k}{a+b+k}</script><p>When the prior distribution of $p$ id $Unif(0,1)$, that is $a = 1,b = 1$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k+1}{n+2}$</p><hr><h1 id="Classical-Inference"><a href="#Classical-Inference" class="headerlink" title="Classical Inference"></a>Classical Inference</h1><ul><li>Classical Statistics: unknown constant $\theta$<ul><li>also for vectors $X$ and $\theta$ : $p_{X_1,…,X_n}(x_1,…,x_n; \theta_1,…,\theta_m)$</li><li>$p_X(x;\theta)$ are NOT conditional probabilities; $\theta$ is not random</li><li>mathematically: many models, one for each possible value of $\theta$</li></ul></li></ul><p>For example, the data observation model is $X\sim Binomial(n,\theta)$, Then under each possible value of $\theta$, the candidate model is</p><script type="math/tex; mode=display">p_X(x;\theta) = P(X=x;\theta) = \left( \begin{array}{c} n\\k \end{array} \right) \theta^x (1-\theta)^{n-x}</script><p>Classical Inference use the maximum likelihood to estimate the $\theta$</p><hr><h1 id="Sampling-Moments"><a href="#Sampling-Moments" class="headerlink" title="Sampling Moments"></a>Sampling Moments</h1><h3 id="Definition-Moments"><a href="#Definition-Moments" class="headerlink" title="Definition (Moments)"></a>Definition (Moments)</h3><p>Let $X$ be an r.v. with mean $\mu$ and variance $\sigma^2$, The $n^{th}$ moment of $X$ is $E(X^n)$, the $n^{th}$ central moment is $E((X-\mu)^n)$, and the $n^{th}$ standardized moment is $E((\frac{X-\mu}{\sigma})^n)$</p><h3 id="Definition-Sample-Moments"><a href="#Definition-Sample-Moments" class="headerlink" title="Definition (Sample Moments)"></a>Definition (Sample Moments)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables, the $k^{th}$ sample moment is the</p><script type="math/tex; mode=display">M_k = \frac{1}{n} \sum_{j=1}^n (X_j)^k</script><p>The sample mean $\bar{X}_n$ is the first sample moment:</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n} \sum_{j=1}^n X_j</script><h3 id="Theorem-Mean-and-Var-of-Sample-Mean"><a href="#Theorem-Mean-and-Var-of-Sample-Mean" class="headerlink" title="Theorem (Mean and Var of Sample Mean)"></a>Theorem (Mean and Var of Sample Mean)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. Then the sample mean $\bar{X}_n$ is unbiased for estimating $\mu$. That is</p><script type="math/tex; mode=display">E(\bar{X}_n) = \mu</script><p>The variance is</p><script type="math/tex; mode=display">Var(\bar{X}_n) = \frac{\sigma^2}{n}</script><h3 id="Definition-Sample-Variance"><a href="#Definition-Sample-Variance" class="headerlink" title="Definition (Sample Variance)"></a>Definition (Sample Variance)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables. The sample variance is the r.v.</p><script type="math/tex; mode=display">S_n^2 = \frac{1}{n-1} \sum_{j=1}^n (X_j-\bar{X}_n)^2</script><h3 id="Theorem-Unbiaseness-of-Sample-Var"><a href="#Theorem-Unbiaseness-of-Sample-Var" class="headerlink" title="Theorem (Unbiaseness of Sample Var)"></a>Theorem (Unbiaseness of Sample Var)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. The Sample Var $S_n^2$ is unbiased for estimating $\sigma^2$</p><script type="math/tex; mode=display">E(S_n^2) = \sigma^2</script><h3 id="Definition-Convergence-with-Probability"><a href="#Definition-Convergence-with-Probability" class="headerlink" title="Definition (Convergence with Probability)"></a>Definition (Convergence with Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges <strong>almost surely</strong> (a.s.) to the random variable $X$ as $n\rightarrow \infty$ and only if</p><script type="math/tex; mode=display">P(\left\{  \omega : X_n(\omega) \rightarrow X(\omega)\ as\ n\rightarrow \infty  \right\}) = 1</script><p>Notation: $X_n \xrightarrow{a.s.} X \text{ as }   n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img1.jpg" align="justify"></p><h3 id="Definition-Convergence-in-Probability"><a href="#Definition-Convergence-in-Probability" class="headerlink" title="Definition (Convergence in Probability)"></a>Definition (Convergence in Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges in probability to the random variable $X$ as $n\rightarrow \infty$ if and only if for every $\epsilon &gt;0$</p><script type="math/tex; mode=display">P(|X_n-X|>\epsilon) \rightarrow0 \ as \ n\rightarrow \infty</script><p>Notation: $X_n \xrightarrow{P} X \text{ as } n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img2.jpg" align="justify"></p><hr><h1 id="Law-of-large-Numbers"><a href="#Law-of-large-Numbers" class="headerlink" title="Law of large Numbers"></a>Law of large Numbers</h1><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v. with finite mean $\mu$ and finite variance $\sigma^2$. The samplw mean $\bar{X}_n$ is defined as</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n}\sum_{j=1}^n X_j</script><p>The Sample mean $\bar{X}_n$ is itself an r.v. with mean $\mu$ and variance $\sigma^2/n$</p><h3 id="Theorem-Strong-Law-of-Large-Numbers"><a href="#Theorem-Strong-Law-of-Large-Numbers" class="headerlink" title="Theorem (Strong Law of Large Numbers)"></a>Theorem (Strong Law of Large Numbers)</h3><p>The event $\bar{X}_n \rightarrow \mu$ has probability $1$</p><h3 id="Theorem-Weak-Law-of-Large-Numbers"><a href="#Theorem-Weak-Law-of-Large-Numbers" class="headerlink" title="Theorem (Weak Law of Large Numbers)"></a>Theorem (Weak Law of Large Numbers)</h3><p>For all $\epsilon &gt;0, P(|\bar{X}_n - \mu&gt;\epsilon) \rightarrow 0$ as $n\rightarrow \infty$</p><h3 id="Definition-Time-Average"><a href="#Definition-Time-Average" class="headerlink" title="Definition (Time Average)"></a>Definition (Time Average)</h3><script type="math/tex; mode=display">\bar{N}^{Time\ Average}(\omega)=\mathop{lim}_{t\rightarrow \infty} \frac{\int_0^t N(v,\omega)dv}{t}</script><h3 id="Definition-Ensemble-Average"><a href="#Definition-Ensemble-Average" class="headerlink" title="Definition (Ensemble Average)"></a>Definition (Ensemble Average)</h3><script type="math/tex; mode=display">\bar{N}^{Ensemble}(\omega)=\mathop{lim}_{t\rightarrow\infty} E[N(t)]=\sum_{i=0}^{\infty} i p_i</script><hr><h1 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h1><h3 id="Theorem-Central-Limit"><a href="#Theorem-Central-Limit" class="headerlink" title="Theorem (Central Limit)"></a>Theorem (Central Limit)</h3><p>As $n\rightarrow \infty$</p><script type="math/tex; mode=display">\sqrt{n}\left(  \frac{\bar{X}_n - \mu}{\sigma}  \right)  \rightarrow N(0,1)</script><p><strong>CLT Approximation</strong> For a large $n$, the distribution od $\bar{X}_n$ is approximately $N(\mu,\sigma^2/n)$</p><h3 id="Example-CLT-Example"><a href="#Example-CLT-Example" class="headerlink" title="Example (CLT Example)"></a>Example (CLT Example)</h3><p><img src="/2018/08/04/Stochastic-Process-10/sp10img3.jpg" align="justify"></p><ul><li><strong>Poisson Convergence to Normal</strong> Let $Y\sim Pois(n)$. Consider $Y$ as sum of $n$ i.i.d. $Pois(1)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(n,n)</script></li><li><strong>Gamma Convergence to Normal</strong> Let $Y\sim Gamma(n,\lambda)$. Consider $Y$ as sum of $n$ i.i.d. $Expo(\lambda)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(\frac{n}{\lambda},\frac{n}{\lambda^2})</script></li><li><strong>Binomial Convergence to Normal</strong> Let $Y\sim Bin(n,p)$. Consider $Y$ as sum of $n$ i.i.d. $Bern(p)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(np,np(1-p))</script></li></ul><h3 id="De-Moivre-Laplace-Approximation"><a href="#De-Moivre-Laplace-Approximation" class="headerlink" title="De Moivre-Laplace Approximation"></a>De Moivre-Laplace Approximation</h3><script type="math/tex; mode=display">\begin{align}P(Y=k) &= P(k-\frac{1}{2} < Y < k + \frac{1}{2}) \\       &\approx \Phi\left( \frac{k+\frac{1}{2} - np}{\sqrt{np(1-p)}} \right) -\Phi\left( \frac{k-\frac{1}{2} - np}{\sqrt{np(1-p)}} \right)\end{align}</script><ul><li><strong>Possion Approximation</strong> When $n$ is large and $p$ is samll</li><li><strong>Normal Approximation</strong> When $n$ is large and $p$ is around $1/2$</li></ul><hr><h1 id="Inequality"><a href="#Inequality" class="headerlink" title="Inequality"></a>Inequality</h1><h3 id="Basic-Inequalities"><a href="#Basic-Inequalities" class="headerlink" title="Basic Inequalities"></a>Basic Inequalities</h3><ul><li><p><strong>Cauchy-Schwarz Inequality</strong></p><script type="math/tex; mode=display">|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}</script></li><li><p><strong>Jensen’s Inequality</strong></p><ul><li>If $g$ is a convex function and $X$ is a r.v. then<script type="math/tex; mode=display">E(g(x)) \geq g(E(X))</script></li><li>If $g$ is a concave function<script type="math/tex; mode=display">E(g(x)) \leq g(E(X))</script></li></ul></li><li><p><strong>Markov’s Inequality</strong></p><script type="math/tex; mode=display">P(|X|\geq a) \leq \frac{E|X|}{a}</script></li><li><p><strong>Chebyshev’s Inequality</strong> Let $X$ have mean $\mu$ and variance $\sigma^2$</p><script type="math/tex; mode=display">P(|X-\mu|\geq a) \leq \frac{\sigma^2}{a^2}</script></li><li><p><strong>Chernoff’s Inequality</strong></p><script type="math/tex; mode=display">P(X\geq a) \leq \frac{E(e^{tX})}{e^{ta}}</script></li></ul><h3 id="Concentration-Inequalities"><a href="#Concentration-Inequalities" class="headerlink" title="Concentration Inequalities"></a>Concentration Inequalities</h3><p><strong>Hoeffding Lemma</strong> Let r.v. $X$ satisfy $E(X) = 0$ and $X\leq b$, Then for any $h&gt;0$</p><script type="math/tex; mode=display">E(e^{hX}) \leq e^{\frac{1}{8}h^2 (b-a)^2}</script><p><strong>Hoeffding Inequality</strong> Let the r.v. $X_1,X_2,…,X_n$ be independent, with $x_k\leq X_k\leq b_k$ for each $k$, Let $S_n = \sum_{k=1}^n X_k$. Then</p><script type="math/tex; mode=display">P(|S_n- \mu|\geq t) \leq 2e^{-\frac{2t^2}{\sum_{k=1}^n(b_k-a_k)^2}}</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Inference&quot;&gt;&lt;a href=&quot;#Bayesian-Inference&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Inference&quot;&gt;&lt;/a&gt;Bayesian Inference&lt;/h1&gt;&lt;h3 id=&quot;B
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conditional Expectation)</title>
    <link href="http://yoursite.com/2018/08/03/Stochastic-Process-8/"/>
    <id>http://yoursite.com/2018/08/03/Stochastic-Process-8/</id>
    <published>2018-08-03T12:33:40.000Z</published>
    <updated>2018-08-04T03:18:47.805Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Conditional-Expectation-Given-An-Event"><a href="#Conditional-Expectation-Given-An-Event" class="headerlink" title="Conditional Expectation Given An Event"></a>Conditional Expectation Given An Event</h1><p>If $Y$ is a discrete r.v.</p><script type="math/tex; mode=display">E(Y|A) = \sum_y P(Y=y|A)</script><p>If $Y$ is continuous r.v.</p><script type="math/tex; mode=display">E(Y|A) = \int_{-\infty}^{\infty} yf(y|A)dy</script><h3 id="Approximation"><a href="#Approximation" class="headerlink" title="Approximation"></a>Approximation</h3><p>Image a large number of $n$ of replication of experiments $y_1,…,y_n$</p><script type="math/tex; mode=display">E(Y)\approx \frac{1}{n}\sum_{j=1}^n y_j</script><p>If $I_j$ is the indicator of $A$ occurring</p><script type="math/tex; mode=display">E(Y|A) \approx \frac{\sum_{j=1}^n y_j I_j}{\sum_{j=1}^n I_j}</script><h3 id="Example-Life-Expectation"><a href="#Example-Life-Expectation" class="headerlink" title="Example (Life Expectation)"></a>Example (Life Expectation)</h3><p>Yang is 24 years old, he hear average life expectancy is $80$, Should he conclude he has 50 years of life left ?</p><p>Of Course not, cause he already live $24$ years and some people may die less than $24$</p><script type="math/tex; mode=display">E(T) < E(T|T\geq 30)</script><h3 id="Law-of-Total-Expectation"><a href="#Law-of-Total-Expectation" class="headerlink" title="Law of Total Expectation"></a>Law of Total Expectation</h3><p>Let $A_1,…,A_n$ be partition of a sample space, $Y$ be a random variable on sample space. Then</p><script type="math/tex; mode=display">E(Y) = \sum_{i=1}^n E(Y|A_i) P(A_i)</script><h3 id="Example-Geometric-Expectation-Redux"><a href="#Example-Geometric-Expectation-Redux" class="headerlink" title="Example (Geometric Expectation Redux)"></a>Example (Geometric Expectation Redux)</h3><p>Let $X\sim Geom(p)$, as the number of Tails before the first Heads in a sequence of coin flips with p. $p$ of head. To get $E(X)$ from sum of series, it also can be obtained in another way. We condition on the outcome of the first toss: if it lands heads, then $X$ is $0$ and we’re done ; if it lands Tails, then we wasted one toss and back to where we started by memorylessness Therefore</p><script type="math/tex; mode=display">\begin{align}E(X) &= E(X|\text{first toss }H)\cdot p + E(X|\text{first toss }T)\cdot q \\     &= 0 \cdot p + (1+E(X)) \cdot q\end{align}</script><p>which gives $E(X) = q/p$</p><h3 id="Example-Time-until-HH-vs-HT"><a href="#Example-Time-until-HH-vs-HT" class="headerlink" title="Example (Time until HH vs. HT)"></a>Example (Time until HH vs. HT)</h3><p>You toss a fair coin repeatedly. What is the expected number of tosses until the pattern HT/HH appears for the first times ?</p><p><strong>Times until HT</strong></p><ul><li>$W_{HT}$: number of tosses untill HT appears</li><li>$W_1$: waiting time for first H</li><li><p>$W_2$: additional waiting time for the first T</p><p>Then</p><p>$W_1\sim Fs(\frac{1}{2}),E[W_1] = 2$</p><p>$W_2\sim Fs(\frac{1}{2}),E[W_2] = 2$</p><script type="math/tex; mode=display">E[W_{HT}] = E[W_1+W_2]=E[W_1] + E[W_2] = 4</script></li></ul><p><strong>Times until HH</strong></p><script type="math/tex; mode=display">E[W_{HH}] = E[W_HH|\text{first toss }H]\cdot \frac{1}{2} + E[W_HH|\text{first toss }T]\cdot \frac{1}{2}</script><p>  where</p><script type="math/tex; mode=display">E[W_{HH}|\text{first toss }T] = 1 + E[W_{HH}]</script><p>  and</p><script type="math/tex; mode=display">\begin{align}  E[W_{HH}|\text{first toss }H] =& E[W_{HH}|\text{first toss }H, \text{second toss }H]\cdot \frac{1}{2} \\  &+ E[W_{HH}|\text{first toss }H, \text{second toss }T]\cdot \frac{1}{2} \\  =& 2\cdot \frac{1}{2} + (E[W_{HH}]+2)\cdot \frac{1}{2}  \end{align}</script><p>  Thus we get $E[W_{HH}] = 6$</p><p><strong><em>As we can see the above example use the memorylessness property of the Conditional Expectation of distribution, and construct target in both side to calculate the target</em></strong></p><hr><h1 id="Conditional-Expectation-Given-An-R-V"><a href="#Conditional-Expectation-Given-An-R-V" class="headerlink" title="Conditional Expectation Given An R.V."></a>Conditional Expectation Given An R.V.</h1><p>Let $g(x) = E(Y|X=x)$ Then the conditional expectation of $Y$ given $X$, denoted $E(Y|E)$ is defined to be the random variable $g(X)$</p><h3 id="Example-Stick-Length"><a href="#Example-Stick-Length" class="headerlink" title="Example (Stick Length)"></a>Example (Stick Length)</h3><p>Suppose we have a stick of length $1$ and break the stick at a point $X$ chosen uniformly at random. Given that $X=x$, we then choose another breakpoint $Y$ uniformly on the interval $[0,x]$, find $E(Y|X)$, and its mean and variance</p><script type="math/tex; mode=display">E(Y|X) = X/2</script><script type="math/tex; mode=display">E(E(Y|X)) = E(X/2) = \frac{1}{4}</script><script type="math/tex; mode=display">Var(E(Y|X)) = Var(X/2) = \frac{1}{48}</script><hr><h1 id="Properties-of-Conditional-Expectation"><a href="#Properties-of-Conditional-Expectation" class="headerlink" title="Properties of Conditional Expectation"></a>Properties of Conditional Expectation</h1><h3 id="Theorem-Dropping-independent"><a href="#Theorem-Dropping-independent" class="headerlink" title="Theorem (Dropping independent)"></a>Theorem (Dropping independent)</h3><p>If $X$ and $Y$ are independent, then $E(Y|X)=E(Y)$</p><h3 id="Taking-Out-What’s-Known"><a href="#Taking-Out-What’s-Known" class="headerlink" title="Taking Out What’s Known"></a>Taking Out What’s Known</h3><script type="math/tex; mode=display">E(h(X)Y|X) = h(X) E(Y|X)</script><h3 id="Theorem-Linearity"><a href="#Theorem-Linearity" class="headerlink" title="Theorem (Linearity)"></a>Theorem (Linearity)</h3><script type="math/tex; mode=display">E(Y_1+Y_2|X) = E(Y_1|X) + E(Y_2|X)</script><h3 id="Theorem-Adam’s-Law"><a href="#Theorem-Adam’s-Law" class="headerlink" title="Theorem (Adam’s Law)"></a>Theorem (Adam’s Law)</h3><p>For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><p><strong>Proof by LOTP</strong></p><p>  For $X$ discrete</p><script type="math/tex; mode=display">E(Y) = \sum_x E(Y|X=x) P(X=x)</script><p>  We let $E(Y|X=x) = g(x)$, then</p><script type="math/tex; mode=display">E(E(Y|X)) = E(g(X)) = \sum_x g(x) P(X=x) = \sum_x E(Y|X=x) P(X=x)</script><p>  So</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><h3 id="Theorem-Adam’s-Law-with-Extra-Conditioning"><a href="#Theorem-Adam’s-Law-with-Extra-Conditioning" class="headerlink" title="Theorem (Adam’s Law with Extra Conditioning)"></a>Theorem (Adam’s Law with Extra Conditioning)</h3><p>For any r.v.s $X,Y,Z$</p><script type="math/tex; mode=display">E(E(Y|X,Z)|Z) = E(Y|Z)</script><script type="math/tex; mode=display">E(E(X|Z,Y)|Y) = E(X|Y)</script><h3 id="Definition-Conditional-Variance"><a href="#Definition-Conditional-Variance" class="headerlink" title="Definition (Conditional Variance)"></a>Definition (Conditional Variance)</h3><script type="math/tex; mode=display">Var(Y|X) = E((Y-E(Y|X))^2|X)</script><p>this equivalent to</p><script type="math/tex; mode=display">Var(Y|X) = E(Y^2|X) - (E(Y|X))^2</script><h3 id="Theorem-Eve’s-Low"><a href="#Theorem-Eve’s-Low" class="headerlink" title="Theorem (Eve’s Low)"></a>Theorem (Eve’s Low)</h3><script type="math/tex; mode=display">Var(Y) = E(Var(Y|X)) + Var(E(Y|X))</script><h3 id="Example-Random-Sum"><a href="#Example-Random-Sum" class="headerlink" title="Example (Random Sum)"></a>Example (Random Sum)</h3><p>A store receives $N$ customers a day, $N$ is an r.v. with finite mean and variance. Let $X_j$ be the amount spend by the $j^{th}$ customer, $X_j$ has the mean $\mu$ and variance $\sigma^2$, $N$ and $X_j$ are independent of one another. Find the random sum $X = \sum_{j=1}^N X_j$ in terms of $\mu,\sigma^2,E(N),Var(N)$</p><p><strong>For E(X)</strong></p><script type="math/tex; mode=display">E(X|N) = E\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N E(X_j|N)= \sum_{j=1}^N E(X_j) = N\mu</script><p>  Finally, by Adam’s Law</p><script type="math/tex; mode=display">E(X) = E(E(X|N)) = E(N\mu) =\mu E(N)</script><p><strong>For Var(X)</strong></p><p>  We conditon on $N$ get $Var(X|N)$</p><script type="math/tex; mode=display">Var(X|N) = Var\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N Var(X_j|N) = \sum_{j=1}^N Var(X_j) = N\sigma^2</script><p>  Eve’s Law give the unconditional variance of $X$</p><script type="math/tex; mode=display">\begin{align}    Var(X) =& E(Var(X|N)) + Var(E(X|N))\\           =& E(N\sigma^2) + Var(N\mu) \\           =& \sigma^2 E(N) + \mu^2 Var(N)  \end{align}</script><hr><h1 id="Prediction-and-Estimation"><a href="#Prediction-and-Estimation" class="headerlink" title="Prediction and Estimation"></a>Prediction and Estimation</h1><h3 id="Theorem-Projection-Interpretation"><a href="#Theorem-Projection-Interpretation" class="headerlink" title="Theorem (Projection Interpretation)"></a>Theorem (Projection Interpretation)</h3><p>For any function $h$, the r.v. $Y-E(Y|X)$ is Uncorrelated with $h(X)$: $Cov(Y-E(Y|X),h(X)) = 0$, equivalently</p><script type="math/tex; mode=display">E((Y-E(Y|X))h(X)) = 0</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Conditional-Expectation-Given-An-Event&quot;&gt;&lt;a href=&quot;#Conditional-Expectation-Given-An-Event&quot; class=&quot;headerlink&quot; title=&quot;Conditional Expe
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>PGM-0</title>
    <link href="http://yoursite.com/2018/08/01/PGM-0/"/>
    <id>http://yoursite.com/2018/08/01/PGM-0/</id>
    <published>2018-08-01T02:28:56.000Z</published>
    <updated>2018-09-11T14:54:01.302Z</updated>
    
    <content type="html"><![CDATA[<p>概率图有这么几种推断方式</p><p>概率图的任务通俗的讲有两个</p><ul><li>Querying：给定或者不给定条件概率下，计算特定变量的概率称为：<strong>Inference</strong></li><li>Estimation：当模型某部分是未知的时候，通过数据 $D$ 来推断出未知的模型，这个称为：<strong>learning</strong></li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>Querying 任务通常来说有三种</p><p><strong>Likelihood</strong> : Likelihood is calculated to get the conditional probability of a different subset of variables conditioned based on Evidence $\mathbf{E} = \{ X_{k+1},…,X_n \}$, Evidence is the unknown variables, so we need eliminate the unsure variables to get the specific lieklihood.</p><script type="math/tex; mode=display">P(\mathbf{e}) = \sum_{x_1}\cdots \sum_{x_k} P(x_1,...,x_k,\mathbf{e})</script><p><strong>Conditional Probability</strong> : The conditional probability distribution of some query nodes conditioned on an<br>evidence.</p><script type="math/tex; mode=display">P(X|\mathbf{e}) = \frac{P(X,\mathbf{e})}{P(\mathbf{e})}= \frac{P(X,\mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}</script><p>Let $\mathbf{Y}$ be a subset of all domain variables $\mathbf{X} = \{ \mathbf{Y},\mathbf{Z} \}$, $\mathbf{Z}$ is the set of variables under elimination.</p><script type="math/tex; mode=display">P(\mathbf{Y}|\mathbf{e}) = \sum_{z}P(\mathbf{Y},\mathbf{Z}=z|\mathbf{e})</script><p><strong>Most Probable Assignment</strong> : In this query, we are interested in finding only one set of values for the query variables that maximize the given conditional probability instead of finding the entire distribution.</p><script type="math/tex; mode=display">MPA(\mathbf{Y}|\mathbf{e})=\mathop{arg}\mathop{max}_{y\in \mathbf{Y}}P(y|\mathbf{e})=\mathop{arg}\mathop{max}_{y\in \mathbf{Y}}\sum_{z} P(y,z|\mathbf{e})</script><p>Inference 算法可以分为两种</p><ul><li><p>Exact inference（准确推断）</p><ul><li>Elimination</li><li>Message-passing<ul><li>sum-product</li><li>belief propagation</li></ul></li><li>Junction Tree</li></ul></li><li><p>Approximate inference（近似推断）a</p><ul><li>Variational algorithms<ul><li>Loopy belief propagation</li><li>Mean field approximation</li></ul></li></ul><p>一般 Inference 最简单的方法是 Elimination 和 brute force，但是由于概率图模型往往有一些比较特别的结构，我们可以用 Message Passing 的算法来做</p></li></ul><hr><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>根据观测数据，来推理模型中的参数</p><p>在做 inference 的时候，我们希望能将有向图和无向图结合起来，比如，原先的子节点的父节点们，我们希望将他们</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率图有这么几种推断方式&lt;/p&gt;
&lt;p&gt;概率图的任务通俗的讲有两个&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Querying：给定或者不给定条件概率下，计算特定变量的概率称为：&lt;strong&gt;Inference&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Estimation：当模型某部分是未知
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conjugacy &amp; Bayesian)</title>
    <link href="http://yoursite.com/2018/07/31/Stochastic-Process-7/"/>
    <id>http://yoursite.com/2018/07/31/Stochastic-Process-7/</id>
    <published>2018-07-31T02:35:18.000Z</published>
    <updated>2018-08-03T05:40:01.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Beta-Binomial-Distribution"><a href="#Beta-Binomial-Distribution" class="headerlink" title="Beta-Binomial Distribution"></a>Beta-Binomial Distribution</h1><h3 id="Definition-Beta-Distribution"><a href="#Definition-Beta-Distribution" class="headerlink" title="Definition (Beta Distribution)"></a>Definition (Beta Distribution)</h3><p>An r.v. $X$ is said to have <strong>Beta distribution</strong> with parameters $a$ and $b$, if its PDF is</p><script type="math/tex; mode=display">f(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1},0<x<1</script><p>where $\beta(a,b)$ is constant to make PDF integrate to 1, We write this as $X\sim Beta(a,b)$</p><p>By varying the values of $a$ and $b$, we get PDFs with a variety of shapes<br><img src="/2018/07/31/Stochastic-Process-7/sp7img1.jpg" align="justify"></p><p><em>Beta Distribution is the generalization of uniform distribution while $a=b=1$</em></p><p>The Beta is a flexible family of continuous distributions on (0,1), and has many stories. One is Beta r.v. often used to represent an unknown probability. <strong>we can use Beta to put probabilities on unknown probabilities</strong> If a parameter $p$ satisfies $0&lt;p&lt;1$, we can assume the prior distribution of $p$ is Beta(a,b)</p><h3 id="Beta-Integral"><a href="#Beta-Integral" class="headerlink" title="Beta Integral"></a>Beta Integral</h3><p>One inportant issue to analyse the Beta Distribution is the Integral</p><script type="math/tex; mode=display">\beta(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1}dx</script><h3 id="Bayes’-billiards"><a href="#Bayes’-billiards" class="headerlink" title="Bayes’ billiards"></a>Bayes’ billiards</h3><p> It’s hard to get the reuslt directly from calculus, the left and right sides of the above formulation can be connected by one event $P(X=k)$</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) x^k(1-x)^{n-k}dx = \frac{1}{n+1}</script><p><strong>Left side Story :</strong> Having $n+1$ balls , $n$ white and $1$ gray. Randomly throw each ball onto the interval $[0,1]$, so the position of balls are i.i.d. $Unif(0,1)$. Let $X$ be the number of white balls to the left of the gray ball.</p><p>To get the probability of the event $X=k$, we use LOTP. Conditioning on the position of gray ball, call it $B$, Conditional on $B=p$, the number of the white ball landing to the left of $p$ has $Bin(n,p)$ distribution, The PDF of $B$ is $f(p) =1$ since $B\sim Unif(0,1)$</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|B=p)f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-k)^{n-k}dp</script><p><strong>Right side Story :</strong> Having $n+1$ balls, all white, randomly throw onto unit interval; then choose one ball at random and paint it gray. Again, let $X$ be the number of white balls to the left of gray ball. By symmetry, any one of the $n+1$ balls is equally likely to be painted gray, then</p><script type="math/tex; mode=display">P(X=k) = \frac{1}{n+1}</script><p>$X$ has the same distribution, then</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}dp= \frac{1}{n+1}</script><p>Using the result, we are capable to calculate the $\beta(a,b)$ by substituting $a-1$ for $k$ and $b-1$ for $n-k$</p><script type="math/tex; mode=display">\beta(a,b) = \frac{1}{(a+b-1) \left( \begin{array}{c} a+b-2\\a-1 \end{array} \right)} = \frac{(a-1)!(b-1)!}{(a+b-1)!}</script><p>For a r.v. $X\sim Beta(a,b)$, The Expectation</p><script type="math/tex; mode=display">\begin{align}E(X) &= \int_0^1 xf(x)dx= \int_0^1 x\cdot \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}dx=\frac{1}{\beta(a,b)}\int_0^1 x^a(1-x)^{b-1}dx \\&= \frac{1}{\beta(a,b)}\cdot \beta(a+1,b) = \frac{\frac{a!(b-1)!}{(a+b)!}}{\frac{(a-1)!(b-1)!}{(a+b-1)!}} = \frac{a}{a+b}\end{align}</script><h3 id="Beta-Binomial-Conjugacy"><a href="#Beta-Binomial-Conjugacy" class="headerlink" title="Beta-Binomial Conjugacy"></a>Beta-Binomial Conjugacy</h3><p> Now let’s see the connection between Beta distribution and Binomial distribution, the relation we call it Conjugacy.</p><p>We have a coin lands head with p. $p$, and we dont know what %p% is. Our goal is to infer the value of $p$ after observing the outcomes of n tosses of the coin.<br><strong>Bayesian Inference</strong></p><ul><li>Treat all unknown probability $p$ as r.v. and give $p$ a distribution</li><li>Above is called <strong>prior distribution</strong>, it reflects out uncertainty about the ture value of $p$ before observing</li><li>After experiment and data are gathered, prior distribution is updated using Baye’s rule,; This yields the <strong>posterior distribution</strong>, which reflects the new beliefs about $p$</li><li>Specifically<ul><li><strong>prior distribution</strong> $f(p)$</li><li><strong>posterior distribution</strong> $f(p|X=n)$</li></ul></li></ul><p>Suppose the prior distribution on $p$ is Beta distribution. Let $p\sim Beta(a,b)$ for known constants $a$ and $b$, $X$ be the number of heads in $n$ tosses of the coin. Conditional on knowing ture value of $p$ then</p><script type="math/tex; mode=display">X|p \sim Bin(n,p)</script><p>We use the Bayes rule. Letting $f(p)$ be the prior distribution and $f(p|X=k)$ be the posterior distribution after observing $k$ heads</p><script type="math/tex; mode=display">f(p|X=k) = \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}\cdot \frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}</script><p>The denominator $P(X=k)$ is the marginal PMF of $X$, is given by</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|p) f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}f(p)dp</script><p>If $a=b=1$ , $P(X=k) = 1/(n+1)$, but it not seem easy to find $P(X=k)$ in general , Are we stuck ?</p><p>Actually, is much easier than it appears at first, the conditional PDF $f(p|X=k)$ is a function of $p$, so everything doesn’t depend on $p$ is just a constant. After dropping constants gives</p><script type="math/tex; mode=display">f(p|X=k) \propto p^{a+k-1}(1-p)^{b+n-k-1}</script><p>which is the $Beta(a+k,b+n-k)$ PDF.Therefore the posterior distribution of $p$ is</p><script type="math/tex; mode=display">p|X = k\sim Beta(a+k,b+n-k)</script><p>The posterior distribution of $p$ after observing $X=k$ is still a Beta distribution!</p><p>We say <em>Beta is the Conjugate prior of the Binomial</em></p><ul><li>We add the observed successes $k$ to the first parameter</li><li>We add the observed successes $k-n$ to the second parameter</li><li>$a$ and $b$ have a concrete interpretation in this context<ul><li>$a$ as the number of prior successes in earlier experiments</li><li>$b$ as the number of prior failures in earlier experiments</li></ul></li></ul><h3 id="Mean-vs-Bayesian-Average"><a href="#Mean-vs-Bayesian-Average" class="headerlink" title="Mean vs Bayesian Average"></a>Mean vs Bayesian Average</h3><ul><li>Mean: $\frac{k}{n}$</li><li>Bayesian Average: $E(p|X=k) = \frac{a+k}{a+b+n}$</li></ul><hr><h1 id="Dirichlet-Multinomial-Distribution"><a href="#Dirichlet-Multinomial-Distribution" class="headerlink" title="Dirichlet-Multinomial Distribution"></a>Dirichlet-Multinomial Distribution</h1><p>$n$ objects are independently placed into one of $k$ categories, with probability of $p_j$ to category j, and $\sum_{j=1}^k p_j = 1$. Let $X_i$ be the number of objects in category $i$, $X_1 + … + X_n = n$. Then $X = (X_1,…,X_k)$ is said to have <strong>Multinomial distribution</strong> with parameters $n$ and $\mathbf{p} = (p_1,…,p_k)$, write as $\mathbf{X} \sim Mult_k(n,\mathbf{p})$</p><h3 id="Theorem-Multinomial-Joint-PMF"><a href="#Theorem-Multinomial-Joint-PMF" class="headerlink" title="Theorem (Multinomial Joint PMF)"></a>Theorem (Multinomial Joint PMF)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then the joint PMF of $\mathbf{X}$ is</p><script type="math/tex; mode=display">P(X_1=n_1,...,X_k=n_k) = \frac{n!}{n_1!n_2!...n_k!}\cdot p_1^{n_1}p_2^{n_2}\dotsb p_k^{n_k}</script><h3 id="Theorem-Multinomial-Marginals"><a href="#Theorem-Multinomial-Marginals" class="headerlink" title="Theorem (Multinomial Marginals)"></a>Theorem (Multinomial Marginals)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then $X_j \sim Bin(n,p_j)$</p><h3 id="Theorem-Multinomial-Lumping"><a href="#Theorem-Multinomial-Lumping" class="headerlink" title="Theorem (Multinomial Lumping)"></a>Theorem (Multinomial Lumping)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then for distinct $i$ and $j$, $X_i+X_j\sim Bin(n,p_i+p_j)$</p><script type="math/tex; mode=display">(X_1+X_2,X_3,...,X_k)\sim Mult_k(n,((p_1+p_2),p_3,...,p_n))</script><p><img src="/2018/07/31/Stochastic-Process-7/sp7img2.jpg" style=" width:500px;"></p><h3 id="Theorem-Multinomial-Conditioning"><a href="#Theorem-Multinomial-Conditioning" class="headerlink" title="Theorem (Multinomial Conditioning)"></a>Theorem (Multinomial Conditioning)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then</p><script type="math/tex; mode=display">(X_2,...,X_k)|X_1=n_1\sim Mult_k(n-n_1,(p_2^{\prime},...,p_k^{\prime}))</script><p>where $p_j^{\prime} = p_j/(p_2+\dotsb +p_k)$</p><h3 id="Theorem-Covariance-in-A-Multinomial"><a href="#Theorem-Covariance-in-A-Multinomial" class="headerlink" title="Theorem (Covariance in A Multinomial)"></a>Theorem (Covariance in A Multinomial)</h3><p>Let $X_1,…,X_k\sim Mult_k(n,\mathbf{p})$, where $\mathbf{p} = (p_1,…,p_k)$.</p><script type="math/tex; mode=display">Cov(X_i,X_j) = -np_ip_j</script><h3 id="Definition-Dirichlet-Distribution"><a href="#Definition-Dirichlet-Distribution" class="headerlink" title="Definition (Dirichlet Distribution)"></a>Definition (Dirichlet Distribution)</h3><p>Dirichlet distribution is parameterized by a vector $\mathbf{\alpha}$ of positive real numbers. The PDF is:</p><script type="math/tex; mode=display">f(p_1,p_2,...,p_k;\alpha_1,\alpha_2,...,\alpha_k)=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\prod_{i=1}^k\Gamma(\alpha_i)}\prod_{i=1}^kp_i^{\alpha_i-1}</script><p>where $p_1 + … + p_k = 1$</p><h3 id="Dirichlet-Multinomial-Conjugacy"><a href="#Dirichlet-Multinomial-Conjugacy" class="headerlink" title="Dirichlet-Multinomial Conjugacy"></a>Dirichlet-Multinomial Conjugacy</h3><p>Assume we already have the Multinomial distribution $\mathbf{X}\sim Mult_k(n,\mathbf{p})$. The prior distribution of $\mathbf{p}=(p_1,…,p_k)$ is a Dirichlet distribution, i.e. $\mathbf{p}\sim Dir(\alpha)$. Denote $\mathbf{X} = (X_1,…,X_k)$, then</p><script type="math/tex; mode=display">\mathbf{X}|\mathbf{p}\sim Mult_k(n,\mathbf{p})</script><p>Let $f(\mathbf{p})$ to be the prior distribution of $\mathbf{p}$. The observations of the experiment is $\mathbf{N} = (n_1,…,n_k)$ then</p><script type="math/tex; mode=display">\begin{align}f(\mathbf{p}|\mathbf{X}=\mathbf{N}) &= \frac{P(\mathbf{X}=\mathbf{N}|\mathbf{p})f(\mathbf{p})}{P(\mathbf{X}=\mathbf{N})} \\&= \frac{\frac{n!}{n_1!\dotsb n_k!}p_1^{n_1}\dotsb p_k^{n_k}\cdot \frac{\Gamma(\sum_{i=1}^k \alpha_i)}{\prod_{i=1}^k \Gamma(\alpha_i)}\prod_{i=1}^k p_i^{\alpha_i -1}}{P(\mathbf{X}= \mathbf{N})} \\&\propto p_1^{n_1+\alpha_1 - 1}\dotsb p_k^{n_k+\alpha_k - 1} \\&\sim Dir(\mathbf{\alpha} + \mathbf{N})\end{align}</script><p>Thus we can see that</p><script type="math/tex; mode=display">\text{prior}\ Dir(\mathbf{\alpha}) \rightarrow \text{posterior}\ Dir(\mathbf{\alpha} + \mathbf{N})</script><script type="math/tex; mode=display">\alpha_i \rightarrow \alpha_i + n_i</script><p>We can prove that</p><script type="math/tex; mode=display">E[p_i|\mathbf{X} = \mathbf{N}] = \frac{\alpha_i + n_i}{\sum_{i=1}^k(\alpha_i + n_i)}</script><hr><h1 id="Bayesian-Average"><a href="#Bayesian-Average" class="headerlink" title="Bayesian Average"></a>Bayesian Average</h1><p>One application for Bayesian Average is in <strong>Rating System</strong>, usually the customers will rate the movies in 5 star.</p><p>This will rose a problem, which One to Choose</p><ul><li>$5$ Average rating movie A by $1$ voter</li><li>$4.9998$ Average rating movie B by $1400010123$ voter (of course this One)</li></ul><p>To use the Bayesian estimation to compute the posterior probability for star ratings, we must use a <strong>joint distribution</strong>, the random variable is a categorical distribution with probability follows:</p><script type="math/tex; mode=display">p_1+p_2+p_3+p_4+p_5 = 1</script><h5 id="Multinomial-Distribution"><a href="#Multinomial-Distribution" class="headerlink" title="Multinomial Distribution"></a>Multinomial Distribution</h5><p>Let $O$ be the event of movie rating, we compute the posterior probability with $N$ observations for five categories with corresponding numbers $K_1,K_2,K_3,K_4,K_5$ as follows:</p><script type="math/tex; mode=display">Pr(O|p_1,p_2,p_3,p_4,p_5)\propto p_1^{K_1}p_2^{K_2}p_3^{K_3}p_4^{K_4}p_5^{K_5}</script><p>where $K_1+…+K_5 = N$</p><h5 id="Dirichlet-Distribution-Prior"><a href="#Dirichlet-Distribution-Prior" class="headerlink" title="Dirichlet Distribution: Prior"></a>Dirichlet Distribution: Prior</h5><script type="math/tex; mode=display">Pr(p_1,p_2,p_3,p_4,p_5|O) \propto \prod_{j=1}^5 p_j^{K_j+\alpha_j^0 - 1}</script><p>After considering the new votes we can update the distribution of the $\mathbf{p}$ by</p><script type="math/tex; mode=display">\alpha_j^1 = K_j +\alpha_k^0</script><h5 id="Expected-Average"><a href="#Expected-Average" class="headerlink" title="Expected Average"></a>Expected Average</h5><p>What we need is the average rating given posterior in the shape of our Dirichlet distribution:</p><script type="math/tex; mode=display">E(p_1+2p_2+3p_3+4p_4+5p_5|O) = \sum_{i=1}^5 iE(p_i|O)</script><p>According to</p><script type="math/tex; mode=display">E(p_i|O) = \frac{\alpha_i^1}{\sum_{j=1}^5 \alpha_j^1}</script><p>We have</p><script type="math/tex; mode=display">\sum_{i=1}^5 iE(p_i|O) = \frac{\sum_{i=1}^5 i\alpha_i^0 + \sum_{i=1}^5 i K_i}{\sum_{j=1}^5 \alpha_j^0 +N}</script><h3 id="Bayesian-Average-Rating"><a href="#Bayesian-Average-Rating" class="headerlink" title="Bayesian Average Rating"></a>Bayesian Average Rating</h3><p>The final formulation can be express as</p><script type="math/tex; mode=display">\text{Bayes Average Rating} = \frac{C\cdot m +\sum(ratings)}{C+N}</script><ul><li>N: The number of ratings</li><li>m: a prior for the average of rating scores</li><li>C: a prior for the number of rating scores</li></ul><hr><h1 id="Gamma-Distribution"><a href="#Gamma-Distribution" class="headerlink" title="Gamma Distribution"></a>Gamma Distribution</h1><h3 id="Definition-Gamma-Function"><a href="#Definition-Gamma-Function" class="headerlink" title="Definition (Gamma Function)"></a>Definition (Gamma Function)</h3><p>The Gamma function $\Gamma$ is defined by</p><script type="math/tex; mode=display">\Gamma(a) = \int_0^{\infty}x^ae^{-x}\frac{dx}{x}</script><p>Earlier Beta distribution can represent an unknown probability of success cause its support is $(0,1)$. The Gamma distribution can represent an unknown rate in a Poisson process because its support is $(0,\infty)$</p><h3 id="Property-of-Gamma-Function"><a href="#Property-of-Gamma-Function" class="headerlink" title="Property of Gamma Function"></a>Property of Gamma Function</h3><ul><li>$\Gamma(a+1) = a\Gamma(a)$</li><li>$\Gamma(n) = (n-1)!$ if $n$ is a integer</li></ul><h3 id="Definition-Gamma-Distribution"><a href="#Definition-Gamma-Distribution" class="headerlink" title="Definition (Gamma Distribution)"></a>Definition (Gamma Distribution)</h3><p>An r.v. $Y$ is said to have <strong>Gamma distribution</strong> with parameters $a&gt;0, \lambda&gt;0$, if its PDF is</p><script type="math/tex; mode=display">f(y) = \frac{1}{\Gamma(a)}(\lambda y)^a e^{-\lambda y} \frac{1}{y}, y>0</script><p>Write $Y\sim Gamma(a,\lambda)$. <strong>Gamma distribution is a generalization of exponential distribution</strong> when $a=1$</p><h3 id="PDF-of-Gamma-distribution"><a href="#PDF-of-Gamma-distribution" class="headerlink" title="PDF of Gamma distribution"></a>PDF of Gamma distribution</h3><p><img src="/2018/07/31/Stochastic-Process-7/sp7img3.jpg" align="justify"></p><h3 id="Moments-of-Gamma-Distribution"><a href="#Moments-of-Gamma-Distribution" class="headerlink" title="Moments of Gamma Distribution"></a>Moments of Gamma Distribution</h3><script type="math/tex; mode=display">E[X] = \frac{a}{\lambda}, Var(X) = \frac{a}{\lambda^2}</script><h3 id="Theorem-Gamma-Convolution-of-Exponential"><a href="#Theorem-Gamma-Convolution-of-Exponential" class="headerlink" title="Theorem (Gamma: Convolution of Exponential)"></a>Theorem (Gamma: Convolution of Exponential)</h3><p>Let $X_1,…,X_n$ be i.i.d. $Expo(\lambda)$ , Then</p><script type="math/tex; mode=display">X_1+\dotsb +X_n \sim Gamma(n,\lambda)</script><h3 id="Beta-Gamma-Connection"><a href="#Beta-Gamma-Connection" class="headerlink" title="Beta-Gamma Connection"></a>Beta-Gamma Connection</h3><p>We have independent Gamma r.v.s $X$ and $Y$ with the same rate $\lambda$</p><ul><li>$X+Y$ had Gamma distribution</li><li>$\frac{X}{X+Y}$ has Beta distribution</li></ul><h3 id="Binomial-amp-Poisson-amp-Gamma"><a href="#Binomial-amp-Poisson-amp-Gamma" class="headerlink" title="Binomial &amp; Poisson &amp; Gamma"></a>Binomial &amp; Poisson &amp; Gamma</h3><ul><li>The PMF of $Poisson(X=k|\lambda)$ is $P(X=k|\lambda) = \frac{\lambda^ke^{-\lambda}}{k!}$</li><li>The PDF of $X\sim Gamma(a,1)$ is $f_X(x) = \frac{1}{\Gamma(a)}a^{a-1}e^{-x}$. Given $a=k+1$ we have<script type="math/tex; mode=display">P(X=x|a=k+1,1)=\frac{x^{k+1}}{\Gamma(k+1)}e^{-x}= \frac{x^k}{k!}e^{-x}</script></li><li>For a r.v. $X\sim Bin(n,p)$, we have<script type="math/tex; mode=display">P(X\leq k)=\frac{n!}{k!(n-k-1)!}\int_p^1 t^k(1-t)^{n-k-1}dt</script>Let $t=\frac{x}{n}$, then<script type="math/tex; mode=display">P(X\leq k) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n})dx</script>It follows that<script type="math/tex; mode=display">Bin(X\leq k|n,p) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n}dx)</script></li><li>Let $\lambda = np$. We fix $\lambda$, and let $n\rightarrow \infty$, then<script type="math/tex; mode=display">Bin(n,p) \rightarrow Posi(\lambda)</script></li><li>When $\lambda \rightarrow 0$, we have<script type="math/tex; mode=display">1=\mathop{lim}_{\lambda \rightarrow 0} \int_{\lambda}^{\infty} \frac{\lambda^k e^{-x}}{k!}dx = \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx=\int_0^{\infty} Gamma(k+1,1)dx</script></li><li>$1=\int_0^{\infty} \frac{x^ke^{-x}}{k!}dx\Rightarrow k!=\int_0^{\infty}x^k e^{-x}dx$</li><li>Because $Pois(X\leq k|\lambda) = \int_{\lambda}^{\infty}\frac{x^k e^{-x}}{k!} dx$ we have<script type="math/tex; mode=display">Pois(X\leq k|\lambda) + \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx = 1</script></li></ul><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Beta-Binomial-Distribution&quot;&gt;&lt;a href=&quot;#Beta-Binomial-Distribution&quot; class=&quot;headerlink&quot; title=&quot;Beta-Binomial Distribution&quot;&gt;&lt;/a&gt;Beta-Bin
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Multivariate)</title>
    <link href="http://yoursite.com/2018/07/29/Stochastic-Process-6/"/>
    <id>http://yoursite.com/2018/07/29/Stochastic-Process-6/</id>
    <published>2018-07-29T00:41:09.000Z</published>
    <updated>2018-08-02T01:39:52.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Discrete-Multivariate-R-V-s"><a href="#Discrete-Multivariate-R-V-s" class="headerlink" title="Discrete Multivariate R.V.s"></a>Discrete Multivariate R.V.s</h1><p><strong>Definition (Joint CDF)</strong> The Joint CDf of r.v.s $X$ and $Y$ is the function $F_{X,Y}$ given by</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = P(X\leq x,Y\leq y)</script><p><strong>Definition (Joint PMF)</strong> The Joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X,Y}$ given by</p><script type="math/tex; mode=display">p_{X,Y}(x,y) = P(X=x,Y=y)</script><p><strong>Definition (Marginal PMF)</strong> For discrete r.v.s $X$ and $Y$, Marginal PMF of $X$ is</p><script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x,Y=y)</script><p><strong>Definition (Conditional PMF)</strong> For discrete r.v.s $X$ and $Y$, the Conditional PMF of $X$ given $Y=y$ is</p><script type="math/tex; mode=display">P_{X|Y}(x|y) = P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}</script><p><strong>Definition (Independence of Discrete R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x) F_Y(y)</script><p>for all x and y also equivalent to the condition</p><script type="math/tex; mode=display">P(Y=y|X=x) = P(Y=y)</script><hr><h1 id="Continuous-Multivariate-R-V-s"><a href="#Continuous-Multivariate-R-V-s" class="headerlink" title="Continuous Multivariate R.V.s"></a>Continuous Multivariate R.V.s</h1><p><strong>Definition (Joint PDF)</strong> If $X$ and $Y$ are continuous with joint CDF $F_{X,Y}$ then</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y)</script><p><strong>Definition (Marginal PDF)</strong> If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ then</p><script type="math/tex; mode=display">f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy</script><p><strong>Definition (Conditional PDF)</strong> For continuous r.v.s. $X$ and $Y$ with joint PDF $f_{X,Y}$ the Conditional PDF of $Y$ given $X=x$ is</p><script type="math/tex; mode=display">f_{Y|X}(y|x)= \frac{f_{X,Y}(x,y)}{f_X(x)}</script><p><strong>Definition (Independence of Continuous R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x)F_Y(y)</script><p>If $X$ and  $Y$ are continuous with joint PDF $f_{X,Y}$</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = f_X(x) f_Y(y)</script><p><strong>Theorem (2D LOTUS)</strong> Let g be a function from $R^2$ to $R$</p><p>If $X$ and $Y$ are discrete</p><script type="math/tex; mode=display">E(g(X,Y)) = \sum_x \sum_y g(x,y) P(X=x,Y=y)</script><p>If $X$ and $Y$ are continuous</p><script type="math/tex; mode=display">E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f_{X,Y}(x,y) dxdy</script><h3 id="General-Bayes’-Rule"><a href="#General-Bayes’-Rule" class="headerlink" title="General Bayes’ Rule"></a>General Bayes’ Rule</h3><p><img src="/2018/07/29/Stochastic-Process-6/img1.jpg" align="justify"></p><hr><h1 id="Convariance-and-Correlation"><a href="#Convariance-and-Correlation" class="headerlink" title="Convariance and Correlation"></a>Convariance and Correlation</h1><p><strong>Covariance</strong></p><ul><li>Measure a tendency of two r.v.s $X\&amp;Y$ to go up or down together</li><li>Positive Covariance: $X$ go up, $Y$ tends go up</li><li>Negative Covariance: $X$ go up, $Y$ tends go down</li></ul><p><strong>Definition (Covariance)</strong> The covariance between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Cov(X,Y) = E((X-EX)(Y-EY))=E(XY)-E(X)E(Y)</script><p><strong>Theorem (Uncorrelated)</strong> If $X$ and $Y$ are independent, then they are Uncorrelated($Cov(X,Y)=0$)</p><h3 id="Properties-of-Covariance"><a href="#Properties-of-Covariance" class="headerlink" title="Properties of Covariance"></a>Properties of Covariance</h3><ul><li>$Cov(X,X) = Var(X)$</li><li>$Cov(X,Y) = Cov(Y,X)$</li><li>$Cov(X,c) = 0$</li><li>$Cov(a\cdot X,Y) = a\cdot Cov(X,Y)$</li><li>$Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$</li><li>$Cov(X+Y,W+Z) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$</li><li>$Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$</li><li>For n r.v.s $X_1,\dotsb ,X_n$ <script type="math/tex">Var(X_1+\dotsb +X_n)=Var(X_a)+\dotsb+Var(X_n)+2\sum_{i<j}Cov(X_i,Y_j)</script></li></ul><p><strong>Definition (Correlation)</strong> The Correlation between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}</script><p><em>Shifting and Scaling $X$ and $Y$ has no effect on correlation</em><br><img src="/2018/07/29/Stochastic-Process-6/img2.jpg" align="justify"></p><p><strong>Theorem (Correlation Bounds)</strong> For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">-1\leq Corr(X,Y) \leq 1</script><hr><h1 id="Change-of-Variables"><a href="#Change-of-Variables" class="headerlink" title="Change of Variables"></a>Change of Variables</h1><p><strong>Theorem (Change of Variables in One Dimension)</strong> Let $X$ be a continuous r.v. with PDF $f_X$, and let $Y = g(X)$, where $g$ is differentiable and strictly increasing. Then the PDF of $Y$ is given by</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left|  \frac{dx}{dy} \right|</script><p>where $x = g^{-1}(y)$</p><p><strong><em>Proof</em></strong>:</p><script type="math/tex; mode=display">F_Y(y) = P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_X(g^{-1}(y))=F_X(x)</script><p>Then result obtained By the chain rule</p><p><strong>Theorem (Change of Variables)</strong> Let $X = (X_1,…,X_n)$ be a continuous random vector with joint PDF $f_X(x)$ and $Y=g(X)$, $g$ is an invertible function from $R^n$ to $R^n$ then $\frac{\partial \mathbf{x}}{\partial \mathbf{y}}$ form a <strong><em>Jacobian  matrix</em></strong></p><script type="math/tex; mode=display">\frac{\partial \mathbf{x}}{\partial \mathbf{y}} =\left( \begin{array}{cccc}\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \dotsb & \frac{\partial x_1}{\partial y_n} \\\vdots & \vdots & & \vdots \\\frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \dotsb & \frac{\partial x_n}{\partial y_n} \\\end{array} \right)</script><p>Then the joint PDF of $Y$ is</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left| \frac{\partial \mathbf{x}}{\partial \mathbf{y}} \right|</script><hr><h1 id="Convolutions"><a href="#Convolutions" class="headerlink" title="Convolutions"></a>Convolutions</h1><p><strong>Theorem (Convolution Sums and Integrals)</strong><br>If $X$ and $Y$ are independent discrete r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  P(T=t) =& \sum_x P(Y = t-x) P(X=x) \\         =& \sum_y P(X= t-y) P(Y=y)\end{align}</script><p>If $X$ and $Y$ are independent continuous r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  f_T(t)  =& \int_{-\infty}^{\infty} f_Y(t-x) f_X(x) dx \\          =& \int_{-\infty}^{\infty} f_X(t-y) f_Y(y) dy\end{align}</script><hr><h1 id="Order-Statistics"><a href="#Order-Statistics" class="headerlink" title="Order Statistics"></a>Order Statistics</h1><p><strong>Definition (Order Statistics)</strong> For r.v.s $X_1,X_2,…,X_n$ the order statistics sre the random variables $X_{(1)},…,X_{(2)}$, where</p><ul><li>$X_{(1)}  = min (X_1,…,X_n)$</li><li>$X_{(2)}$ is the $2^{nd}$ of $X_1,…,X_n$</li><li>$\vdots$</li><li>$X_{n} = max(X_1,…,X_n)$</li></ul><p>The order statistics are dependent, for example , if $X_{(1)} = 100$, then $X_{(n)}$ is forced to be $\geq 100$</p><p>We foucs on the case $X_1,…,X_n$ are i.i.d continuous r.v.s, with CDF $F$ and PDF $f$</p><p><strong>Theorem (CDF of Order Statistics)</strong>  Let $X_1,…,X_n$ be i.i.d continuous r.v.s with CDF F, Then the CDF of the $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">P(X_{(j)}\leq x)=\sum_{k=j}^n\left( \begin{array}{c} n\\k  \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><p><strong><em>Proof:</em></strong></p><hr><p>Let’s start with a specical case when $j=n, X_{(n)}=max(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(n)}} (x) &= P[max(X_1,...,X_n)\leq x] \\&=P(X_1\leq  x)\dotsb P(X_n\leq x) \\&=[F(x)]^n\end{align}</script><hr><p>Then, consider another special case when $j=1, X_{(1)} = min(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(1)}} (x) &= P[min(X_1,...,X_n)\leq x] \\&=1 - P(X_1>  x)\dotsb P(X_n> x) \\&=1-[1-F(x)]^n\end{align}</script><p>The result here can be rewrite as $\sum_{k=1}^n\left( \begin{array}{c} n\\k \end{array} \right) F(x)^k (1-F(x))^{n-k}$</p><p>This result can be obtained by expand $[F(x) + 1 -F(x)]^n$</p><hr><p>Finally, let’s consider more general case where $1&lt;j&lt;n, X_{(j)}\leq x$, this means at least $j$ of $\{X_i \}$ fall to the left of $x$</p><p>Denote $N$ as the nunber of $X_i$ landing to the left of $x$. $X_i$ lands to the left of $x$ w.p. $P(X_i\leq x) = F(x)$. Then $N\sim Bin(n,F(x))$</p><script type="math/tex; mode=display">P(X_{(j)}\leq x) = P(N\geq j=\sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><hr><p><strong>Theorem (PDF of Order Statistic)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with CDF $F$ and PDF $f$. Then the marginal PDF of $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">f_{X_{(j)}} (x) = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) f(x) F(x)^{j-1} (1-F(x))^{n-j}</script><p><strong>Theorem (Joint PDF)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with PDF $f$, Then the joint PDF of all order statistics is</p><script type="math/tex; mode=display">f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n) = n! \prod_{i=1}^n f(x_i), x_1<x_2<\dotsb <x_n</script><p><strong>Example 1(Order Statistics of Uniforms)</strong> $U_1,U_2,…,U_n$ are i.i.d. $Unif(0,1)$ r.v.s with CDF F and PDF $f$</p><p>For $0\leq x\leq 1$ ,$f(x) = 1$ , $F(x) = x$, Then</p><script type="math/tex; mode=display">f_{U_{(j)}} = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) x^{j-1} (1-x)^{n-j}</script><script type="math/tex; mode=display">F_{U_{(j)}}(x) = \sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right)x^k (1-x)^{n-k} = \int_0^x f_{U_{(j)}} (t) dt = \frac{n!}{(j-1)!(n-j)!}\int_0^x t^{j-1}(1-t)^{n-j}dt</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Discrete-Multivariate-R-V-s&quot;&gt;&lt;a href=&quot;#Discrete-Multivariate-R-V-s&quot; class=&quot;headerlink&quot; title=&quot;Discrete Multivariate R.V.s&quot;&gt;&lt;/a&gt;Discr
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Topology-(引论)</title>
    <link href="http://yoursite.com/2018/07/28/Topology-1/"/>
    <id>http://yoursite.com/2018/07/28/Topology-1/</id>
    <published>2018-07-28T12:19:29.000Z</published>
    <updated>2018-07-29T00:40:25.323Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Eular-定理"><a href="#Eular-定理" class="headerlink" title="Eular 定理"></a>Eular 定理</h1><p>对于一个多面体 P，我们定义</p><ul><li>v：定点数</li><li>e：棱边数</li><li>f：面数</li></ul><p>Eular 定理：$v+f-e = 2$</p><p>但是满足这个定理的多面体是有条件的：</p><ul><li>P 的任何两个顶点可以用一串棱相连接<ul><li>反例：中空的立方体</li></ul></li><li>P 上任意由直线段构成的圈，把 P 分割成两片<ul><li>反例：螺帽柱状体</li></ul></li></ul><p>Eular 定理证明：我们首先来看看树形，这个在图论里面常常出现，树有个性质就是 $v-e=1$，我们可以尝试用一棵树 T 来表示一个多面体，表示的方法是，树中的点就是 P 中的点（树 T 中的点囊括了所有 P 的点），树中的边就是 P 中的棱（边只是一部分的棱哦）。</p><p>然后我们来构造 T 的一种对偶，称为 $\Gamma$，$\Gamma$ 也是一颗树后面会证明，只不过这棵树的点由 P 中面的中心点来表示（也就是用来表示面的数量），这样面与面之间的边在多面体中是可以有一个曲折的，可以想象一下。。</p><p>上面采用这个形式只是因为这样构造能囊括所有的面，下面来证明一下这个 $\Gamma$ 是树，而且曲折所在的棱刚好是 T 的边对于 P 中棱的补集：</p><ul><li>连通性：如果 $\Gamma$ 的某两个顶点不能用 $\Gamma$ 内的一串棱连接，则它们必然被一个圈分开。由于 T 不含任何圈，$\Gamma$ 必然联通。</li><li>无圈：如果 $\Gamma$ 有圈，那么就会把顶点分开成两份，T 中的棱想要连接所有顶点就不可避免的要触碰到这个圈，所以 $\Gamma$ 无圈</li><li>$T,\Gamma$ 包含所有棱：假设一条棱没有被用着，这个棱本可以这样被用：棱两侧的面中点相连（$\Gamma$），或者棱两端的点相连（T），但是都没用着，这样 $\Gamma ,T$ 就会在后面相交。。（这是我的数学直觉，书上并没有这个的证明，我自己补的。。。不是很严谨。。。）</li></ul><p>最后我们有 $v(T) - e(T) = 1$, $v(\Gamma) - e(\Gamma) = 1$, 加起来有</p><script type="math/tex; mode=display">v(T) - [e(T)+e(\Gamma)] + v(\Gamma) = 2</script><p>同时，根据构造有</p><script type="math/tex; mode=display">v(T) = v, e(T) + e(\Gamma)+e, v(\Gamma) = f</script><h2 id="其他的证明方式可以用数学归纳法"><a href="#其他的证明方式可以用数学归纳法" class="headerlink" title="其他的证明方式可以用数学归纳法"></a>其他的证明方式可以用数学归纳法</h2><h1 id="拓扑等价"><a href="#拓扑等价" class="headerlink" title="拓扑等价"></a>拓扑等价</h1><p>我们考虑一个正四面体未冲气的气球，我们把它吹胖，吹成了一个圆形。</p><p>这样多面体的点和球面的点之间的对应就是<strong>拓扑等价</strong>或<strong>同胚</strong>的一个例子，确切的说就是一对一的连续满映射</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Eular-定理&quot;&gt;&lt;a href=&quot;#Eular-定理&quot; class=&quot;headerlink&quot; title=&quot;Eular 定理&quot;&gt;&lt;/a&gt;Eular 定理&lt;/h1&gt;&lt;p&gt;对于一个多面体 P，我们定义&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v：定点数&lt;/li&gt;
&lt;li&gt;e：棱
      
    
    </summary>
    
    
      <category term="Topology" scheme="http://yoursite.com/tags/Topology/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Generating Function)</title>
    <link href="http://yoursite.com/2018/07/27/Stochastic-Process-5/"/>
    <id>http://yoursite.com/2018/07/27/Stochastic-Process-5/</id>
    <published>2018-07-27T02:23:34.000Z</published>
    <updated>2018-07-28T09:06:43.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Generating-Function"><a href="#Generating-Function" class="headerlink" title="Generating Function"></a>Generating Function</h1><h3 id="Three-kinds-of-generating-functions"><a href="#Three-kinds-of-generating-functions" class="headerlink" title="Three kinds of generating functions"></a>Three kinds of generating functions</h3><ul><li>Probability Generating Function (PGF) : related to Z-transform</li><li>Moment Generating Function (MGF) : related to Laplace transform</li><li>Characteristic Function (CF) : related to Fourier transform</li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>PGF: handling non-negative integral random variables</li><li>MGF: handling general random variables</li><li>CF: equally useful with MGF</li></ul><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><ul><li>Easy to characterizing the distribution of the sum of independent random variables</li><li>Play a central role in the study of branching processes</li><li>Provide a bridge between complex analysis and probability</li><li>……</li></ul><hr><h1 id="Moment-Generating-Function"><a href="#Moment-Generating-Function" class="headerlink" title="Moment Generating Function"></a>Moment Generating Function</h1><p><strong>Definition (Moment Generating Function)</strong> MGF of an r.v. $X$ is $M(t) = E(e^{tX})$, as a function of $t$ (different t denote different valued moment) <strong><em>and this must finite on some open interval (-a,a) containing 0</em></strong> or dont exist.</p><h3 id="Why-we-need-MGF"><a href="#Why-we-need-MGF" class="headerlink" title="Why we need MGF"></a>Why we need MGF</h3><ul><li>MGF encodes the moments of an r.v.</li><li>MGF of an r.v. Determines its distribution, like CDF and PMF/PDF</li><li>MFG make it easy to find the distribution of a sum of i.r.v.s.</li></ul><p><strong>Theorem (Moments via Derivatives of the MGF)</strong> $E(X^n) = M^{(n)}(0)$<br>Using Taylor expansion of $M(t)$ at 0</p><script type="math/tex; mode=display">M(t) = \sum_{n=0}^{\infty} M^{(n)}(0) \frac{t^n}{n!}</script><p>Using Taylor expansion of $E(X)$</p><script type="math/tex; mode=display">M(t) = E(e^{tX}) = E\left(  \sum_{n=0}^{\infty} X^n \frac{t^n}{n!}  \right)=\sum_{n=0}^{\infty} E(X^n) \frac{t^n}{n!}</script><p>Matching the coefficients of two expansions, we get $E(X^n) = M^{(n)}(0)$</p><h3 id="MGF-of-Distribution"><a href="#MGF-of-Distribution" class="headerlink" title="MGF of Distribution"></a>MGF of Distribution</h3><p><strong>Theorem (MGF Determines the Distribution)</strong> Two r.v. have the same MGF have the same distribution, more strictly, if there is even a tiny interval containing 0 on which the MGF are equal, the the r.v.s must have same distribution.</p><p><strong>Example 1 (Bernoulli MGF)</strong> MGF of $X\sim Bern(p)$<br>$e^{tX}=e^t$ with probability $p$, and $1$ with probability $q$, so $M(t) = E(e^{tX})=pe^t + q$</p><p><strong>Example 2 (Geometric MGF)</strong> MGF of $X\sim Geom(p)$</p><script type="math/tex; mode=display">M(t) = E(e^{tX})=\sum_{k=0}^{\infty} e^{tk}q^kp=p\sum_{k=0}^{\infty} (qe^t)^k=\frac{p}{1-qe^t}</script><p>t in $(-\infty, log(1/p))$</p><p><strong>Example 3 (Uniform MGF)</strong> MGF of $U\sim Unif(a,b)$</p><script type="math/tex; mode=display">M(t) = E(e^{tU}) = \int_a^b e^{tu}\frac{1}{b-a} du = \frac{e^{tb}-e^{ta}}{t(b-a)}</script><p>and $M(0) = 1$</p><p><strong>Example 4 (Binomial MGF)</strong> $Bin(n,p)$</p><script type="math/tex; mode=display">M(t) = (pe^t+q)^n</script><p><strong>Example 5 (Negative Binomial)</strong> $NBin(r,p)$</p><script type="math/tex; mode=display">M(t) = \left(  \frac{p}{1-qe^t}  \right)^r</script><p><strong>Theorem (MGF of Location-Scale Transformation)</strong> If $X$ has MGF $M(t)$, then MGF of $a+bX$ is</p><script type="math/tex; mode=display">E(e^{t(a+bX)})=e^{at}E(e^{btX})=e^{at}M(bt)</script><p><strong>Example 6 (Normal MGF)</strong> MGF of $(X = \mu + \sigma Z) \sim N(\mu,\sigma^2)$</p><script type="math/tex; mode=display">M_Z(t) = E(e^{tZ})=\int_{-\infty}^{\infty}e^{tz}\frac{1}{\sqrt{2\pi}}e^{-z^2/2}dz=e^{t^2/2}</script><p>Use the Theorem above then</p><script type="math/tex; mode=display">M_X(t) = e^{\mu t}M_Z(\sigma t) = e^{\mu t}e^{(\sigma t)^2/2}  = e^{\mu t + \frac{1}{2} \sigma^2 t^2}</script><h3 id="Sum-of-Independent-Distributions"><a href="#Sum-of-Independent-Distributions" class="headerlink" title="Sum of Independent Distributions"></a>Sum of Independent Distributions</h3><p><strong>Theorem (MGF of A Sum of Independent R.V.s)</strong> If $X$ and $Y$ are independent, Then</p><script type="math/tex; mode=display">M_{X+Y} (t) = M_X(t) M_Y(t)</script><p><strong>Example 1 (Sum of Poissons)</strong> $X\sim Pois(\lambda), Y\sim Pois(\mu)$, $X$ and $Y$ are independent. Then $X+Y \sim Pois(\lambda + \mu)$<br>The MGF of $X$ is</p><script type="math/tex; mode=display">E(e^{tX}) = \sum_{k=0}^{\infty}e^{tk}\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^{\infty}\frac{(\lambda e^t)^k}{k!}=e^{-\lambda}e^{\lambda e^t}=e^{\lambda(e^t-1)}</script><p>The MGF of $X+Y$ is</p><script type="math/tex; mode=display">E(e^{tX})E(e^{tY}) = e^{\lambda (e^t-1)} e^{\mu (e^t-1)} = e^{(\lambda + \mu)(e^t-1)}</script><p>Which is the $Pois(\lambda + \mu)$, so $X+Y\sum Pois(\lambda+\mu)$</p><p><strong>Example 2 (Sum of Normals)</strong> $X_1\sim N(\mu_1,\sigma_1^2)$ and $X_2 \sim N(\mu_2,\sigma_2^2)$, $X_1+X_2 = ?$<br>MGF of $X_1+X_2$ is</p><script type="math/tex; mode=display">M_{X_1+X_2}(t)=  M_{X_1}(t)M_{X_2}(t)= e^{\mu_1t+\frac{1}{2}\sigma^2_1t^2}\cdot e^{\mu_2 t+\frac{1}{2}\sigma_2^2 t^2}=e^{(\mu_1+\mu_2)t+\frac{1}{2}(\sigma_1^2+\sigma_2^2)t^2}</script><p>Which is the N(\mu_1 + \mu_2, \sigma_2^2 + \sigma_1^2) MGF.</p><hr><h1 id="Probability-Generating-Function"><a href="#Probability-Generating-Function" class="headerlink" title="Probability Generating Function"></a>Probability Generating Function</h1><p><strong>Definition (PGF)</strong> PGF of a nonnegative integer-valued r.v. $X$ with PMF $p_k = P(X=k)$ is the generating function of the PMF, By LOTUS , this is</p><script type="math/tex; mode=display">E(t^X) = \sum_{k=0}^{\infty} p_k t^k</script><p><strong>Example 1 (Generating Dice Probabilities)</strong> Let $X$ be the sum from rolling 6 pair dice, $X_1,…,X_6$ be the individual rolls, what is $P(X=18)$ ?<br>The PGF of $X_1$ is</p><script type="math/tex; mode=display">E(t^{X_1}) = \frac{1}{6}(t+t^2+\dotsb+t^6)</script><p>The PGF of $X$ is</p><script type="math/tex; mode=display">E(t^X) = E(t^{X_1}\dotsb t^{X_6}) = E(t^{X_1})\dotsb E(t^{X_6})=\frac{t^6}{6^6}(1+t+\dotsb +t^5)^6</script><p>The coefficient of $t^{18}$ in the PGF is $P(X=18)$, so</p><script type="math/tex; mode=display">P(X=18) = \frac{3421}{6^6}</script><p><strong>Theorem (PMF \&amp; PGF)</strong></p><script type="math/tex; mode=display">P(X=k) = \frac{g_{X}^{(k)}(0)}{k!}</script><hr><h1 id="Characteristic-Function"><a href="#Characteristic-Function" class="headerlink" title="Characteristic Function"></a>Characteristic Function</h1><p><strong>Definition CF</strong> The Characteristic function of a random variable $X$ is the function $\phi : R \rightarrow C$ defined by</p><script type="math/tex; mode=display">\phi(t) = E(e^{itX}), i = \sqrt{-1}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Generating-Function&quot;&gt;&lt;a href=&quot;#Generating-Function&quot; class=&quot;headerlink&quot; title=&quot;Generating Function&quot;&gt;&lt;/a&gt;Generating Function&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Continuous Random Variable)</title>
    <link href="http://yoursite.com/2018/07/25/Stochastic-Process-4/"/>
    <id>http://yoursite.com/2018/07/25/Stochastic-Process-4/</id>
    <published>2018-07-25T12:04:57.000Z</published>
    <updated>2018-07-28T12:05:50.907Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Probability-Density-Function"><a href="#Probability-Density-Function" class="headerlink" title="Probability Density Function"></a>Probability Density Function</h1><p><strong>Definition (Probability Density Function)</strong>: For a continuous r.v. $X$ with CDF $F$, the <strong><em>probability density function (PDF)</em></strong> of $X$ is the derivative $f$ of the $F$</p><script type="math/tex; mode=display">P(a\leq X \leq b) = \int_a^b f_X(x)dx = F(b) - F(a)</script><script type="math/tex; mode=display">P(X\in [x,x+\delta]) \approx f_X(x) \cdot \delta</script><p><strong>Relation between PDF &amp; PMF</strong>: The PDF is the analogous to the PMF in many ways. But, the PDF $f(x)$ is not a probability.</p><p><strong>Relation between PDF &amp; CDF</strong>: Let $X$ be a continuous r.v. with PDF $f$. Then the CDF of $X$ is given by</p><script type="math/tex; mode=display">F(x) = \int_{-\infty}^x f(x) dt</script><p><strong>CDF of Logistic Distribution</strong></p><script type="math/tex; mode=display">F(x) = \frac{e^x}{1+e^x}, x\in R</script><p><strong>CDF of Rayleigh Distribution</strong></p><script type="math/tex; mode=display">F(x) = 1 - e^{-x^2/2}, x>0</script><p><strong>Theorem (Expectation of Continuous R.V.)</strong></p><script type="math/tex; mode=display">E(X) = \int_{-\infty}^{\infty} x f(x) dx</script><p><strong>Theorem (Expectation via Survial Function)</strong> Let $G$ be the survial function of $X$, Then</p><script type="math/tex; mode=display">E(X) = \int_0^{\infty} G(x) dx</script><p><strong>Theorem (LOTUS: Continuous)</strong></p><script type="math/tex; mode=display">E(g(X)) = \int_{-\infty}^{\infty} g(x) f(x) dx</script><hr><h1 id="Uniform-Distribution"><a href="#Uniform-Distribution" class="headerlink" title="Uniform Distribution"></a>Uniform Distribution</h1><p><strong>Definition (Uniform Distribution)</strong> Distribution on the interval$(a,b)$, and its PDF is</p><script type="math/tex; mode=display">f(x) = \left \{  \begin{array}{ll} \frac{1}{b-a} & if\ a<x<b \\ 0 & otherwise \end{array} \right.</script><p>We denote this by $U\sim Unif(a,b)$</p><p><strong>Example</strong>: suppose $X_1,X_2,…,X_n$ are i.i.d $Unif(0,1)$ random variable and let $Y = min(X_1,X_2,…,X_n)$ be their minimum. Find $E(Y)$</p><p>If $0&lt;x&lt;1$, we have $P(X_i \leq x) = x$, it follows that</p><script type="math/tex; mode=display">\begin{align}  P(Y>x)&= P(min (X_1,...,X_n)>x) \\        &= P(X_1>x,...,X_n>x)\\        &= \prod_{i=1}^n P(X_i > x) = (1-x)^n\end{align}</script><p>From above, we use the survial function to calculate the expectation</p><script type="math/tex; mode=display">E(Y) = \int_0^{\infty} P(Y>x) dx = \int_0^1(1-x)^ndx = \int_0^1 x^n dx = \frac{1}{n+1}</script><hr><h1 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h1><p><strong>Definition (Standard Normal Distribution)</strong> A c.r.v. $Z$ is said to have the standard Normal Distribution if its $PDF$ $\varphi$ is given by:</p><script type="math/tex; mode=display">\varphi(z) = \frac{1}{\sqrt(2\pi)} e^{-z^2/2},-\infty < z < \infty</script><p>We write this as $Z\sim N(0,1)$, and $Z$ has mean 0 and variance 1, the CDF $\phi$ is</p><script type="math/tex; mode=display">\phi(z) = \int_{-\infty}^z \varphi(t) dt = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt</script><p><strong>Definition (Normal Distribution)</strong> If $Z\sim N(0,1)$ then</p><script type="math/tex; mode=display">X = \mu + \sigma Z</script><p>is said to have the Normal Distribution with mean $\mu$ and variance $\sigma^2$, denote this by $X\sim N(\mu,\sigma^2)$</p><p><strong>Theorem (Normal CDF and PDF)</strong> Let $X\sim N(\mu, \sigma^2)$,<br>CDF of $X$ is</p><script type="math/tex; mode=display">F(x) = \phi(\frac{x-\mu}{\sigma})</script><p>PDF of $X$ is</p><script type="math/tex; mode=display">f(x) = \varphi (\frac{x-\mu}{\sigma})\frac{1}{\sigma}</script><hr><h1 id="Exponential"><a href="#Exponential" class="headerlink" title="Exponential"></a>Exponential</h1><p><strong>Definition (Exponential Distribution)</strong></p><script type="math/tex; mode=display">f(x) = \lambda e ^{-\lambda x}, x>0</script><p>we denote this by $X\sim Expo(\lambda)$. The corresponding CDF is</p><script type="math/tex; mode=display">F(x) = 1-e^{-\lambda x},x>0</script><p><strong>Theorem (Memoryless Property)</strong></p><script type="math/tex; mode=display">P(X\geq s+t | X\geq s) = P(X\geq t)</script><ul><li>If $X$ is a positive continuous random variable with memoryless property, then $X$ has an Exponential distribution</li><li>Geometric Distribution is also Memoryless</li><li>Exponential distribution as the “continuous counterpart” of the Geometric distribution</li></ul><p><strong>Exponential \&amp; Geometric via $\delta$- Steps</strong><br>We devide a unit of time into n pieces, each of size $\delta = \frac{1}{n}$, and the trial occurs every $\delta$ time period and success with probability $\lambda \delta$. Denote $Y$ as the number of trials until first success, $\hat{Y}$ as the time until first success under $Y$.</p><script type="math/tex; mode=display">Y\sim FS(\lambda \delta)</script><p>Thus we have</p><script type="math/tex; mode=display">F(\hat{Y}) = E(Y)\cdot \delta = \frac{1}{\lambda\delta}\cdot \delta=\frac{1}{\lambda}</script><p>And</p><script type="math/tex; mode=display">\begin{array}P(Y>t) &= P\{ all\ trials\ up\ to\ time\ t\ has\ been\ failures \} \\&=P\{ at\ least\ \frac{t}{\delta}\ failures\}\\&=(1-p)^{\frac{t}{\delta}}=(1-\lambda \delta)^{\frac{t}{\delta}}\\&=\left[  (1-\lambda\delta)^{\frac{1}{\lambda\delta}}  \right]^{\lambda t} \xrightarrow{\delta \rightarrow 0} e^{-\lambda t}\end{array}</script><p><strong>Theorem property of Exponential</strong> Given $X_1\sim Expo(\lambda_1)$, $X_2\sim Expo(\lambda_2)$, $X_1 \bot X_2$ ($X_1$ and $X_2$ are independent), then</p><script type="math/tex; mode=display">P(X_1<X_2) = \frac{\lambda_1}{\lambda_1 + \lambda_2}</script><p>It can be solved by $\delta$-step</p><p>Some physical phenomenon follow the Exponential distribution like the radioactive decay.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Probability-Density-Function&quot;&gt;&lt;a href=&quot;#Probability-Density-Function&quot; class=&quot;headerlink&quot; title=&quot;Probability Density Function&quot;&gt;&lt;/a&gt;Pr
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Generative-Adversarial-Nets</title>
    <link href="http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/"/>
    <id>http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/</id>
    <published>2018-07-25T11:21:44.000Z</published>
    <updated>2018-08-10T01:04:48.720Z</updated>
    
    <content type="html"><![CDATA[<ul><li>$x$: Data we already have</li><li>$z$: Data we want to generate</li><li>$\theta_g$: parameter for Generator</li><li>$\theta_d$: parameter for D</li><li>$G$: Generator</li><li>$D$: Discriminator</li></ul><p>target is training $G$ to minimize</p><script type="math/tex; mode=display">\mathop{min}_G \mathop{max}_D V(D,G) = E_{x\sim p_{data(x)}}[\mathop{log}D(x)] + E_{z\sim p_z(z)}[\mathop{log}(1-D(G(z)))]</script><p>For player $D$, want V bigger by</p><ul><li>making more accurate estimate on real date x as $D(x)\rightarrow 1$</li><li>discriminate the fake data by $D(G(z)) \rightarrow 0$ equals to $(1 - D(G(z))) \rightarrow 1$</li><li>more accurate the $D$ is ,the larger value of $V$ can be</li></ul><p>For player $G$, want V smaller by</p><ul><li>enlarge $D(G(z))\rightarrow 1$ for $(1 - D(G(z))) \rightarrow 0$</li><li>better fake of $G$, $G(z)\rightarrow 1$, and less value of $V$</li></ul><hr><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><blockquote><p><strong>for</strong> number of iterations <strong>do</strong></p><blockquote><p><strong>for</strong> $k$ steps <strong>do</strong></p><blockquote><ul><li>Sample $m$ noise samples $\{ z^{(1)},…,z^{(m)} \}$ from noise prior $p_g(z)$</li><li>Sample $m$ examples $\{ x^{(1)},…,x^{(m)} \}$ from data generating distribution $p_{data}(x)$</li><li>Update the discriminator by ascending its stochastic gradient :<script type="math/tex; mode=display">\nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^m \left[ \mathop{log}D(x^{(i)})+log\left(1-D(G(z^{(i)}))\right) \right]</script></li></ul></blockquote><p><strong>end for</strong></p><ul><li>Sample $m$ noise samples $\{ z^{(1)},…,z^{(m)} \}$ from noise prior $p_g(z)$</li><li>Update the generator by descending its stochastic gradient :<script type="math/tex; mode=display">\nabla_{\theta_g} \frac{1}{m}\sum_{i=1}^m\mathop{log}\left( 1-D(G(z^{(i)})) \right)</script></li></ul></blockquote><p><strong>end for</strong></p></blockquote><pre><code>  ew  - q  - q</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;$x$: Data we already have&lt;/li&gt;
&lt;li&gt;$z$: Data we want to generate&lt;/li&gt;
&lt;li&gt;$\theta_g$: parameter for Generator&lt;/li&gt;
&lt;li&gt;$\theta_d$: 
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Process (Expectation)</title>
    <link href="http://yoursite.com/2018/07/22/Stochastic-Process-3/"/>
    <id>http://yoursite.com/2018/07/22/Stochastic-Process-3/</id>
    <published>2018-07-22T02:30:26.000Z</published>
    <updated>2018-08-14T02:44:03.525Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h1><p><strong>Definition (Expectation of R.V.)</strong></p><script type="math/tex; mode=display">E(X) = \sum_{x}\underbrace{x}_{value} \underbrace{P(X=x)}_{PMF\ at\ x}</script><p><strong>Theorem (Monotonicity)</strong>: $X$ and $Y$ are r.v.s. such that $X&gt;Y$ with probability 1.<br>Then $E(X)\geq E(Y)$</p><p><strong>Theorem (Expectation via Survial Function)</strong>: Let $X$ be a nonnegative r.v. Let $F$ be the CDF of $X$, and define survial function of $X$ named $G$ as $G(x) = 1-F(x) = P(X&gt;x)$, Then</p><script type="math/tex; mode=display">E(x) = \sum_{n=0}^{\infty} G(x)</script><p><strong>Theorem (Low Of The Unconscious Statistician(LOTUS))</strong>: If $X$ is discrete r.v. and $g$ is a function from $R$ to $R$, then</p><script type="math/tex; mode=display">E(g(x)) = \sum_x g(x)P(X=x)</script><h3 id="Propertise-of-Expectation"><a href="#Propertise-of-Expectation" class="headerlink" title="Propertise of Expectation"></a>Propertise of Expectation</h3><ul><li>$E(X+Y) = E(X) + E(Y)$</li><li>$E(cX) = c E(x)$</li><li>If $X$ and $Y$ are independent, $E(XY) = E(X) E(Y)$</li></ul><h3 id="Inequalities-of-Expectation"><a href="#Inequalities-of-Expectation" class="headerlink" title="Inequalities of Expectation"></a>Inequalities of Expectation</h3><ul><li>Cauchy–Bunyakovsky–Schwarz inequality<script type="math/tex; mode=display">E[XY]^2\leq E[X^2] E[Y^2]</script></li><li><a href="https://en.wikipedia.org/wiki/Expected_value#Inequalities" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Expected_value#Inequalities</a> …</li></ul><hr><h1 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h1><p><strong>Definition (Variance and Standard Deviation)</strong> variance of an r.v. $X$ is</p><script type="math/tex; mode=display">Var(X) = E(X-EX)^2</script><p>Square root of the variance is standard deviation (SD):</p><script type="math/tex; mode=display">SD(X)  = \sqrt{Var(X)}</script><h3 id="Propertise-of-Variance"><a href="#Propertise-of-Variance" class="headerlink" title="Propertise of Variance"></a>Propertise of Variance</h3><ul><li>For any r.v. $X$, $Var(X) = E(X^2) - (EX)^2$</li><li>$Var(X + c ) = Var(X)$</li><li>$Var(c X ) = c^2Var(X)$</li><li>If $X$ and $Y$ are independent, then $Var(X+Y) = Var(X) + Var(Y)$</li></ul><hr><h1 id="Geometric-and-Negative-Binomial"><a href="#Geometric-and-Negative-Binomial" class="headerlink" title="Geometric and Negative Binomial"></a>Geometric and Negative Binomial</h1><p><strong>Definition (Geometric Distribution)</strong>: Consider a sequence of independent Bernoulli trials, each with the same success probability $p\in (0,1)$, trails performed until a success occurs. Let $X$ be the number of the failures before the first successful trail. Then $X$ has the Geometric Distributions, denote by $X\sim Geom(p)$</p><p><strong>Theorem (Geometric PMF)</strong>: If $X\sim Geom(p)$, then the PMF of $X$ is</p><script type="math/tex; mode=display">P(X=k) = (1-p)^kp</script><p><strong>Theorem (Memoryless Property)</strong>: If $X\sim Geom(p)$, then for positive integer n</p><script type="math/tex; mode=display">P(X\geq n+k | X \geq k ) = P(X\geq n)</script><p><strong>Definition (First Success Distribution)</strong>: very similay to Geometric $X$, Let it be $Y$, and $X+1 = Y$ …. , we denote it by $FS(p)$</p><p><strong>Definition (Negative Binomial Distribution)</strong>: In a sequence of independent Bernoulli trails with p, if $X$ is the number of failures before $r^{th}$ success, then $X$ is the Negative Binomial Distribution with $r$ and $p$, denoted by $X\sim NBin(r, p)$</p><p><strong>Theorem (Negative Binomial PMF)</strong>: If $X\sim NBin(r,p)$, then the PMF of $X$ is</p><script type="math/tex; mode=display">P(X=n) = \left ( \begin{array}{c} n+r-1 \\ r-1 \end{array}   \right )p^r(1-p)^n</script><p><strong>Theorem (Geometric &amp; Negative Binomial)</strong>: Let $X\sim NBin(r,p)$, and $X_i$ are $i.i.d. Geom(p)$ , Then we have $X= X_1+\dotsb + X_r$</p><hr><h1 id="Indicator-R-V"><a href="#Indicator-R-V" class="headerlink" title="Indicator R.V."></a>Indicator R.V.</h1><p><strong>Definition (Indicator R.V.)</strong></p><script type="math/tex; mode=display">I_A =\left \{  \begin{array}{ll}    1 & if\ A\ occurs \\    0 & otherwise  \end{array}\right .</script><h3 id="Propertise-of-Indicator-R-V"><a href="#Propertise-of-Indicator-R-V" class="headerlink" title="Propertise of Indicator R.V."></a>Propertise of Indicator R.V.</h3><ul><li>$(I_A)^k = I_A$</li><li>$I_{A^c} = 1- I_A$</li><li>$I_{A\cap B} = I_A I_B$</li><li>$I_{A\cup B} = I_A + I_B - I_A I_B$</li></ul><p><strong>Theorem (Bridge between Probability &amp; Expectation)</strong></p><script type="math/tex; mode=display">P(A) = E(I_A)</script><p><strong>Example 1</strong>: Au urn contain R G B three balls, r g b is probability draw a ball from it (r+g+b = 1), whats the expected number of different <strong><em>colors</em></strong> of ball before getting the first R ball ?<br>Let $I_g$ be the $1$ if G is obtained before R, and define the $I_b$ similarly. Then</p><script type="math/tex; mode=display">E(I_g) = P(green\ before\ red) = \frac{g}{g+r}</script><p>since “green before red” means that first non-blue ball is green , so probability is $frac{g}{g+r}$, then, the final result is</p><script type="math/tex; mode=display">E(I_g+I_b) = \frac{g}{g+r} + \frac{b}{b+r}</script><h3 id="Moments-amp-Indicators"><a href="#Moments-amp-Indicators" class="headerlink" title="Moments &amp; Indicators"></a>Moments &amp; Indicators</h3><p>Given n events $A_1,\dotsb, A_n$ and indicators $I_j, j = 1, \dotsb, n$</p><ul><li>$X = \sum_{j=1}^n I_j$: the number of events occur</li><li><script type="math/tex">\left( \begin{array}{c} X \\ 2 \end{array} \right) = \sum_{i<j}I_iI_j</script>: the number of pairs of events that occur</li><li><script type="math/tex">E( \left( \begin{array}{c} X \\ 2 \end{array} \right) ) = \sum_{i<j} P(A_iA_j)</script> .<ul><li>$E(X^2) = 2\sum_{i&lt;j} P(A_iA_j) + E(X)$</li><li>$Var(X) = 2\sum_{i&lt;j} P(A_iA_j) + E(X) - (E(X))^2$</li></ul></li></ul><hr><h1 id="Poisson"><a href="#Poisson" class="headerlink" title="Poisson"></a>Poisson</h1><p><strong>Definition (Poisson Distribution)</strong> $X\sim Pois(\lambda)$</p><script type="math/tex; mode=display">P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!},k=0,1,2,\dotsb</script><h3 id="Property-of-poisson"><a href="#Property-of-poisson" class="headerlink" title="Property of poisson"></a>Property of poisson</h3><ul><li>$E(X) = \lambda$</li><li>$E(X^2)  = \lambda(1+\lambda)$</li><li>$Var(X) = \lambda$</li></ul><p><strong>Poisson Approximation</strong>: Let $A_1,A_2,\dotsb,A_n$ be events with $p_j = P(A_j)$, here $n$ is larger where $p_j$ is small , and $A_j$ is independent or weakly dependent, let <script type="math/tex">X = \sum_{j=1}^n I(A_j)</script> count how many $A_j$ occur. Then $X$ is approximately $Pois(\lambda)$, with $\lambda = \sum_{j=1}^n p_j$</p><p><strong>Theorem（Sum of Independent Poisson)</strong>: If $X\sim Pois(\lambda_1), Y\sim Pois(\lambda_2)$, and $X$ is independent of $Y$, then $X+Y \sim Pois(\lambda_1 + \lambda_2)$</p><p><strong>Theorem（Poisson Approximation to Binomial)</strong>: If $X\sim Bin(n,p)$ and we let $n \rightarrow \infty and p\rightarrow 0$ , such that $\lambda = np$, then the PMF of $X$ converges to the $Pois(\lambda)$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Expectation&quot;&gt;&lt;a href=&quot;#Expectation&quot; class=&quot;headerlink&quot; title=&quot;Expectation&quot;&gt;&lt;/a&gt;Expectation&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Definition (Expectati
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
</feed>
