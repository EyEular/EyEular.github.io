<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EyEular</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-05T13:21:07.743Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Eulring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>R-CNN</title>
    <link href="http://yoursite.com/2018/10/05/RCNN/"/>
    <id>http://yoursite.com/2018/10/05/RCNN/</id>
    <published>2018-10-05T02:21:58.000Z</published>
    <updated>2018-10-05T13:21:07.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p>R-CNN 流程</p><ul><li>对输入图片使用 select search 选择 2000 个区域<ul><li>select search 有区域合并的操作</li></ul></li><li>拉伸缩放图片到固定尺寸（227 * 227）传入 CNN 生成 4096 维的特征向量</li><li>对于特征用 SVM 预测每个类别（C+1，1是背景），通过 IoU 来打分</li><li>预测最大的判别为一类</li><li>最后对这些区域根据极大值抑制来合并和省略</li></ul><p><img src="/2018/10/05/RCNN/rcnnimg2.jpg" align="justify"></p><p>对于每一个类，都训练一个 SVM 分类器，优化的核心是建立一个 loss 方程，对于 SVM 的 loss function 我们要有 predict 的正负样本，以及 groundtruth 的正负样本</p><ul><li>predict：对于每一个区域的对应的类 SVM 直接返回 0/1（负样本或者正样本）</li><li>groundtruth：把所有的框和真实手工标注的框做一个 IoU 的计算，然后根据 IoU 的阈值来生成 data 的负样本与正样本，然后还要进行一次极大值抑制，尽管有些框的 IoU 大于阈值了，但是依旧判为负样本，因为它周围已经有比他更大的了。。</li></ul><p>我们还可以对位置进行一个修正（用修正后的位置来计算 IoU）：输入为深度网络pool5层的4096维特征，输出为 xy 方向的缩放和平移。 训练样本判定为本类的候选框中，和真值重叠面积大于 0.6 的候选框，loss 为真实框和预测框的二范数。这个预测应该是在一次识别操作之后进行的。。</p><p><img src="/2018/10/05/RCNN/rcnnimg1.jpg" align="justify"></p><hr><h1 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a>SPPnet</h1><p><img src="/2018/10/05/RCNN/rcnnimg3.jpg" align="justify"></p><p>R-CNN 对于每一个 region 图片都进行了卷积，这样效率很低，一个直观的 idea 就是直接做一次卷积生成一个 feature map，然后在这个 feature map 中找到对应的 region，但是这样有一个问题，就是最后我们的全连接层是固定尺寸的，在 R-CNN 中，我们通过 warp 区域来使输入一致，在 SPPnet 中，我们在 feature map 上面，是否也可以 warp 呢 ？答案是不可以的。。。因为在图像进行 warp，伸展以后图像的意义仍然保留了，但是特征图 warp 以后，不经过修正，就失去意义了。。。所以我们要对 feature 重新提取特征。</p><p><img src="/2018/10/05/RCNN/rcnnimg4.jpg" width="500px"></p><p>SPP 全名是 Spatial Pyramid Pooling，借用了图像金字塔的概念，如上图所示，我们将图像分成 $4\times 4$<br>, $2\times 2$, $1\times 1$ 的区域，然后做这些区域的 max-pooling，假设经过卷积我们得到的 feature map 是 $n\times m\times 256$ 的，经过 SPP 以后，就变成了 $21\times 256$ 的了</p><p>SPP 还有要解决的点就是根据 region 在图像中的位置找到，对应在 feature map 中的位置。。<br>计算公式是这样的：$(x, y) = (Sx^{\prime}, Sy^{\prime})$，其中 S 是所有步长的乘积，实际中的公式为：</p><script type="math/tex; mode=display">x^{\prime} = \lfloor x/S \rfloor + 1</script><p><img src="/2018/10/05/RCNN/rcnnimg5.jpg" align="justify"></p><p>论文中用到的 ZF-5 模型，它的 S 就是 $2^4$ 也就是里面的四个 str 2</p><hr><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><p>R-CNN 缓慢的原因是因为，对于每一张图片都进行来深度网络的卷积<br>SPPnet 解决 R-CNN 的这个问题，但是 SPPnet 还是有缺陷的，就是和 R-CNN 一样仍然是 multi-stage 的框架</p><p>Fast R-CNN 的优点是：</p><ul><li>更高的探测精度</li><li>在一个 stage 上通过不同任务的 loss 完成训练</li><li>训练可以更新网络的每一层</li><li>feature 的存储不需要放在硬盘上</li></ul><p>-</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zhuanlan.zhihu.com/p/23006190" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23006190</a><br><a href="https://blog.csdn.net/xjz18298268521/article/details/52681966" target="_blank" rel="noopener">https://blog.csdn.net/xjz18298268521/article/details/52681966</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;R-CNN&quot;&gt;&lt;a href=&quot;#R-CNN&quot; class=&quot;headerlink&quot; title=&quot;R-CNN&quot;&gt;&lt;/a&gt;R-CNN&lt;/h1&gt;&lt;p&gt;R-CNN 流程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对输入图片使用 select search 选择 2000 个区域&lt;ul
      
    
    </summary>
    
    
      <category term="DL" scheme="http://yoursite.com/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>RL-1</title>
    <link href="http://yoursite.com/2018/10/04/RL-1/"/>
    <id>http://yoursite.com/2018/10/04/RL-1/</id>
    <published>2018-10-04T08:51:19.000Z</published>
    <updated>2018-10-05T15:18:44.104Z</updated>
    
    <content type="html"><![CDATA[<p>RL 和传统的监督学习和无监督学习是有区别的，RL 没有 label 只有 reward</p><p>数据是有时序性的，也就是数据之间并不是独立的</p><p>数据只会对它后面的数据造成影响</p><blockquote><p>比如一个棋局只会对后面的落子有影响，当前位置只会对后面的位置移动产生作用</p></blockquote><h3 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h3><p>一个 Reward  $R_t$ 就是一个常数的信号，用来对状态的衡量，或者 agent 在时间 $t$ 做的好坏程度，agent 的目标就是最大化 reward，RL 的基础就是建立在最大化 reward 的假设</p><p>下面是一些 reward 在特定任务下的例子：</p><p><img src="/2018/10/04/RL-1/rlimg1.jpg" width="600"></p><h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><p><img src="/2018/10/04/RL-1/rl1img2.jpg" align="justify"></p><ul><li>History：可以由上面介绍的组成一个序列 $H_t= O_1,R_1,A_1,…,A_{t-1},O_t,R_t$</li><li>State： 其实就是信息，决定着接下来会发生的信息是什么，State 只和 history 有关 $S_t = f(H_t)$<ul><li>Environment State：隐藏在能给出反馈的环境中，一般是看不到的，看到也没用。。</li><li>Agent State：决策者的状态 $S_t^a = f(H_t)$</li><li>Information State：只包含历史中有用信息，例如假设了 Markov 性，inf state 只有之前的状态</li></ul></li></ul><p>Fully Observation Environments：$O_t = S_t^a = S_t^e$</p><p>Partially Observation Environments：间接的观察环境</p><h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>RL 的 agent 一般有如下几个组件构成</p><ul><li>Policy：agent 的行为函数</li><li>Value function：评价 state 或者 action 的好坏</li><li>Model：agent 对于 Environment 的表示</li></ul><p><img src="/2018/10/04/RL-1/rl1img4.jpg" width="600"></p><h5 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h5><p>policy 是 state 到 action 的映射，是 agent 的行为</p><p>Deterministic Policy：$a = \pi(s)$</p><p>Stochastic policy：$\pi(a|s) = P[A_t = a | S_t = s]$</p><p><img src="/2018/10/04/RL-1/rl1img3.jpg" width="400"></p><h5 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h5><p>对于未来的预测</p><script type="math/tex; mode=display">v_{pi}(s) = E_{\pi}[R_{t+1} + \gamma R_{t+1} + \gamma^2 R_{t+3}+...|S_t=s]</script><p><img src="/2018/10/04/RL-1/rl1img5.jpg" width="400"></p><h5 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h5><p>model 是用来预测 environment 接下来会做什么</p><p>$\mathcal{P}$ 用来预测接下来的的 state</p><p>$\mathcal{R}$ 用来预测接下来的 reward</p><script type="math/tex; mode=display">\mathcal{P}_{ss^{\prime}} = P[S_{t+1} = S^{\prime}|S_t = s, A_t = a]</script><script type="math/tex; mode=display">\mathcal{R}_s^a  = E[R_{t+1}|S_t = s, A_t = a]</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;RL 和传统的监督学习和无监督学习是有区别的，RL 没有 label 只有 reward&lt;/p&gt;
&lt;p&gt;数据是有时序性的，也就是数据之间并不是独立的&lt;/p&gt;
&lt;p&gt;数据只会对它后面的数据造成影响&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;比如一个棋局只会对后面的落子有影响
      
    
    </summary>
    
    
      <category term="RL" scheme="http://yoursite.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>NLP-2</title>
    <link href="http://yoursite.com/2018/09/25/NLP-2/"/>
    <id>http://yoursite.com/2018/09/25/NLP-2/</id>
    <published>2018-09-25T11:26:50.000Z</published>
    <updated>2018-09-25T14:10:27.580Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h1><h3 id="Statistical-Machine-Translation"><a href="#Statistical-Machine-Translation" class="headerlink" title="Statistical Machine Translation"></a>Statistical Machine Translation</h3><p>早期的机器翻译的方法是 SMT，S 代表 statistic 表示统计的意思，然后 14 年的时候，一切都变了。。。</p><h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p>NMT 降临了，吊打了 SMT，所以 SMT 我就不学啦，哈哈哈</p><hr><h1 id="Seq-to-Seq-model"><a href="#Seq-to-Seq-model" class="headerlink" title="Seq-to-Seq model"></a>Seq-to-Seq model</h1><p><img src="/2018/09/25/NLP-2/nlp2img1.jpg" align="justify"></p><blockquote><ul><li>上图称为 sequence-to-sequence 模型，有两个 RNN</li><li>待翻译的句子为输入 $x$，翻译的结果称为 $y$</li><li>第一个 RNN 用来将 $x$ 做一个 encoding，类似于将输入生成一个特征</li><li>生成这个特征的结果放到下一个 RNN 中做 Decodeing</li><li>Decoding 的过程相当于做一连串的单词预测</li></ul></blockquote><p><img src="/2018/09/25/NLP-2/nlp2img2.jpg" align="justify"></p><blockquote><ul><li>这是 seq2seq 模型的训练过程</li></ul></blockquote><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><p><img src="/2018/09/25/NLP-2/nlp2img3.jpg" align="justify"></p><p>对于每一个单词后面一个单词的预测，最简单的方法就是上面的贪心法 (greedy decoding), 每一步直接输出概率最高的，但是其实这并不是一定我们期望的最优解，甚至离最优解的很远，一组输出的概率表示形式如下：</p><script type="math/tex; mode=display">P(y|x) = P(y_1|x) P(y_2|y_1,x) P(y_3|y_2,y_1,x),...,P(y_T|y_{T-1},...,y_1,x)</script><p>我们希望输出的值概率尽可能的大，要想获得最大值，最粗暴有效的方法就是遍历上面的所有取值，但是上面的所有取值的复杂度是 $O(V^T)$，于是我们可以做一个在 greedy 和 optimal 之间做一个折衷，这个方法就叫 Beam Search。<br></p><p><strong>Beam search: On each step of decoder, keep track of the k most probable partial translations</strong></p><p>举个 $k=2$ 的例子</p><p><img src="/2018/09/25/NLP-2/nlp2img4.jpg" align="justify"></p><blockquote><p>每个单词我们生成它最大概率的 2 个，然后每次选取概率最大的两个分支继续进行生成</p></blockquote><p><img src="/2018/09/25/NLP-2/nlp2img5.jpg" align="justify"></p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Machine-Translation&quot;&gt;&lt;a href=&quot;#Machine-Translation&quot; class=&quot;headerlink&quot; title=&quot;Machine Translation&quot;&gt;&lt;/a&gt;Machine Translation&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PGM-CRF</title>
    <link href="http://yoursite.com/2018/09/22/PGM-CRF/"/>
    <id>http://yoursite.com/2018/09/22/PGM-CRF/</id>
    <published>2018-09-22T02:25:18.000Z</published>
    <updated>2018-10-04T08:50:25.664Z</updated>
    
    <content type="html"><![CDATA[<p>CRF 可以看作是 logistics 回归的一个扩展</p><h3 id="Notions"><a href="#Notions" class="headerlink" title="Notions"></a>Notions</h3><ul><li>$\mathbf{X}$: 我们观测到的输入变量</li><li>$\mathbf{Y}$: 我们待预测的输出变量</li><li>$\sum_{\mathbf{y} \setminus y_s}$: 在$y_s$ 给定的情况下，y 中所有其他变量的可能的取值的遍历</li></ul><h3 id="G-amp-D"><a href="#G-amp-D" class="headerlink" title="G &amp; D"></a>G &amp; D</h3><h5 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h5><p>Classification 是分类问题，通过给定的特征向量 $\mathbf{x}$ 我们来估计隐藏在这些数据后面的类别 $y$ 是什么，其中一个简单的方法是假设特征向量里面的变量都是相互独立的，这称为 <strong>_naive Bayes classifier_</strong> 这是基于 x 的联合分布：</p><script type="math/tex; mode=display">p(y|\mathbf{x}) = p(y) \prod_{k=1}^K p(x_k | y)</script><p>上面的公式也是可以写成因子图的表示形式的</p><p>另一个比较常见的模型就是 logistic regression：</p><script type="math/tex; mode=display">p(y|\mathbf{x}) = \frac{1}{Z(\mathbf{x})}  exp \left{   \sum_{k=1}^K \theta_k f_k (y, \mathbf{x})   \right}</script><h5 id="Sequence-Models"><a href="#Sequence-Models" class="headerlink" title="Sequence Models"></a>Sequence Models</h5><p>Classifier 只能对于单变量做预测，我们希望能对多变量做预测，为了引出这个模型，我们来讨论一个 NLP 的应用，称为 NER，NER 是用来定义一个词的类别的，比如，china，它的 NER 就是 location，更加具体一些，给定一个句子，我们确定哪些词是组合在一起的，同时对于这些词做一个区分。</p><p>一个直观的 NER 方法是对于单词做独立的区分，这种方法每个单词和它周围的单词是独立的，比如 new york 是一个地名，但是 new york times 它就是一个报纸了。。。一个方法就是把这些输出变量都串起来，形成一个链式的模型，称为 HMM，HMM 有两个独立性假设</p><ul><li>每一个 state $y_t$ 只和前一个 state $y_{t-2}$ 相关</li><li>观测 $x_t$ 由 $y_t$ 导出</li></ul><p>于是，state y 和 观测 x 的联合分布可以写成</p><script type="math/tex; mode=display">p(\mathbf{y},\mathbf{x}) = p(y_1) \prod_{t=2}^ T p(y_t|y_{t-1}) p(x_t|y_t)</script><h5 id="Comparision"><a href="#Comparision" class="headerlink" title="Comparision"></a>Comparision</h5><p>Generative Model</p><ul><li>Naive Bayes</li><li>HMM</li></ul><p>Discriminative Model</p><ul><li>logistic regression</li></ul><p>我们还回忆一下生成模型和判别模型的区别，生成模型中概率的一部分是分给 x 的，所以生成模型最后的联合分布的概率通常很小，就比如 naive bayes，对于判别模型，概率是 0-1 之间的条件，这就符合了逻辑回归呀，在条件概率中，x 只对 y 的概率有影响，但是完全不出现在概率中。之所以不对 $p(x)$ 建模，是因为输入的特征变量之间的相关性太高</p><p><img src="/2018/09/22/PGM-CRF/pgmcrf_img1.jpg" align="justify"></p><h3 id="Linear-Chain-CRFs"><a href="#Linear-Chain-CRFs" class="headerlink" title="Linear Chain CRFs"></a>Linear Chain CRFs</h3><p>我们重写 HMM 模型到一个更加 general 的 case：</p><script type="math/tex; mode=display">p(\mathbf{y}, \mathbf{x}) = \frac{1}{Z}   \prod_{t=1}^T  exp \left \{  \sum_{i,j \in  S} 1_{\{y_t=i\}} 1_{\{y_{t-1}=j\}} + \sum_{i \in S} \sum_{o \in O} \mu_{oi} 1_{\{y_t = i\}} 1_{\{x_t = o\}} \right \}</script><p>当 $\theta_{ij} = log p(y^{\prime} = i | y = j)$ 时，$\mu_{oi} = log p(x=o|y=i)$ 时，就是 HMM 了，写成更一般的简洁一些的形式，即把指数的参数直接写成 feature function 为 $f_k(y_t,y_{t-1},x_t)$ 的形式：</p><script type="math/tex; mode=display">p(\mathbf{y}, \mathbf{x}) = \frac{1}{Z} \prod_{t=1}^T exp \left\{  \sum_{k=1}^K \theta_k f_k (y_t, y_{t-1},x_t)  \right\}</script><p>上面是生成模型，根据 bayes 定理我们把它写成条件概率的判别模型：</p><script type="math/tex; mode=display">p(\mathbf{y}| \mathbf{x}) = \frac{p(\mathbf{y}, \mathbf{x})}{\sum_{\mathbf{y^{\prime}}}} = \frac{\prod_{t=1}^T exp \left\{  \sum_{k=1}^K \theta_k f_k(y_t,y_{t-1},x_t) \right\}}{\sum_{\mathbf{y^{\prime}}} \prod_{t=1}^T  exp \left \{ \sum_{k=1}^K \theta_k f_k(y_t^{\prime}, y_{t-1}^{\prime},x_t)  \right \} }</script><p>上面的条件概率就是 linear-chain CRF，</p><p><strong>Definition：linear-chain CRF</strong></p><script type="math/tex; mode=display">p(\mathbf{y}| \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \prod_{t=1}^T exp \left\{  \sum_{k=1}^K \theta_k f_k(y_t,y_{t-1},\mathbf{x}_t)  \right\}</script><blockquote><ul><li>$Y,X$ 是随机变量</li><li>$\theta$ 是参数向量</li><li>$f_k(y,y^{\prime},x_t)$ 是 feature function</li></ul></blockquote><p>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CRF 可以看作是 logistics 回归的一个扩展&lt;/p&gt;
&lt;h3 id=&quot;Notions&quot;&gt;&lt;a href=&quot;#Notions&quot; class=&quot;headerlink&quot; title=&quot;Notions&quot;&gt;&lt;/a&gt;Notions&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;$\mathbf{
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Information-Theory</title>
    <link href="http://yoursite.com/2018/09/18/Information-Theory/"/>
    <id>http://yoursite.com/2018/09/18/Information-Theory/</id>
    <published>2018-09-18T02:06:08.000Z</published>
    <updated>2018-09-25T14:12:53.437Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>信息熵</strong>：编码方案完美时，最短平均编码长度</li><li><strong>交叉熵</strong>：编码方案不一定完美时，平均编码长度</li><li><strong>相对熵</strong>：不同方案之间的差异性的衡量</li></ul><p>信息论引用了熵（entropy）的概念，熵的概念在物理中十分常见，是用来衡量系统的混沌程度的，相应的在信息科学中，熵是用来衡量不确定性的，信息熵的本质是香农信息量 $log \frac{1}{p}$ 的期望</p><p>下面我们就来讲讲这三个熵到底有什么含义</p><p>首先我们通过一个例子来引出什么是信息熵：<br>假设一个箱子里面有4种不同颜色的球（a,b,c,d），A 从其中拿出一个，B 来猜（B 可以向 A 提问），B希望提问的次数越少越好。</p><ul><li><strong>case1</strong>：B 不知道颜色的分布，于是先问 “是否是 a,b”，如果是再问 “是否是 a”，不是改为问 “是否是 c”，猜球的次数为：$H=\frac{1}{4}\times 2+\frac{1}{4}\times 2+\frac{1}{4}\times 2+\frac{1}{4}\times 2=2$</li><li><strong>case2</strong>：B 了解到 a 的比例为 1/2，b 的比例为 1/4，c 和 d 的比例都是 1/8，于是 B 先问 a，再问 b，最后问 c，猜球期望为：$H=\frac{1}{2}\times 1+\frac{1}{4}\times 2+\frac{1}{8}\times 3+\frac{1}{8}\times 3=1.75$</li><li><p><strong>case3</strong>：B 了解到了里面的球全是 a，那么不用猜了：$H=1\times 0=0$</p><p>_假设上面的 B 是足够聪明的，所以他做出策略是最优的_</p></li></ul><hr><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>通过上面我们发现：针对特定概率为 $p$ 的小球，猜球的次数为 $log_2  \frac{1}{p}$，所以整体的期望为：</p><script type="math/tex; mode=display">\sum_{i=1}^N p_k log_2 \frac{1}{p_k}</script><p>这就是信息熵，case1 的信息熵为 2，case2 的信息熵为 1.75，case3 的信息熵为 0.</p><p><strong>信息熵代表随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大</strong>。<br>上面的熵 case1&gt;case2&gt;case3，在 case1 中，B 对于系统一无所知，在 case2 中，B 知道了系统的分布，但是取了哪个球并不知道，case3 中，B 对于系统完全了解了！所以2是这个系统熵的最大值</p><hr><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>每个系统都会有一个真实的概率分布，称为真实分布，case1 的真实分布为 $(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$，case2 的真实分布为 $(\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8})$,而根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的 [最小努力]（猜题次数、编码长度等）的大小就是信息熵。具体来讲，case1 只需要猜两次就能确定任何一个小球的颜色，case2 只需要猜测1.75次就能确定任何一个小球的颜色</p><p>回到 case2 假设现在是 C 来猜，C 的智商不高，所以仍然使用了 case1 的策略来，相当于认为小球出现的概率是一样的，即分布为 $(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$，这个分布就是非真实分布，最后的信息熵算出来又是 2 了，显然这个策略是不好的比最优策略多了 0.25，那么，当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？</p><p>这就需要引入 <strong>交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小</strong>。</p><p>交叉熵的公式如下，<strong>其中 $p_k$  表示真实分布，$q_k$ 表示非真实分布</strong></p><script type="math/tex; mode=display">\sum_{i=1}^N p_k log_2 \frac{1}{q_k}</script><p>上面所讲将策略 1 用于 case 2，真实分布 $p_k=(\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8})$，非真实分布 $q_k=(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$，交叉熵为 $\frac{1}{2}\times log_2 4+\frac{1}{4}\times log_2 4+\frac{1}{8}\times log_2 4+\frac{1}{8}\times log_2 4=2$ 比最优分布的 $1.75$ 大</p><p>因此，交叉熵越低，策略就越好，最低的也就是真实的信息熵了，此时 $p_k=q_k$，这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布</p><hr><h1 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h1><p>我们如何衡量不同策略之间的差异呢？这里就需要用到相对熵（KL divergence），其用来衡量两个取值为正的函数或者概率分布之间的差异（可以发现相同的话每一项都是 0）：</p><script type="math/tex; mode=display">KL(P||Q) = \sum_x p(x) log \frac{p(x)}{q(x)}</script><blockquote><ul><li>$p(x)$ 是真实分布</li><li>$q(x)$ 是近似分布</li></ul></blockquote><p><strong>相对熵</strong> 可以写成 <strong>交叉熵</strong> 和 <strong>信息熵</strong> 之差。。</p><script type="math/tex; mode=display">KL(p||q) = H(p,q) - H(p) = \sum_{k=1}^N p_k log_2 \frac{p_k}{q_k}</script><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.zhihu.com/question/41252833answer/195901726" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833answer/195901726</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;信息熵&lt;/strong&gt;：编码方案完美时，最短平均编码长度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交叉熵&lt;/strong&gt;：编码方案不一定完美时，平均编码长度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;相对熵&lt;/strong&gt;：不同方案之间的差异性的衡量&lt;/
      
    
    </summary>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>MatplotLib-1</title>
    <link href="http://yoursite.com/2018/09/16/MatplotLib-1/"/>
    <id>http://yoursite.com/2018/09/16/MatplotLib-1/</id>
    <published>2018-09-16T08:14:42.000Z</published>
    <updated>2018-09-16T08:14:42.834Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP-</title>
    <link href="http://yoursite.com/2018/09/14/NLP-1/"/>
    <id>http://yoursite.com/2018/09/14/NLP-1/</id>
    <published>2018-09-14T12:01:35.000Z</published>
    <updated>2018-09-25T07:02:56.473Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h1><p>语言模型（Language Model）</p><p>语言模型是用来预测下一个出现的单词会是哪一个的，说的数学一些就是给定一个单词序列 $x^1,x^2,…,x^t$，来计算下一个单词 $x^{t+1}$ 的概率分布：</p><script type="math/tex; mode=display">P(x^{t+1}=w_j|x^1,x^2,...,x^t)</script><blockquote><p>其中 $w_j$ 是单词 j 的词向量</p></blockquote><p>我们将任务做一个简化，我们不去考虑离下一个词太远的词的影响，只考虑下一个词前面的 $n-1$ 个，这称为 <strong>n-gram Language Model</strong></p><p><img src="/2018/09/14/NLP-1/nlp1img1.jpg" style=" width:600px;"></p><p>于是概率可以写成下面的形式了：</p><script type="math/tex; mode=display">\begin{align}P(x^{t+1} | x^t,...,x^{t-n+2}) &= \frac{P(x^{t+1},x^t,...,x^{t-n+2})}{P(x^t,...,x^{t-n+2})} \notag \\&\approx \frac{count(x^{t+1},x^t,...,x^{t-n+2})}{count(x^t,...,x^{t-n+2})}\end{align}</script><blockquote><ul><li>第一行通过 Bayesian 概率转换</li><li>第二行通过统计量来近似概率</li></ul></blockquote><p>假如 n 过大，会导致表格的内存呈指数级上升，所以一般不会超过 5</p><hr><h1 id="fixed-window-Neural-Language-Model"><a href="#fixed-window-Neural-Language-Model" class="headerlink" title="fixed-window Neural Language Model"></a>fixed-window Neural Language Model</h1><p>我们先看一种基于窗口的神经网络的结构</p><p><img src="/2018/09/14/NLP-1/nlp1img2.jpg" align="justify"></p><p><strong>优点</strong></p><ul><li>不存在 n-grim 的稀疏性</li><li>模型的 size 大大减小</li></ul><p><strong>缺点</strong></p><ul><li>窗口尺寸太小</li><li>每一个 $x^i$ 只使用了 $W$ 一行的参数，并没有共享参数</li></ul><hr><h1 id="Recurrent-Neural-Networks-RNN-Language-Model"><a href="#Recurrent-Neural-Networks-RNN-Language-Model" class="headerlink" title="Recurrent Neural Networks (RNN) Language Model"></a>Recurrent Neural Networks (RNN) Language Model</h1><p>我们先来看一看常见的一种 RNN 的结构：</p><p><img src="/2018/09/14/NLP-1/nlp1img4.jpg" align="justify"></p><p>RNN 的核心 Idea 就是复用权重 $W$</p><p>下面是一个用于 Language Model 的 RNN 模型：</p><p><img src="/2018/09/14/NLP-1/nlp1img3.jpg" align="justify"></p><p><strong>RNN 的优点</strong>：</p><ul><li>由于序列之间相连的传递是共享了参数的，所以这个序列可以任意的延长</li><li>模型的大小不会随着输入的增加而增加</li><li>某一步的计算，会考虑之前几步计算的结果</li></ul><p><strong>RNN 的缺点</strong>：</p><ul><li>计算速度比较慢</li><li>很难考虑到之前好几步的信息</li><li>存在梯度消失</li></ul><p>有了模型，接下来介绍这个模型是如何进行优化的，对于这个模型有一点要说明的就是，输出 $\hat{y}^t$ 其实是在给定单词到 $x^t$ 时，下一个单词 $x^{t+1}$ 出现的概率的预测，然后交叉熵来代替目标优化函数：</p><script type="math/tex; mode=display">\begin{align}  J^t(\theta) &= CE(y^t,\hat{y}^t)=  - \sum_{j=1} ^ {|V|} y_j^t \mathop{log} \hat{y}_j^t \notag \\  J(\theta) &= \frac{1}{T} \sum_{t=1}^T J^t(\theta)\end{align}</script><blockquote><p>上面的真实 label $y^t$ 是根据 $x^{t+1}$ 生成的 one-hot vector</p></blockquote><p>这个模型的 BP 是 $\frac{\partial J^t}{\partial W_h} \sum_{i=1}^t \frac{\partial J^t}{\partial W_h}|_i$</p><p>训练所有的数据是庞大且消耗巨大的任务，我们一般会把使用 SGD 也就是每一次拿一个句子进去做梯度下降</p><p>衡量某个模型的好坏的函数是 ：</p><script type="math/tex; mode=display">PP = \prod_{t=1}^T \left(  \frac{1}{\sum_{j=1}^{|V|} y_j^t \hat{y}_j^t} \right)^{1/T}</script><blockquote><p>上面的计算结果越小越好</p></blockquote><h3 id="RNN-Gradient"><a href="#RNN-Gradient" class="headerlink" title="RNN Gradient"></a>RNN Gradient</h3><p>多维变量的链式求导法则：</p><script type="math/tex; mode=display">\frac{d}{dt} f(x(t), y(t)) = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}</script><p>RNN 的基本公式可以写成：</p><script type="math/tex; mode=display">\begin{align}  h_t &= W f(h_{t-1}) + W^{hx} x_t \notag \\  \hat{y}_t &= W^S f(h_t) \notag\end{align}</script><p><img src="/2018/09/14/NLP-1/nlp1img6.jpg" style=" width:300px;"></p><p>根据上面这张图，RNN 的梯度为：</p><script type="math/tex; mode=display">\begin{align}  \frac{\partial E}{\partial W} &= \sum_{t=1}^T \frac{\partial E_t}{\partial W} \notag \\  \frac{\partial E_t}{\partial W} &= \sum_{k=1}^t \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W} \notag \\  \frac{\partial h_t}{\partial h_k} &= \prod_{j=k+1}^t \frac{\partial h_j}{\partial h_{j-1}}\end{align}</script><blockquote><p>$\frac{\partial h_t}{\partial h_k}$ 这一部分可能造成梯度消失</p></blockquote><h3 id="解决-RNN-梯度消失的方法"><a href="#解决-RNN-梯度消失的方法" class="headerlink" title="解决 RNN 梯度消失的方法"></a>解决 RNN 梯度消失的方法</h3><p><strong>clipping trick</strong>：</p><p><img src="/2018/09/14/NLP-1/nlp1img7.jpg" align="justify"></p><p><strong>Initialization + Relus</strong>：</p><ul><li>初始矩阵 $W$ 为单位矩阵</li><li>函数 $f$ 变为 $f(z) = max(z, 0)$</li></ul><p>这些只是一些小的 trick 要从根本上解决，就需要建立新的模型</p><hr><h3 id="GRUS"><a href="#GRUS" class="headerlink" title="GRUS"></a>GRUS</h3><p><img src="/2018/09/14/NLP-1/nlp1img8.jpg" align="justify"></p><hr><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul><li>决定哪些信息我们要从 cell state 中删除</li></ul><p><img src="/2018/09/14/NLP-1/nlp1img9.jpg" align="justify"></p><p><img src="/2018/09/14/NLP-1/nlp1img10.jpg" align="justify"></p><p><img src="/2018/09/14/NLP-1/nlp1img11.jpg" align="justify"></p><blockquote><p>上面黄色的区域是有参数需要学习的 layer，粉色的是直接进行的计算操作，计算操作有相乘还有相加等等。。</p></blockquote><h5 id="LSTM-Structure"><a href="#LSTM-Structure" class="headerlink" title="LSTM Structure"></a>LSTM Structure</h5><p><img src="/2018/09/14/NLP-1/nlp1img12.jpg" align="justify"></p><blockquote><ul><li>上面这条线贯穿着整个结构的称为 cell state，传递着最主要的信息</li><li>LSTM 有能力控制这个信息传输的增和删</li></ul></blockquote><p><img src="/2018/09/14/NLP-1/nlp1img13.jpg" align="justify"></p><blockquote><ul><li>确定了哪些信息我们要从 cell state 中删去</li><li>黄色的是 sigmoid 函数，输出在 0-1 之间</li><li>状态 1 表示完全保留 cell state $C_{i-1}$</li><li>状态 0 表示完全抛弃 cell state $C_{i-1}$</li></ul></blockquote><p><img src="/2018/09/14/NLP-1/nlp1img14.jpg" align="justify"></p><blockquote><ul><li>这一步决定了我们要保存哪些数据</li><li>sigmoid 函数决定哪些数据我们要更新</li><li>tanh 层生成了一个新的向量 $\hat{C}_t$ 待用于生成新的 $C_t$</li></ul></blockquote><p><img src="/2018/09/14/NLP-1/nlp1img15.jpg" align="justify"></p><blockquote><ul><li>将删选后的信息 $f_t * C_{t-1}$ 和新生成的信息 $\hat{C}_t$ 做一个相加</li></ul></blockquote><p><img src="/2018/09/14/NLP-1/nlp1img16.jpg" align="justify"></p><blockquote><ul><li>最后我们决定信息的输出</li><li>上面 sigmoid 节点的输出 $o_t$ 和输入 $x_t$ 以及之前输出 $h_{t-1}$ 有关</li><li>最终的输出 $h_t$ 和 $o_t$ 以及 cell state $C_t$ 相关</li><li>$h_t$ 有两个去处，一个是直接输出，还有一个是做为 $h_{t+1}$ 的生成信息 </li></ul></blockquote><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>  <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Language-Model&quot;&gt;&lt;a href=&quot;#Language-Model&quot; class=&quot;headerlink&quot; title=&quot;Language Model&quot;&gt;&lt;/a&gt;Language Model&lt;/h1&gt;&lt;p&gt;语言模型（Language Model）&lt;/
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PGM-Approximate Inference</title>
    <link href="http://yoursite.com/2018/09/11/PGM-4/"/>
    <id>http://yoursite.com/2018/09/11/PGM-4/</id>
    <published>2018-09-11T00:50:05.000Z</published>
    <updated>2018-09-25T12:36:13.418Z</updated>
    
    <content type="html"><![CDATA[<p>概率的推断就是计算 conditional 和 marginal，之前我们学习了 exact inference 也就是准确的推断概率图概率，我们学习message passing 算法、sum-product、</p><p>inference is answer a query</p><p>Approximate inference 就是对于 inference 的一个数值估计，不一定最后的结果要在 0-1 之内</p><h1 id="Exact-Inference-Revisit"><a href="#Exact-Inference-Revisit" class="headerlink" title="Exact Inference Revisit"></a>Exact Inference Revisit</h1><h3 id="Sum-Product"><a href="#Sum-Product" class="headerlink" title="Sum-Product"></a>Sum-Product</h3><p><img src="/2018/09/11/PGM-4/pgm4img3.jpg" align="justify"></p><p><img src="/2018/09/11/PGM-4/pgm4img1.jpg" align="justify"></p><h3 id="Factor-Graph"><a href="#Factor-Graph" class="headerlink" title="Factor Graph"></a>Factor Graph</h3><p><img src="/2018/09/11/PGM-4/pgm4img4.jpg" align="justify"></p><h3 id="Junction-Tree"><a href="#Junction-Tree" class="headerlink" title="Junction Tree"></a>Junction Tree</h3><p><img src="/2018/09/11/PGM-4/pgm4img2.jpg" align="justify"></p><blockquote><p>在 Junction Tree 中 local Consistency 等价于 global Consistency</p></blockquote><hr><h1 id="Loopy-Belief-Propagation"><a href="#Loopy-Belief-Propagation" class="headerlink" title="Loopy Belief Propagation"></a>Loopy Belief Propagation</h1><p>Junction Tree 虽然可以处理所有的 graph，但是只适用于树形结构，在密集结构中用 Junction Tree 复杂度依然会比较高，假设树变成了 gird，我们不打算用 Junction Tree 算法来计算出这个图的 inference 的精确解。<br><br><img src="/2018/09/11/PGM-4/pgm4img5.jpg" align="justify"></p><h3 id="LBP-The-Algorithm"><a href="#LBP-The-Algorithm" class="headerlink" title="LBP : The Algorithm"></a>LBP : The Algorithm</h3><p>我们在这个图上运行直接做 Belief Propagation，在树形结构上，Belief Propagation 只要来回传递两次就能够得到精确解了，但是在这个图上，我们需要多次的运行 BP ，最终可能会收敛，也有可能会呈现出周期性的数值变化，也就是不收敛。</p><p>一般来讲好的 近似可以通过以下的方式</p><ul><li>在固定的迭代次数后停止</li><li>如果结果没有明显变化，就停止</li><li>如果在数值上没有震荡，并且收敛了，那么通常就是接近真实了</li></ul><p><img src="/2018/09/11/PGM-4/pgm4img6.jpg" align="justify"></p><h3 id="LBP-The-Bethe-Approximation"><a href="#LBP-The-Bethe-Approximation" class="headerlink" title="LBP : The Bethe Approximation"></a>LBP : The Bethe Approximation</h3><p>我们对于 LBP 算法的正确性做一个分析：</p><p>一般来讲，真实的分布 P 是这样子的：</p><script type="math/tex; mode=display">P(X) = \frac{1}{Z} \prod_{f_a \in F} f_a(X_a)</script><p>但是这种分布的计算很困难（$f_a 是 factor graph 的算子$）。。</p><p>于是我们转向另一个分布 Q ，在后面，Q 是我们近似得到的分布，我们希望来评价 Q，也就是 Q 和 P 的相似度，对于某一事件不同概率的衡量，最常用的就是相对熵 KL Divergence ：</p><script type="math/tex; mode=display">KL(Q\Vert P) = \sum_X Q(X) log(\frac{Q(X)}{P(X)})</script><p>相对熵是来衡量两个取值为正的函数或者概率分布之间的差异的，有以下的特性：</p><ul><li>$KL(Q\Vert P) \geq 0$</li><li>$KL(Q\Vert P) = 0$ iff $Q=P$</li></ul><p>相对熵还可以写成 <strong>信息熵</strong> 减去 <strong>交叉熵</strong>：</p><script type="math/tex; mode=display">\begin{align}KL(Q||P) &= \sum_X Q(X)log Q(X) - \sum_X Q(X) log P(X) \\&= -H_Q(X) -E_Q logP(X)\end{align}</script><p>我们把真实分布带入到 $P(X)$ 中，可以得到：</p><script type="math/tex; mode=display">\begin{align}  KL(Q||P) &= -H_Q(X) - E_Q log (\frac{1}{Z} \prod_{f_a \in F} f_a(X_a)) \\  &= -H_Q(X) - E_Q\sum_{f_a \in F}log f_a(X_a) + E_Q log Z\end{align}</script><p>我们定义一下 free-energy 为前两项：</p><script type="math/tex; mode=display">F(P,Q) = -H_Q(X) - \sum_{f_a \in F} E_Q log f_a(X_a)</script><p>对于 Energy Functional：</p><ul><li>$\sum_{f_a \in F} E_Q log f_a(X_a)$ 的计算比较方便。。。</li><li>$H_Q$ 的计算会比较复杂，因为我们需要遍历所有 $X$ 的取值再做计算<br>-</li></ul><p>树形结构的 Energy Functional Tree 是有 closed-forms，也就是说某些 energy functional 是好可以计算的</p><p>当因子图树一颗树的时候 Bathe Approximation 和 Gibbs free energy 是等价的</p><p>我们用一张图来说明一下推导的流程</p><p><img src="/2018/09/11/PGM-4/pgm4img8.jpg" style=" width:600px;"></p><ul><li>首先我们将原图转化成因子图，可以得出图的分布的表示</li><li>我们假设一种分布和原分布进行比较，得出和原分布之间的 energy function</li><li>我们定义这种近似分布为 bathe approximation，这个分布只和 $b_a$ 和 $b_i$ 相关</li><li>我们对 bathe 的 energy function 进行优化，从而得到 $b_a$ 和 $b_i$ 的更新值</li><li>$b_a$ 和 $b_i$ 优化的方式引入到图里面就是在做 BP ！</li></ul><blockquote><p>用 Bathe 来近似原分布相当于在图上做 BP</p></blockquote><p>每一种假设的近似分布都对应这一种更新策略<br>我们先来考虑图 (a) 树形的结构，树形结构的概率图都可以转换成树形结构的因子图，树形结构的因子图的概率可以写成</p><script type="math/tex; mode=display">b(\mathbf{x}) = \prod_a b_a(\mathbf{x}_a) \prod_i b_i(x_i)^{1-d_i}</script><ul><li>$d_i$: degree of point i</li><li>$b_a$: doubleton (pairwise) factor</li><li>$b_i$: singleton factor</li></ul><p>我们会对图 (b) 也使用 $b(\mathbf{x})$ 来近似计算概率，这种近似称为 bethe approximation，若 bethe 近似和 gibbs 分布完全相等，当且仅当因子图是树形的</p><p><img src="/2018/09/11/PGM-4/pgm4img7.jpg" style=" width:600px;"><br><br>下面，我们来求这两个图的 free energy 并进行优化，来求解近似分布 b 从而得到 b 的更新策略，而后要用来从数值优化的形式转化到结构上</p><p><strong>a</strong></p><script type="math/tex; mode=display">H_{tree} = -\sum_a \sum_{x_a} b_a(x_a) log b_a(x_a) + \sum_i (d_i-1) \sum_{x_i} b_i (x_i) log b_a(x_i)</script><script type="math/tex; mode=display">\begin{align}F_{tree} &= \sum_a \sum_{x_a} b_a(x_a) log \left( \frac{b_a(x_a)}{f_a(x_a)}  \right) + \sum_i (1-d_i) \sum_{x_i} b_i(x_i) log b_i (x_i) \notag  \\&= F_{12} + F_{23} + ... + F_{67} + F_{78} - F_1 - F_5 - F_2 - F_6 - F_3 - F_7\end{align}</script><p>$H_{tree}$ 是分布 $b(\mathbf{x})$ 的信息熵，H 的形式应该是根据连续变量的信息熵公式得出的</p><p><strong>b</strong></p><script type="math/tex; mode=display">H_{Bethe} = -\sum_a \sum_{x_a} b_a(x_a) log b_a(x_a) + \sum_i (d_i-1) \sum_{x_i} b_i (x_i) log b_a(x_i)</script><script type="math/tex; mode=display">\begin{align}F_{tree} &= \sum_a \sum_{x_a} b_a(x_a) log \left( \frac{b_a(x_a)}{f_a(x_a)}  \right) + \sum_i (1-d_i) \sum_{x_i} b_i(x_i) log b_i (x_i) \notag  \\&= F_{12} + F_{23} + ... + F_{67} + F_{78} - F_1 - F_5 - 2F_2 - 2F_6 - ... - F_8\end{align}</script><p>free energy 公式想要进行优化，还要有一些约束：</p><ul><li>$\sum_{\mathbf{x}_a} b_a(\mathbf{x}_a) = 1$</li><li>$\sum_i b_i(x_i) = 1$</li><li>$\sum_{\mathbf{x}_a \setminus x_i} b_a(\mathbf{x}_a) = b_i(x_i)$</li></ul><p>最终的优化目标函数变为了:</p><script type="math/tex; mode=display">L = F_{Bethe} + \sum_i \gamma_i \left \{  1 - \sum_{x_i} b_i (x_i)  \right \}  + \sum_a \sum_{i\in N(a)} \sum_{x_i}  \lambda_{ai} (x_i)   \left\{  b_i(x_i) - \sum_{\mathbf{x}_a \setminus x_i} b_a(\mathbf{x}_a)  \right\}</script><p>对目标函数求导得到<br><img src="/2018/09/11/PGM-4/pgm4img10.jpg" style=" width:600px;"><br>上面的 $\lambda_{ai}$ 可以替换成：</p><script type="math/tex; mode=display">\lambda_{ai} = log (m_{i\rightarrow a} (x_i)) = log \prod _{b\in N(i) \setminus a} m_{b\rightarrow i} (x_i)</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率的推断就是计算 conditional 和 marginal，之前我们学习了 exact inference 也就是准确的推断概率图概率，我们学习message passing 算法、sum-product、&lt;/p&gt;
&lt;p&gt;inference is answer a 
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>NLP-Basis</title>
    <link href="http://yoursite.com/2018/09/09/NLP-0/"/>
    <id>http://yoursite.com/2018/09/09/NLP-0/</id>
    <published>2018-09-09T07:50:49.000Z</published>
    <updated>2018-09-21T14:49:38.320Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word-Vector"><a href="#Word-Vector" class="headerlink" title="Word Vector"></a>Word Vector</h1><p>Word Vector 也就是词向量可以分为两种</p><ul><li>Count Based：这种方法是通过统计完全局的信息最后来做特征提取</li><li>Direct Prediction：这种方法只选取来局部的统计，但是能够直接进行计算</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img5.jpg" align="justify"></p><p>有一种方法能够统一上面两种性质称为 GloVe</p><p>那么我们来介绍一下几种常见的方法</p><ul><li>SVD</li><li>CBOW</li><li>SkipGram</li><li>GloVe</li></ul><p>词向量也就是把单词转换成为向量的表示，这样方便计算机进行计算<br>运算</p><p>首先，英语单词的数量很多有将近 13m，同时我们定义所有的单词集合为 $V$，以及单词的数量为 $|V|$</p><p>最简单的词向量是 <strong>one-hot Vector</strong></p><hr><p>在讲词向量时，我们先来引出共现矩阵 co-occurrence matrix 的概念，我们假设共现矩阵为 X ，其元素 $X_{i,j}$ 表示单词 i 和单词 j 在同一个窗口一起出现的次数的统计，这里的统计是对于某个数据库下的统计。<br>共现矩阵在很多的算法中都会出现，为了让共现矩阵更加完善，我们会作出一下修改和限制，比如，我们会抑制共现矩阵中出现的较大元素让他们 $\leq 100$ ，比如一些常见的单词 ‘the’ ‘he’ 等等造成的统计<br>使用 ramp window 也就是共现矩阵中的元素更新不再是 +1 而是根据离中心词的距离加权考虑</p><hr><h3 id="SVD-Method"><a href="#SVD-Method" class="headerlink" title="SVD Method"></a>SVD Method</h3><p>直接对于共现矩阵做 SVD 分解，分解后选取前 k 大的特征值对应的特征向量来作为词向量</p><p><img src="/2018/09/09/NLP-0/nlp0img1.jpg" align="justify"></p><p>这个方法其实是有很多缺点的</p><ul><li>矩阵是稀疏的，因为很多次是不会一起出现的</li><li>矩阵的维度很大，所以做 SVD 很花费时间</li></ul><hr><h3 id="CBOW-（Continuous-Bag-of-words-Models）"><a href="#CBOW-（Continuous-Bag-of-words-Models）" class="headerlink" title="CBOW （Continuous Bag of words Models）"></a>CBOW （Continuous Bag of words Models）</h3><p>CBOW 其实就是计算以某个单词为中心，固定一个窗口，计算周围单词出现的概率乘积，然后对于这个乘积就是这个事件的概率，与此同时在训练的过程中，我们</p><p>首先我们来看看这个问题的几个参数</p><ul><li>$w_i$ : 在字典 $|V|$ 中的单词 i</li><li>$V (n\times |V|)$ : context 词向量矩阵</li><li>$U (n\times |V|)$ : center 词向量矩阵</li><li>$v_i (n\times 1)$ : V 矩阵的某一列，也就是 $w_i$ 的上下文词向量</li><li>$u_i (n\times 1)$ : U 矩阵的某一列，也就是 $w_i$ 的中心词向量</li><li>$m$ : 窗口的大小</li></ul><h5 id="CBOW-Algorithm"><a href="#CBOW-Algorithm" class="headerlink" title="CBOW Algorithm"></a>CBOW Algorithm</h5><ul><li>对于 context 单词，我们生成 2m 个的 one-hot 向量 $[ x^{(c-m)},…,x^{(c-1)},x^{(c+1)},x^{(c+m)} ]$</li><li>用 context 词向量矩阵乘以 one-hot vector 从而得到 context 单词对应的 context 词向量 $v_i = Vx^i$</li><li>将这些得到的 context 词向量取均值得到 $\hat{v} = \frac{v_{c-m}+…+v_{c+m}}{2m}$</li><li>用 center 词向量矩阵去乘以上面得到的 context 均值词向量矩阵得到：所有单词以中心词向量表示于所有 context 的乘积，结果就是对于每个单词的一个 score : $z = U\hat{v}$</li><li>我们将 score 转换成概率，用 softmax 来实现：$\hat{y} = softmax(z)$</li><li>首先我们是知道真实的分布的也就是中心单词的 one-hot vector ，为 $y$ , 这样我们就可以用各种 loss function 来优化了，一般是 cross entropy 。。</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img2.jpg" align="justify"></p><p>上面这张图 $W$ 相当于 context matrix ，$W^{\prime}$ 相当于 center matrix</p><hr><h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><h5 id="Skip-Gram-Algorithm"><a href="#Skip-Gram-Algorithm" class="headerlink" title="Skip-Gram Algorithm"></a>Skip-Gram Algorithm</h5><ul><li>对于 center 单词，生成它的 one-hot 向量</li><li>然后再获取中心单词的词向量 $v_c = V x$</li><li>用上面的中心词向量乘以 context 词向量矩阵得到一个 $|V|$ 维向量</li><li>对于这一个向量，我们取不同的位置的值做 softmax 预测，从而生成对应的 loss function</li></ul><p><img src="/2018/09/09/NLP-0/nlp0img4.jpg" align="justify"></p><p>上面一张图是 CBOW 和 Skip-Gram 算法之间的比较，总的来说</p><ul><li>CBOW ：根据周围单词，来估计中心单词出现的概率</li><li>Skip-Gram ：根据中心单词，来估计周围单词出现的概率</li></ul><hr><h3 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h3><p>直接写出 Glove 模型的优化函数吧：</p><script type="math/tex; mode=display">J(\theta)  = \frac{1}{2} \sum_{i,j=1}^w f(X_{ij}) (u_i^Tv_j - log X_{ij})^2</script><blockquote><p>对于上面这个公式，f 其实是一个权重函数，$\theta$ 是所有的变量 $U,V$</p></blockquote><hr><h1 id="Dependecy-Parsing"><a href="#Dependecy-Parsing" class="headerlink" title="Dependecy Parsing"></a>Dependecy Parsing</h1><p>如何描述语法，有两种主流观点，其中</p><ul><li>一种是短语结构文法：这种短语语法用固定数量的rule分解句子为短语和单词、分解短语为更短的短语或单词。。。</li><li>一种是依存结构：用单词之间的依存关系来表达语法。如果一个单词修饰另一个单词，则称该单词依赖于另一个单词。</li></ul><p>为什么要引用描述语法呢，因为一个句子，可以看作是几个单词的组合，但是机器要理解的是这些单词传递给人的意思，所以不仅仅是单词的出现，这些单词是如何表达意思的同样重要，因为这些句子可能会有多种意思的表达。。</p><p><img src="/2018/09/09/NLP-0/nlp0img6.jpg" align="justify"></p><p>比如这句活就可能有两个意思，但是确定了句法树，也就是上面的箭头，一个句子的意思就得到了确定。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Word-Vector&quot;&gt;&lt;a href=&quot;#Word-Vector&quot; class=&quot;headerlink&quot; title=&quot;Word Vector&quot;&gt;&lt;/a&gt;Word Vector&lt;/h1&gt;&lt;p&gt;Word Vector 也就是词向量可以分为两种&lt;/p&gt;
&lt;ul&gt;

      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Sparse-Coding</title>
    <link href="http://yoursite.com/2018/09/09/Sparse-Coding/"/>
    <id>http://yoursite.com/2018/09/09/Sparse-Coding/</id>
    <published>2018-09-09T01:21:38.000Z</published>
    <updated>2018-09-09T07:48:19.909Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经学启发"><a href="#神经学启发" class="headerlink" title="神经学启发"></a>神经学启发</h1><p>稀疏编码的概念来自于神经生物学。生物学家提出，哺乳类动物在长期的进化中，生成了能够快速，准确，低代价地表示自然图像的视觉神经方面的能力。我们直观地可以想象，我们的眼睛每看到的一副画面都是上亿像素的，而每一副图像我们都只用很少的代价重建与存储。我们把它叫做稀疏编码，即Sparse Coding。</p><p>从上可以看出稀疏编码的目的是：在大量的数据集中，选取很小部分作为元素来重建新的数据。</p><hr><h1 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h1><p>稀疏编码是一种 unsupervised learning，我们希望找到一组 over-complete 的 基向量 basic vector 来表示我们的数据，也就是数据 $\textbf{x}$ 可以表示为这些基向量的线性组合：</p><script type="math/tex; mode=display">  \textbf{x} = \sum_{i=1}^k a_i \phi_i</script><p>对于基向量的学习，我们一般有一组训练数据，另外，基向量的大小规定为 $\textbf{x}\in R^n$，同时，也是数据的大小。一般来讲，$n$ 维的数据最多只需要 $n$ 个线性不相关的基向量就可以了（使用 PCA），但是就 n 个基向量的话，可能有一个现象就是，没个数据可能都需要接近 n 个基向量来表示，我们希望 稀疏编码 能有一个优势，就是组成数据的基向量个数尽可能的小一些，也就是上面的 $\mathbf{a}$ 是稀疏的。。。</p><p>为了达成这个目的，我们可以增加基向量的个数 $k  &gt; n$，也就是说，一个数据，可能由多种基向量来表示了，在所有的表示中，我们可以尽可能的选取稀疏的表示方法。。</p><p>Sparse Coding 可以分为两个部分，一个是 Training 阶段，一个是 Coding阶段</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>给定一些列样本数据 $[ x_1,x_2,… x_m]$ 我们希望学到一组基 $[ \phi_1,\phi_2,…,\phi_k  ]$ 来表示前面的数据，训练的 objective function 如下：</p><script type="math/tex; mode=display">\mathop{min}_{a,\phi} \sum_{i=1}^m \left \Vert x_i-\sum_{j=1}^k a_{i,j}\phi_j \right \Vert^2 + \lambda \sum_{i=1}^m \sum_{j=1}^k |a_{i,j}|</script><p>优化的迭代分为两部（都可以用凸优化来求解）</p><ul><li>固定字典 $\phi$ 更新 $a$，这个问题其实就是一个 Lasso 问题</li><li>固定表达 $a$ 更新 $\phi$，这个其实就是一个 QP 问题</li></ul><h3 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h3><p>给定一个新的数据，获得它关于字典的表达，这一步，其实就是上面的优化的第二步</p><script type="math/tex; mode=display">\mathop{min}_{a} \sum_{i=1}^m \left \Vert x_i-\sum_{j=1}^k a_{i,j}\phi_j \right \Vert^2 + \lambda \sum_{i=1}^m \sum_{j=1}^k |a_{i,j}|</script><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/aixueshuqian/p/3936892.html" target="_blank" rel="noopener">https://www.cnblogs.com/aixueshuqian/p/3936892.html</a><br><a href="https://www.cnblogs.com/caocan702/p/5666175.html" target="_blank" rel="noopener">https://www.cnblogs.com/caocan702/p/5666175.html</a><br>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;神经学启发&quot;&gt;&lt;a href=&quot;#神经学启发&quot; class=&quot;headerlink&quot; title=&quot;神经学启发&quot;&gt;&lt;/a&gt;神经学启发&lt;/h1&gt;&lt;p&gt;稀疏编码的概念来自于神经生物学。生物学家提出，哺乳类动物在长期的进化中，生成了能够快速，准确，低代价地表示自然图像的
      
    
    </summary>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>PGM-3</title>
    <link href="http://yoursite.com/2018/08/18/PGM-3/"/>
    <id>http://yoursite.com/2018/08/18/PGM-3/</id>
    <published>2018-08-18T13:21:46.000Z</published>
    <updated>2018-09-11T00:50:11.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Exponential-Family"><a href="#The-Exponential-Family" class="headerlink" title="The Exponential Family"></a>The Exponential Family</h1><p>random variable $\mathbf{X}$ is in the exponential family</p><script type="math/tex; mode=display">\begin{align}P(\mathbf{X}=x;\eta) &= h(x)\mathop{exp}\{ \eta^T\mathbf{T}(x) - A(\eta) \} \\&= \frac{1}{Z(\eta)} h(x) exp \{ \eta^T T(x) \}\end{align}</script><ul><li>$\eta$ :  vector of natural parameters</li><li>$\mathbf{T}$ :  vector of sufficient statistics</li><li>$\mathbf{A}$ : log partition function</li><li>$log Z(\eta) = A(\eta)$</li></ul><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><h5 id="Multivariate-Gaussian"><a href="#Multivariate-Gaussian" class="headerlink" title="Multivariate Gaussian"></a>Multivariate Gaussian</h5><script type="math/tex; mode=display">\begin{align}  P(\mathbf{x};\mu;\Sigma) &=  \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \text{exp}\left(  -\frac{1}{2} (\mathbf{x}-\mu)^T \Sigma^{-1} (\mathbf{x}-\mu) \right) \\  &= \frac{1}{(2\pi)^{p/2}} \text{exp} \left( -\frac{1}{2}(\text{tr } \mathbf{x}^T\Sigma^{-1} \mathbf{x} +\mu^T \Sigma^{-1} \mu - 2 \mu^T \Sigma^{-1} \mathbf{x} +ln |\Sigma| )\right) \\  &=  \underbrace{ \frac{1}{(2\pi)^{p/2}} }_{h(\mathbf{x})} \text{exp} \left( -\frac{1}{2} \underbrace{\text{tr } \Sigma^{-1}\mathbf{x}\mathbf{x}^T}_{\text{vec}(\Sigma^{-1})^T \text{vec}(\mathbf{x}\mathbf{x}^T)} + \mu^T\Sigma^{-1}\mathbf{x} - \underbrace{\frac{1}{2} \mu^T \Sigma^{-1} \mu - \frac{1}{2} \text{ln}|\Sigma|   }_{A(\eta)} \right)\end{align}</script><p>This implies that:</p><ul><li>$\eta = \left( \Sigma^{-1} \mu, -\frac{1}{2} vec (\Sigma^{-1}) \right)$</li><li>$\mathbf{T}(\mathbf{x}) = (\mathbf{x}, vec(\mathbf{x}\mathbf{x}^T))$</li><li>$A(\eta) = \frac{1}{2} (\mu^T \Sigma^{-1} \mu + \text{ln} |\Sigma|)$</li><li>$h(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}}$</li></ul><h5 id="Bernoulli"><a href="#Bernoulli" class="headerlink" title="Bernoulli"></a>Bernoulli</h5><script type="math/tex; mode=display">\begin{align}P(x;p) &= p^x (1-p)^{1-x} \\\text{ln} P(x;p) &= x\text{ln}(p)+(1−x)\text{ln}(1−p) \\ &= x\text{ln}(p)−x\text{ln}(1−p)+\text{ln}(1−p) \\ &= x(\text{ln}(p)−\text{ln}(1−p))+\text{ln}(1−p) \\ &= x\text{ln} (\frac{p}{1-p}) + \text{ln} (1-p) \\ \text{exp} (\text{ln} P(x;p)) &= \text{exp} \left( x\text{ln} (\frac{p}{1-p}) + \text{ln} (1-p)  \right)\end{align}</script><p>This implies that:</p><ul><li>$\eta = \text{ln} (\frac{p}{1-p})$</li><li>$T(x) = x$</li><li>$A(\eta) = -\text{ln}(1-p)$</li><li>$h(x) = 1$</li></ul><h5 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h5><p>the univariate Gaussian, Poisson, gamma, multinomial, linear regression, Ising model, restricted Boltzmann machines, and conditional random fields (CRFs) are all in the exponential family</p><h3 id="Why-Exponential-Family"><a href="#Why-Exponential-Family" class="headerlink" title="Why Exponential Family"></a>Why Exponential Family</h3><h5 id="Moment-generating-property"><a href="#Moment-generating-property" class="headerlink" title="Moment generating property"></a>Moment generating property</h5><script type="math/tex; mode=display">\begin{align}  \int P(x,\eta) dx = \int h(x) e^{\eta^T T(x) - A(\eta)} dx &= 1 \\  \int h(x) e^{\eta^T T(x)} &= Z(\eta)\end{align}</script><p>-</p><script type="math/tex; mode=display">\begin{align}  \frac{dA}{d\eta} &= \frac{d}{d\eta} log (Z(\eta)) = \frac{1}{Z(\eta)} \frac{d}{d\eta} Z(\eta) \\  &= \frac{1}{Z(\eta)} \frac{d}{d\eta} \int h(x) e^{\eta^T T(x)}dx \\  &= \int T(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} = E[T(x)]\end{align}</script><p> -</p><script type="math/tex; mode=display">\begin{align}\frac{d^2 A}{d^2 \eta} &= \int T^2(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} dx - \int T(x) \frac{h(x) e^{\eta^T T(x)}}{Z(\eta)} dx \frac{1}{Z(\eta)} \frac{d}{d \eta} Z(\eta) \\&= E[T^2(x)] - E^2[T(x)] \\&= Var[T(x)]\end{align}</script><ul><li><p>$A(\eta)$ is convex  since</p><script type="math/tex; mode=display">\frac{d^2 A(\eta)}{d \eta^2} = Var[T(x)] > 0</script></li><li><p>specific $\eta$ map to mean $\mu$, so we define an invert $\psi (\mu) = \eta$</p></li></ul><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;The-Exponential-Family&quot;&gt;&lt;a href=&quot;#The-Exponential-Family&quot; class=&quot;headerlink&quot; title=&quot;The Exponential Family&quot;&gt;&lt;/a&gt;The Exponential Fami
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>PGM-Exact Inference</title>
    <link href="http://yoursite.com/2018/08/14/PGM-2/"/>
    <id>http://yoursite.com/2018/08/14/PGM-2/</id>
    <published>2018-08-14T02:46:32.000Z</published>
    <updated>2018-09-13T01:24:04.963Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h1><p>某个确定了的概率图，它的推断可以看作是一个关于所有变量的函数，我们要求的是这个函数的具体值是多少，从概率的角度上消除变量，其实就是做这个函数的边缘化 marginalization，<br>我们尽量争取每次计算(消除)的变量比较少，这样总的复杂度不会高。。<br>Elimination 似乎可以适用于所有结构的 graph 。</p><h3 id="Directed-Chain"><a href="#Directed-Chain" class="headerlink" title="Directed Chain"></a>Directed Chain</h3><p>假设我们的图的结构是这样的 $A\rightarrow B\rightarrow C\rightarrow D\rightarrow E$</p><script type="math/tex; mode=display">\begin{align}P(e) &= \sum_{a,b,c,d} p(a,b,c,d) \\    &= \sum_{a,b,c,d} P (a)P (b|a)P (c|b)P (d|c)P (e|d) \\    &=\sum_{d,c,b} P(c|b) P(d|c) P(e|d) \sum_a P(a) P(b|a) \\    &\text{this is an one variable elimination cost } k^2\\    &= \sum_{d,c,b} P(c|b) P(d|c) P(e|d) p(b) \\    &\ \cdots \\    &= \sum_d P(e|d) p(d)\end{align}</script><p>复杂度 Complexity:</p><ul><li>Eliminate 方法: costs $O(k^2 n)$ _这里面的 $k^2$ 表示迭代 k 次，每次计算概率也要 k_</li><li>Naive 方法: cost $O(k^n)$</li></ul><h3 id="Undirected-Chain"><a href="#Undirected-Chain" class="headerlink" title="Undirected Chain"></a>Undirected Chain</h3><p>假设我们的图是这样的无向图 $A - B - C - D - E$</p><script type="math/tex; mode=display">\begin{align}P(e) &= \sum_{a,b,c,d} \frac{1}{Z} \phi(b,a) \phi(c,b) \phi(d,c) \phi(e,d) \\ &\propto \sum_{a,b,c,d} \phi(b,a) \phi(c,b) \phi(d,c) \phi(e,d) \\ &= \sum_{a,b,c,d}  \phi(c,b) \phi(d,c) \phi(e,d)\sum_a \phi(b,a)  \\ &= \sum_{a,b,c,d}  \phi(c,b) \phi(d,c) \phi(e,d) m_a(b)  \\ &\ \cdots \\ &= m_d(e)\end{align}</script><p>这里是无向图，原始的势函数运算成为 m 的结果不是概率，所以这里我们要 normalize 一下:</p><script type="math/tex; mode=display">P(e) =  \frac{m_d(e)}{\sum_e m_d(e)}</script><h3 id="Graph-Elimination"><a href="#Graph-Elimination" class="headerlink" title="Graph Elimination"></a>Graph Elimination</h3><p><img src="/2018/08/14/PGM-2/pgm2img1.jpg" align="justify"><br>我们从一张图来看 Elimination 的每一次过程后，剩下的图的结构</p><ul><li>对于一张 graph 首先我们确认 elimination 的 order</li><li>对于每一个待消除的变量，它会连接一些变量，我们将这些变量两两相连</li><li>消除待消除的变量</li></ul><p><img src="/2018/08/14/PGM-2/pgm2img9.jpg" align="justify"></p><p>我们再来观察每次 Elimination 后，也就是边缘化操作后形成的函数，发现这些函数的变量在一起，刚好能组成这个概率图结构的 cliques 集合，如果我们考虑这些 cliques 组成的树，那么 elimination 操作其实就是在这颗树上进行 message passing。</p><p>Key insight 就是这些 message 其实是可以 reused 的，重复使用是指，当我们要进行多次 querying 的时候，信息的重复使用，所以我们希望设计好的算法，能够在 querying 的过程中保存下来这些信息，于是有了后面的 Sum-Product 算法。</p><h3 id="Complexity-of-Variable-Elimination"><a href="#Complexity-of-Variable-Elimination" class="headerlink" title="Complexity of Variable Elimination"></a>Complexity of Variable Elimination</h3><p>这里 $y$ 未知的，后面操作可能要消除的变量，$x$ 是正在消除的变量，$m$ 是当前要消除的乘子，sum-product 算法分为下面两部</p><ul><li>Sum:<script type="math/tex; mode=display">m_x(y_1,...,y_k) = \sum_x m^{\prime}_x (x,y_1,...,y_k)</script></li><li>Product:<script type="math/tex; mode=display">m_x^{\prime} (x,y_1,...,y_k) = \prod_{i=1}^k m_i(x,y_{c_i})</script></li></ul><p></p><p>乘起来就是一次 Elimination 的复杂度： $k\cdot  | Var(X) |\cdot \prod_i |Var(Y_{C_i})|$<br>也就是当前变量的状态乘上，乘子也就是对应的 clique 的所有变量的状态叉乘</p><p>我们发现整个算法的复杂度取决于最大的最大的 clique，我们称这个 clique 的变量数量 k 为 <strong>Tree-width</strong> , 同时要注意的是，不同的 elimination order 的 Tree-Width 是不同的，找到最优的 order 是 np-hard 问题。。。</p><hr><p></p><h1 id="Belief-Propagation"><a href="#Belief-Propagation" class="headerlink" title="Belief Propagation"></a>Belief Propagation</h1><p>asd</p><ul><li>Trees<ul><li>Two-pass Algorithm</li></ul></li><li>Factor Trees<ul><li>Message Passing on Factor Graph</li></ul></li><li>Non-Trees (General Graph)<ul><li>Junction Tree Algorithm</li></ul></li></ul><p>概率图中有向图模型其实是无向图的一种特例，从有向图到无向图的转换关系如下</p><ul><li><p>Undirected Tree:</p><script type="math/tex; mode=display">p(x) = \frac{1}{Z}\left( \prod_{i\in V} \psi(x_i) \prod_{(i,j)\in E} \psi (x_i,x_j) \right)</script></li><li><p>Directed Tree:</p><script type="math/tex; mode=display">p(x)  = p(x_r) \prod_{(i,j)\in E} p(x_i|x_j)</script></li><li><p>Equivalence:</p><script type="math/tex; mode=display">\psi (x_r) = p(x_r);\ \psi(x_i,x_j)=p(x_j|x_i);\ Z=1,\psi(x_i) = 1</script></li></ul><h3 id="Trees"><a href="#Trees" class="headerlink" title="Trees"></a>Trees</h3><p>Elimination 操作可以看作是 message passing.</p><p>令 $m_{ji}(x_i)$ 当作是从 i 那里变量消除后生成的乘子，同时，这就是 $x_i$ 的函数:</p><script type="math/tex; mode=display">m_{ji}(x_i) = \sum_{x_j} \left( \psi(x_j)\psi(x_i,x_j) \prod_{k\in N(j)\setminus i} m_{kj}(x_j) \right)</script><p>上面的公式可以理解为：从 $i$ 到 $j$ 的信息，只和传递信息箭头相反的范围内的那些节点相关</p><p>对于某个节点所对应的概率，我们可以这样表示：</p><script type="math/tex; mode=display">p(x_i)\propto \psi(x_i) \prod_{e\in N(i)} m_{ei}(x_i)</script><p>可以看到计算 $p(x_i)$ 的时候， $m_{ij}(x_i)$ 会被重复的使用, 所以我们可以存储 $m$ 的值</p><p>树的 Elimination 来做 querying 算法的复杂度是 $O(NC)$ (where N=nodes, C=complexity of one complete passing/clique bottleneck). 但是使用了 two path 算法（因为是无向图所以每条边有两个方向）以后，复杂度就变成了: 2C, or $O(C)$</p><p><strong>belief propagation is only valid on trees</strong></p><h3 id="Factor-Trees"><a href="#Factor-Trees" class="headerlink" title="Factor Trees"></a>Factor Trees</h3><p><br>首先，我们定义一个变换，这个变换把一个图变成了一个新的图，变换后的图称为 <strong>Factor Graph</strong>,如果变换后刚好是一颗树，那么我们也可以称之为 Factor Tree。 在新的 Factor Graph 中，每一个 factor (clique) 在图中表示一个节点 f, 下面是一个例子,  其中的一个性质就是 $f$ 节点只和 $x$ 节点相连，也就是说，x 的某个变量把它和其他变量的关系都托付给了 f 节点。</p><p><img src="/2018/08/14/PGM-2/pgm2img2.jpg" align="justify"></p><p>对于一个图，可能有好几种变换方式，我们希望变换后的结果就是一个树，和下面的 Example 3 一样。</p><p><img src="/2018/08/14/PGM-2/pgm2img3.jpg" align="justify"></p><p>另外变换后的图其实是一个二分图（bipartite），二分图每一侧都是一种类型的节点，所以信息传递策略于传统的方法有些不同。。有两种信息的传递方式</p><p><img src="/2018/08/14/PGM-2/pgm2img4.jpg" align="justify"></p><ul><li>$\nu$ : from variables to factors（左图）<script type="math/tex; mode=display">\nu_{is}(x_i) = \prod_{t\in N(i)\setminus s} \mu_{ti}(x_i)</script></li><li>$\mu$ : from factors to variables（右图）<script type="math/tex; mode=display">\mu_{si}(x_i) = \sum_{x_{N(s)}\setminus i}\left( f_s(x_{N(s)}) \prod_{j\in N(s)\setminus i} \nu_{js}(x_j)\right)</script><ul><li><strong>_上面的 $\sum$ 操作是遍历变量的赋值，$\sum$ 操作下面的 x 可以看作是一个向量，遍历向量里面所有的赋值._</strong></li></ul></li></ul><p>Factor Tree 算法只能够处理一些长得像树的概率图</p><h3 id="Junction-Trees"><a href="#Junction-Trees" class="headerlink" title="Junction Trees"></a>Junction Trees</h3><p>Junction tree data-structure for exact inference on general graphs</p><p><strong>Algorithm</strong></p><ul><li>Moralization</li><li>Triangulation</li><li>Junction tree</li><li>Message Propagation</li></ul><h5 id="Moral-Graph"><a href="#Moral-Graph" class="headerlink" title="Moral Graph"></a>Moral Graph</h5><p>因为我们要处理的是广泛结构的概率图模型，所以我们先把 BN 纳入到 MRF 的框架里面，这一步骤叫做 Moralization，我们知道 BN 中的 factor 是某些父变量对于指定变量的条件概率，我们不管哪些是条件变量，我们就把他们看成是一个整体的函数，我们的终极目的是生成一个 clique，clique 有要求是全联通的，于是我们就将这个变量的父节点两两配对相连，这样就形成了一个 clique，原来的 factor 就变成了势函数 potential。</p><p>在这里我们得到一个启发，就是增加一条边后，原本的 graph 是新的 graph 的一种特殊情况。</p><p><img src="/2018/08/14/PGM-2/pgm2img5.jpg" align="justify"></p><h5 id="Triangulation"><a href="#Triangulation" class="headerlink" title="Triangulation"></a>Triangulation</h5><p>对于三角化的操作，我们可以先看后面两个操作的介绍再回来，因为这是为了解决后面问题的而诞生的一个步骤</p><p>问题就是 Local Consistency 不能导出 Global Consistency，只有在三角化后的图中</p><p>三角化以后的图是没有大于 4 个节点以上的环的， 三角化的方法就是在大的环中添加额外边</p><h5 id="Clique-Tree"><a href="#Clique-Tree" class="headerlink" title="Clique Tree"></a>Clique Tree</h5><p><img src="/2018/08/14/PGM-2/pgm2img6.jpg" align="justify"></p><p>下面的推断可以知道，有向图条件概率乘积的表达形式，其实就是 clique tree 表达形式的一种特殊情况</p><script type="math/tex; mode=display">\begin{align}&P(X_1,X_2,X_3,X_4,X_5,X_6)\\& = P(X_1)P(X_2)P(X_3 | X_1,X_2)P(X_4 | X_3)P(X_5 | X_3)P(X_6 | X_4,X_5) \\& = P(X_1,X_2,X_3) \frac{P(X_3,X_4,X_5)}{P(X_3)} \frac{P(X_4,X_5,X_6)}{P(X_4,X_5)} \\& = \psi(X_1,X_2,X_3) \frac{\psi(X_3,X_4,X_5)}{\phi(X_3)} \frac{\psi(X_4,X_5,X_6)}{\phi(X_4,X_5)}\end{align}</script><p><strong>General Form :</strong> 之所以下面是要除以 cliques 之间的交集 S ，是因为交集的信息可能出现了多次</p><script type="math/tex; mode=display">P(\mathbf{X}) = \frac{\prod_{c} \psi_c(\mathbf{X_c})}{\prod_{s} \phi_s(\mathbf{X_s})}</script><h5 id="Message-Passing"><a href="#Message-Passing" class="headerlink" title="Message Passing"></a>Message Passing</h5><p><br>传递方式有两种，这两种方法算出来的结果应该是一样的，这是我们做出的假设称为 <strong>Local Consistency</strong></p><script type="math/tex; mode=display">P(S) = \sum_{V\setminus S} \psi(V) \qquad \qquad P(S) = \sum_{W\setminus S} \psi(W)</script><p>下面的第一行是 forward update，第二行是 backward update，其中 $\frac{\phi_S^*}{\phi_S}$ 是通过 Local Consistency 得出的，是建立起矩形节点两边沟通的桥梁</p><p><img src="/2018/08/14/PGM-2/pgm2img7.jpg" align="justify"><br><img src="/2018/08/14/PGM-2/pgm2img11.jpg" align="justify"></p><p>上面是 clique tree 信息传递的方式，</p><p><strong>Shafer-Shenoy algorithm</strong><br><img src="/2018/08/14/PGM-2/pgm2img8.jpg" align="justify"></p><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h3 id="General-Variable-Elimination"><a href="#General-Variable-Elimination" class="headerlink" title="General Variable Elimination"></a>General Variable Elimination</h3><p>为了让计算机能够自动的处理各种各样结构的概率图的 Elimination，我们可以设计一种更加 general 的形式，但是这个形式的设定，主要还是为了进行计算机的运算的。。</p><ul><li>Let $X$ be set of all random variables</li><li>Let $F$ denote the set of factors and then for each $\phi \in F$,$Scope[\phi] \in X$</li><li>There three type of variables in Elimination Model<ul><li>Let $Y\subset X$ be a set of <strong>query</strong> variables</li><li>Let $Z = X - Y$ would be the set of variables to be <strong>eliminated</strong></li><li>Let $\mathcal{E}$ be the <strong>known</strong> variables, and $\bar{e}_i$ is the assignment</li></ul></li></ul><p>The core operation can be view as the form of, we can extend it to general form by import evidence potential</p><script type="math/tex; mode=display">\tau(Y) =  \sum_z \prod_{\phi \in F} \phi</script><ul><li><p>The evidence potantial:</p><script type="math/tex; mode=display">\begin{align}  \delta(\mathcal{E}) = \left \{ \begin{array}{ll} 1& if\ \mathcal{E_i} \equiv \bar{e}_i \\ 0 & if\ \mathcal{E_i} \neq \bar{e}_i  \end{array}  \right .\end{align}</script></li><li><p>Total evidence potential:</p><script type="math/tex; mode=display">\begin{align}  \delta(\mathbf{\mathcal{E}},\mathbf{\bar{e}})= \prod_{i\in I_{\mathcal{E}}} \delta (\mathcal{E}_i,\bar{e}_i)\end{align}</script></li><li>Introducing evidence:<script type="math/tex; mode=display">\tau(\mathbf{Y},\mathbf{\bar{e}}) = \sum_{z,e}\prod_{\phi \in F} \phi \times \delta(\mathbf{\mathcal{E}},\bar{\mathbf{e}})</script></li></ul><h5 id="The-elimination-algorithm"><a href="#The-elimination-algorithm" class="headerlink" title="The elimination algorithm"></a>The elimination algorithm</h5><p>… …</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.jianshu.com/p/f90100680749" target="_blank" rel="noopener">https://www.jianshu.com/p/f90100680749</a></p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Variable-Elimination&quot;&gt;&lt;a href=&quot;#Variable-Elimination&quot; class=&quot;headerlink&quot; title=&quot;Variable Elimination&quot;&gt;&lt;/a&gt;Variable Elimination&lt;
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Random-Record</title>
    <link href="http://yoursite.com/2018/08/09/Random-Record/"/>
    <id>http://yoursite.com/2018/08/09/Random-Record/</id>
    <published>2018-08-09T10:55:56.000Z</published>
    <updated>2018-08-24T14:41:49.394Z</updated>
    
    <content type="html"><![CDATA[<p>如果一个变量在两个 clique 之间出现，那么这个变量一定在这两个 clique 的路径之间</p><p>learning is parameter estimation</p><p>sufficient statistics $T(x)$ means x self or some transformation of x</p><p>先验概率可理解为统计概率，后验概率可理解为条件概率</p><hr><p>In Bayesian probability theory, if the posterior distributions $p(\theta | x)$ are in the same probability distribution family as the prior probability distribution $p(\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function.</p><h2 id="For-example-the-Gaussian-family-is-conjugate-to-itself-or-self-conjugate-with-respect-to-a-Gaussian-likelihood-function-if-the-likelihood-function-is-Gaussian-choosing-a-Gaussian-prior-over-the-mean-will-ensure-that-the-posterior-distribution-is-also-Gaussian"><a href="#For-example-the-Gaussian-family-is-conjugate-to-itself-or-self-conjugate-with-respect-to-a-Gaussian-likelihood-function-if-the-likelihood-function-is-Gaussian-choosing-a-Gaussian-prior-over-the-mean-will-ensure-that-the-posterior-distribution-is-also-Gaussian" class="headerlink" title="For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian"></a>For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian</h2><p>Learning Graphical Models</p><p>The target is given the assignments and predict the best (most likely) structure of the network.</p><p>“Optimal” here means the employed algorithms guarantee to<br>return a structure that maximizes the objectives (e.g., LogLik)</p><script type="math/tex; mode=display">\begin{align}l(\theta_G,G;D) &= log p(D|\theta_G,G) \\&= log \prod_n \left(  \prod_i p(x_{n,i}|\mathbf{x}_{n,\pi_i(G)}, \theta_{i|\pi_i(G)})  \right)\end{align}</script><ul><li>$\prod_n$ means enum $n$ data</li><li>$\prod_i$ means enum all nodes</li><li>$\mathbf{x}_{n,\pi_i(G)}$ is the assignments of x’s parents</li></ul><p>M is the number of the state, add this we can turn the count function into probability representation</p><p>the right decomposited part is entropy !</p><p>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如果一个变量在两个 clique 之间出现，那么这个变量一定在这两个 clique 的路径之间&lt;/p&gt;
&lt;p&gt;learning is parameter estimation&lt;/p&gt;
&lt;p&gt;sufficient statistics $T(x)$ means x self
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PGM-Representation</title>
    <link href="http://yoursite.com/2018/08/08/PGM-1/"/>
    <id>http://yoursite.com/2018/08/08/PGM-1/</id>
    <published>2018-08-08T12:51:00.000Z</published>
    <updated>2018-09-10T06:37:07.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction-of-PGM"><a href="#Introduction-of-PGM" class="headerlink" title="Introduction of PGM"></a>Introduction of PGM</h1><h3 id="Why-Using-PGM"><a href="#Why-Using-PGM" class="headerlink" title="Why Using PGM"></a>Why Using PGM</h3><p>For a joint distribution $P(X_1,X_2,…,X_n)$, can be written in two ways</p><ul><li>All Dependent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2|X_1)P(X_2|X_1,X_2)…P(X_n|X_1,…,X_{n-1})$<ul><li>pro: the formular are correct in any case</li><li>con: require huge probability table $O(k^n)$</li></ul></li><li>Independent: $P(X_1,X_2,…,X_n) = P(X_1)P(X_2)…P(X_n)$<ul><li>pro: probability table only take $O(kn)$ memory</li><li>con: independent are restrictive hypothese</li></ul></li></ul><p>The PGM are target a middle-ground between two extremes</p><p>Two types of GMs</p><ul><li><strong>Directed edges</strong> give causality relationships (Bayesian Network or Directed Graphical Model)</li><li><strong>Undirected edges</strong> simply give correlations between variables (Markov Random Field or Undirected Graphical model)</li></ul><h1 id="Bayesian-Network"><a href="#Bayesian-Network" class="headerlink" title="Bayesian Network"></a>Bayesian Network</h1><p>Firstly the Bayesian Network is <strong>DAG</strong> (Directed Acyclic Graph)</p><p><strong>Factorization Theorem</strong> : The probability of specific BN</p><script type="math/tex; mode=display">P(X_1,X_2,...,X_n) = \prod_{i=1:n} P(X_i|Parents(X_i))</script><h3 id="Graph-model-and-Bayesian"><a href="#Graph-model-and-Bayesian" class="headerlink" title="Graph model and Bayesian"></a>Graph model and Bayesian</h3><p>Suppose the distribution is $A\leftarrow B \rightarrow C$</p><ul><li>Bayesian Interpretation: $P(ABC) = P(B) P(A|B) P(C|B)$</li><li><p>Graph Model Interpretation: $I(G) = \{ A\bot C |B \}$<br>now we want to prove $I(P(ABC)) = I(\{ A\bot C |B \})$</p><script type="math/tex; mode=display">\begin{align}  P(AC|B) &= P(A|B)P(C|B) \leftarrow \{ A\bot C |B \}\\  P(AC|B) &= \frac{P(ABC)}{P(B)} = \frac{ P(B) P(A|B) P(C|B)}{P(B)} \leftarrow P(ABC)\end{align}</script></li></ul><p><strong>conditional Independence :</strong><br>for $X\leftarrow Z \rightarrow Y$, $Z$ represent the height of father, $X,Y$ are the brother, the relation between $X$ and $Y$ :</p><ul><li>Dependent : we dont know the height of father, but we know the height of $Y$, so $Y$ may effect the distribution of $Z$, at the same time also influence the $X$</li><li>Independent : we already know the height of father, so whether $Y$ is dont influence $X$</li></ul><p><strong>local Markov assumption :</strong><br>each node $X_i$ is independent of its nondescendants given its parents.<br>_when you confirm the parents, you only dependent with your childs_</p><script type="math/tex; mode=display">I_l(G):\{ X_i\bot NonDescendants_{X_i} | Pa_{X_i} : \forall i \}</script><h3 id="Independencies-Three-Basic-Model"><a href="#Independencies-Three-Basic-Model" class="headerlink" title="Independencies (Three Basic Model)"></a>Independencies (Three Basic Model)</h3><p><img src="/2018/08/08/PGM-1/pgm1img2.jpg" align="justify"></p><ul><li><strong>a: Cascade</strong><ul><li>$Y$ observed $X,Z$ are independent</li></ul></li><li><strong>b: Common parent</strong><ul><li>$Y$ observed $X,Z$ are independent</li><li>$Y$ unknow $X,Z$ are dependent</li></ul></li><li><strong>c: V-structure</strong><ul><li>$Y$ observed $X,Z$ are dependent</li><li>$Y$ unknow $X,Z$ are independent</li></ul></li></ul><p>Let $P$ be a distribution of $X$, $I(P)$ is the set of independence assertions of the form $(X \bot Y | Z)$ that hold in $P$. $I(G)$ is the sub-set of the $I(P)$,</p><p>We say that $K$ is an $I$-map for a _set_ of independencies $I$ if $I(K) ⊆ I$</p><h3 id="Active-trail"><a href="#Active-trail" class="headerlink" title="Active trail"></a>Active trail</h3><ul><li>Causal Trail $X → Z → Y$ : active iff $Z$ is not observed.</li><li>Evidential Trail $X ← Z ← Y$ : active iff $Z$ is not observed.</li><li>Common Cause $X ← Z → Y$: active iff $Z$ is not observed.</li><li>Common Effect $X → Z ← Y$ : active iff $Z$ (or any of its descendents) is observed.<br>_here active means the dependent relation estibilish_</li></ul><p>Definition : Let $\textbf{X}, \textbf{Y} , \textbf{Z}$ be three sets of nodes in $G$. We say that $\textbf{X}$ and $\textbf{Y}$ are d-separated given $\textbf{Z}$, denoted $dsep_G(\textbf{X};\textbf{Y}|\textbf{Z})$, if there is no active trail between any node $X \in \textbf{X}$ and $Y \in \textbf{Y}$ given $\textbf{Z}$</p><p>Definition: $I(G)=$ all independence properties that correspond to d- separation:</p><script type="math/tex; mode=display">I(G) = {X \bot Y | Z : dsep_G(X \bot Y | Z)}</script><hr><h1 id="Undirected-GM"><a href="#Undirected-GM" class="headerlink" title="Undirected GM"></a>Undirected GM</h1><p>An undirected graphical model represents a distribution $P(X1,…Xn)$ defined by an undirected graph H, and a set of positive-valued potential functions $\psi_c$ corresponding to each clique $c \in C$ of $H$ such that:</p><script type="math/tex; mode=display">\begin{align}P(X_1,...,X_n) = \frac{1}{Z} \prod_{c\in C} \psi_c(X_c) \\Z = \sum_{X_1,...,X_n} \prod_{c\in C} \psi_c(X_c)\end{align}</script><p>potential function can be joint and conditional probability function, or even a table of values</p><p>Clique Example<br><img src="/2018/08/08/PGM-1/pgm1img4.jpg"></p><ul><li>max-cliques = $\{A,B,D\}, \{B,C,D\}$,</li><li>sub-cliques = $\{A,B\}, \{C,D\}, …\text{ all edges and single point}$</li></ul><script type="math/tex; mode=display">\begin{align}P^{\prime}(x_1,x_2,x_3,x_4) = \frac{1}{Z} \psi_c(X_{124}) \times \psi_c(X_{234}) \\Z = \sum_{x_1,x_2,x_3,x_4} \psi_c(X_{124})\end{align}</script><h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence:"></a>Independence:</h3><ul><li><strong>global Markov independencies $I(H)$</strong> $= \{ A\bot C|B:sep_H(A,C|B) \}$<ul><li>any disjoint A,B,C in distribution, B separates A and C, A is independent of C given B.</li></ul></li><li><strong>local Markov independencies $I_l(H)$</strong>   $=\{X_i \bot V \setminus (X\cup MB_{X_i})|MB_{X_i} :\forall i  \}$<ul><li>Independent with node that dont near it.</li></ul></li><li><strong>pairwise Markov independencies $I_p(H)$</strong> $=\{X \bot Y|V \setminus \{X,Y\}:\{X,Y\}\notin E\}$<ul><li>Independent when no shared edge.</li></ul></li><li><strong>Markov blanket</strong> of $X_i$ denoted $MB_{X_i}$, is the neighbors of $X_i$ in graph</li><li>B separates A and C if every path from A to C through B: $sep_H(A,C|B)$</li></ul><p>relation of local and global</p><ul><li>Thm: $P\models I(H) \Rightarrow P \models I_l(H) \Rightarrow P \models I_p(H)$</li><li>Corollary: For a positive distribution P, global, local, and pairwise indepedencies are equivalent</li></ul><h3 id="Exponential-Model"><a href="#Exponential-Model" class="headerlink" title="Exponential Model"></a>Exponential Model</h3><p>Form discussed above, we need find a clique potential can ensure the positive distribution, one form is negative exponential: $\Psi (\mathbf{x}_c) = exp\{ -\phi_c(\mathbf{x_c}) \}$</p><p>The exponential form of the distribution structure is:</p><script type="math/tex; mode=display">p(\mathbf{x}) = \frac{1}{Z}\mathop{exp}\left\{ - \sum_{c\in C} \phi_c(\mathbf{x_c}) \right\}=\frac{1}{Z} \mathop{exp}\{ -H(\mathbf{x}) \}</script><p>$H(\mathbf{x})$ is the “free energy”.</p><h5 id="Boltzmann-Machines"><a href="#Boltzmann-Machines" class="headerlink" title="Boltzmann Machines"></a>Boltzmann Machines</h5><p>A Boltzmann Machine is a fully connected graph with pairwise potentials on binary-valued nodes. The energy function for this is expressed in sub-clique form, which comes from the physics tradition.</p><script type="math/tex; mode=display">\begin{align}P(\mathbf{x}) &= \frac{1}{Z} \mathop{exp} \left\{ \sum_{i,j} \phi_{ij}(x_i,x_j) \right\} \\&= \frac{1}{Z} \mathop{exp} \left\{ \sum_{i,j} \theta_{ij}x_ix_j + \sum_i \alpha_i x_i + C \right\} \\&= \frac{1}{Z} \mathop{exp} \left\{ (x-\mu)^T \Theta (x-\mu) \right\}\end{align}</script><h5 id="Restricted-Boltzmann-Machines"><a href="#Restricted-Boltzmann-Machines" class="headerlink" title="Restricted Boltzmann Machines"></a>Restricted Boltzmann Machines</h5><p>This is inspired by the Boltzmann Machine and is responsible for much of the deep learning craze. An RBM consists of many layers. Within each layer, there are two sublayers: one of hidden units (factors, $h_j$), and one of visible units ($x_i$). The probability function for an RBM is</p><script type="math/tex; mode=display">p(x,h|\theta) = \mathop{exp} \left\{ \sum_i \theta_i \phi_i (x_i) + \sum_j \theta_j \phi_j(h_j) + \sum_{i,j} \theta_{i,j} \phi_{i,j} (x_i,h_j) - A(\theta) \right\}</script><h5 id="Conditional-Random-Fields"><a href="#Conditional-Random-Fields" class="headerlink" title="Conditional Random Fields"></a>Conditional Random Fields</h5><p>…</p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction-of-PGM&quot;&gt;&lt;a href=&quot;#Introduction-of-PGM&quot; class=&quot;headerlink&quot; title=&quot;Introduction of PGM&quot;&gt;&lt;/a&gt;Introduction of PGM&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process-11</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-11/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-11/</id>
    <published>2018-08-04T08:05:19.000Z</published>
    <updated>2018-08-06T01:27:43.065Z</updated>
    
    <content type="html"><![CDATA[<p>i.i.d sequence of random variables: too restrictive assumption</p><p>completely dependent among random variables: hard to analysis</p><p>balance between complete independence &amp; complete dependence</p><p>Classification of Markov Process</p><ul><li>Discrete-Time Markov Chain: Discrete $S$ &amp; Discrete $T$</li><li>Continuous-Time Markov Chain: Discrete $S$ &amp; Continuous $T$</li><li>Discrete Markov Chain: Continuous $S$ &amp; Discrete $T$</li><li>Continuous Markov Chain: Continuous $S$ &amp; Continuous $T$</li></ul><h3 id="Definition-Markov-Chain"><a href="#Definition-Markov-Chain" class="headerlink" title="Definition (Markov Chain)"></a>Definition (Markov Chain)</h3><p>A sequence of random variables $X_0,X_1,X_2,…$ taking values in the state space $\{1,2,…,M\}$ is called <strong>Markov Chain</strong>, the event $X_i+1$ only influenced by $X_i$</p><script type="math/tex; mode=display">P(X_{n+1}=j|X_n = i,X_{n-1}=i_{n-1},...,X_0=i_0) = P(X_{n+1}=j|X_n=i)</script><h3 id="Definition-Transition-Matrix"><a href="#Definition-Transition-Matrix" class="headerlink" title="Definition (Transition Matrix)"></a>Definition (Transition Matrix)</h3><p>Let $X_0,X_1,X_2,…$ be a Markov Chain with state space $\{1,2,…,M\}$, and let<br>$q_{ij} = P(X_{n+1}=j|X_N=i)$ be the transition probability from state $i$ to state $j$. The $M \times M$ matrix $Q=(q_{ij})$ is called transition matrix of the chain.</p><p><img src="/2018/08/04/Stochastic-Process-11/sp11img1.jpg" align="justify"></p><h3 id="Definition-n-step-Transition-Probability"><a href="#Definition-n-step-Transition-Probability" class="headerlink" title="Definition (n-step Transition Probability)"></a>Definition (n-step Transition Probability)</h3><p>The n-step transition probability from $i$ to $j$ is the probability of being at $j$ exactly $n$ steps after being at $i$. Denote this by $q^{(n)}_{ij}$</p><script type="math/tex; mode=display">q^{(n)}_{ij} = P(X_n=j|X_0=i)=\sum_{k\in S} q^{(1)}_{kj} q^{(n-1)}_{ik}</script><p>which implies</p><script type="math/tex; mode=display">Q^n = Q^{n-1}\cdot Q</script><h3 id="Theorem-Chapman-Kolmogorov-Equation"><a href="#Theorem-Chapman-Kolmogorov-Equation" class="headerlink" title="Theorem (Chapman-Kolmogorov Equation)"></a>Theorem (Chapman-Kolmogorov Equation)</h3><script type="math/tex; mode=display">q_{ij}^{(n+m)} = \sum_{k\in S} q^{(n)}_{ik} q^{(m)}_{kj}</script><h1 id="First-Step-Analysis"><a href="#First-Step-Analysis" class="headerlink" title="First Step Analysis"></a>First Step Analysis</h1><h3 id="Example-Toss-A-Coin-till-HH-Appear"><a href="#Example-Toss-A-Coin-till-HH-Appear" class="headerlink" title="Example (Toss A Coin till HH Appear)"></a>Example (Toss A Coin till HH Appear)</h3><p>This problem can be formulated as a 3-state markov chain<br><img src="/2018/08/04/Stochastic-Process-11/sp11img2.jpg" align="justify"><br>The Transition graph is equivalent to<br><img src="/2018/08/04/Stochastic-Process-11/sp11img3.jpg" align="justify"><br>Let $e_s = E[\text{waiting time for HH|initial state = s}]$, then we have</p><script type="math/tex; mode=display">\begin{align}e_{Null} &= \frac{1}{2} (1+e_{Null})+\frac{1}{2} (1+e_H) \\e_H &= \frac{1}{2} (1+e_{HH}) + \frac{1}{2} (1+e_{Null})\\e_{HH} &= 0\end{align}</script><h3 id="Example-Toss-A-Coin-till-HTHT-Appear"><a href="#Example-Toss-A-Coin-till-HTHT-Appear" class="headerlink" title="Example (Toss A Coin till HTHT Appear)"></a>Example (Toss A Coin till HTHT Appear)</h3><p>Let see a more complicate case , this can be done by establish a linear equation</p><p><img src="/2018/08/04/Stochastic-Process-11/sp11img4.jpg" align="justify"></p><h1 id="Classification-of-States"><a href="#Classification-of-States" class="headerlink" title="Classification of States"></a>Classification of States</h1><h3 id="Definition-Recurrent-and-Transition-States"><a href="#Definition-Recurrent-and-Transition-States" class="headerlink" title="Definition (Recurrent and Transition States)"></a>Definition (Recurrent and Transition States)</h3><ul><li><strong>Recurrent</strong> State $i$ of Markov chain have the probability of $1$ eventually return to $i$</li><li><strong>Transient</strong> Other-wise, the state is Transient</li></ul><p><img src="/2018/08/04/Stochastic-Process-11/sp11img5.jpg" align="justify"></p><h3 id="Definition-Irreducible-amp-Reducible-Chain"><a href="#Definition-Irreducible-amp-Reducible-Chain" class="headerlink" title="Definition (Irreducible &amp; Reducible Chain)"></a>Definition (Irreducible &amp; Reducible Chain)</h3><ul><li><strong>Irreducible</strong> any state $i$ and $j$, possible to go from $i$ to $j$ in a finite number of steps.</li><li><strong>Reducible</strong> not irreducible</li></ul><h3 id="Theorem-Irreducible-Implies-All-States-Recurrent"><a href="#Theorem-Irreducible-Implies-All-States-Recurrent" class="headerlink" title="Theorem (Irreducible Implies All States Recurrent)"></a>Theorem (Irreducible Implies All States Recurrent)</h3><p>In an irreducible Markov Chain with a finite state space, all states are recurrent</p><h3 id="Example-Coupon-Collector"><a href="#Example-Coupon-Collector" class="headerlink" title="Example (Coupon Collector)"></a>Example (Coupon Collector)</h3><p>We want to collect all $C$ types coupons, Let $X_n$ be the number of distinct coupon types in our collection after $n$ attempts. Then $X_0,X_1,…$ is a Markov Chain on the state space $\{ 0,1,…,C \}$<br><img src="/2018/08/04/Stochastic-Process-11/sp11img6.jpg" align="justify"></p><h3 id="Definition-Period"><a href="#Definition-Period" class="headerlink" title="Definition (Period)"></a>Definition (Period)</h3><p>The <strong>period</strong> of state $i$ in a markov chain is the gcd of the possible numbers of steps can return to $i$ when starting at $i$. A state is called <strong>aperiodic</strong> if its period equals 1, and periodic otherwise.</p><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;i.i.d sequence of random variables: too restrictive assumption&lt;/p&gt;
&lt;p&gt;completely dependent among random variables: hard to analysis&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process-10</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-10/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-10/</id>
    <published>2018-08-04T08:05:13.000Z</published>
    <updated>2018-08-05T08:29:32.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Inference"><a href="#Bayesian-Inference" class="headerlink" title="Bayesian Inference"></a>Bayesian Inference</h1><h3 id="Bayesian-Inference-Framework"><a href="#Bayesian-Inference-Framework" class="headerlink" title="Bayesian Inference Framework"></a>Bayesian Inference Framework</h3><p>We aim to extract information about $\Theta$, based on observing a collection $X = (X_1,…,X_n)$</p><ul><li>Unknow $\Theta$<ul><li>treated as a random variable</li><li>prior distribution $p_\Theta$ or $f_\Theta$</li></ul></li><li>Observation $X$<ul><li>observation model $p_X|\Theta$ or $f_X|\Theta$</li></ul></li><li>Use appropriate version of the bayes rule to find $p_{\Theta|X}(\cdot|X=x)$</li></ul><h3 id="Principal-Bayesian-Estimation-Method"><a href="#Principal-Bayesian-Estimation-Method" class="headerlink" title="Principal Bayesian Estimation Method"></a>Principal Bayesian Estimation Method</h3><ul><li><strong>Maximum a posterior probability (MAP) rule</strong> Select the possible parameter with maximum conditional/posterior probability given the data</li><li><strong>Least mean squares (LMS) estimation</strong> Select an estimator/function of the data that minimizes the mean squared error between the parameterand its estimate<ul><li>$p_{\Theta|X}(\theta^*|x)=\mathop{max}_{\theta}p_{\Theta|X}(\theta|x)$</li></ul></li></ul><h3 id="Example-Inferring-the-Unknown-Bias-of-A-Coin"><a href="#Example-Inferring-the-Unknown-Bias-of-A-Coin" class="headerlink" title="Example (Inferring the Unknown Bias of A Coin)"></a>Example (Inferring the Unknown Bias of A Coin)</h3><p>We wish to estimate the probability of heads, denoted by $p$, suppose the prior is a beta density with $a,b$, that is , $p \sim Beta(a,b)$. We consider n independent tosses and let $X$ be the number of heads observed</p><p><strong>MAP Estimate</strong> The posterior PDF of $p$ has the form</p><script type="math/tex; mode=display">\begin{align}f(p|X=k) &= \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)} \\&= c\cdot p^{a+k-1} (1-p)^{b+(n-k)-1}\end{align}</script><p>Hence the posterior density is beta with parameters $a+k$ and $b+(n-k)$</p><p>By MAP rule, we select the eatimator as</p><script type="math/tex; mode=display">\hat{p}_{MAP} = \mathop{arg}\mathop{max}_p f(p|X=k) = \mathop{arg}\mathop{max}_p p^{a+k-1}(1-p)^{b+(n-k)-1}</script><p>Let $g(p) = p^{a+k-1}(1-p)^{b+(n-k)-1}$,then we have</p><script type="math/tex; mode=display">log(g(p)) = (a+k-1)\mathop{log}p + (b+(n-k)-1)\mathop{log}(1-p)</script><p>To find $p^*$ let</p><script type="math/tex; mode=display">\partial \frac{  \mathop{log}(g(p))} {\partial p}|_{p=p^*}  = 0</script><p>which yields</p><script type="math/tex; mode=display">\hat{p}_{MAP}  = \frac{a+k-1}{a+b+n-2}</script><p>When the prior distribution of $p$ is $Unif(0,1)$, that is $a=0,b=0$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k}{n}$</p><p><strong>LMS Estimate</strong> By Beta-Binomial conjugacy, $f(p|X=k) \sim Beta(a+k,b+n-k)$, the expectation of random variable $Y\sim Beta(a,b)$ is $E(Y) = \frac{a}{a+b}$, we have</p><script type="math/tex; mode=display">\hat{p}_{LMS} = E(p|X=k) = \frac{a+k}{(a+k)+(b+n-k)} = \frac{a+k}{a+b+k}</script><p>When the prior distribution of $p$ id $Unif(0,1)$, that is $a = 1,b = 1$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k+1}{n+2}$</p><hr><h1 id="Classical-Inference"><a href="#Classical-Inference" class="headerlink" title="Classical Inference"></a>Classical Inference</h1><ul><li>Classical Statistics: unknown constant $\theta$<ul><li>also for vectors $X$ and $\theta$ : $p_{X_1,…,X_n}(x_1,…,x_n; \theta_1,…,\theta_m)$</li><li>$p_X(x;\theta)$ are NOT conditional probabilities; $\theta$ is not random</li><li>mathematically: many models, one for each possible value of $\theta$</li></ul></li></ul><p>For example, the data observation model is $X\sim Binomial(n,\theta)$, Then under each possible value of $\theta$, the candidate model is</p><script type="math/tex; mode=display">p_X(x;\theta) = P(X=x;\theta) = \left( \begin{array}{c} n\\k \end{array} \right) \theta^x (1-\theta)^{n-x}</script><p>Classical Inference use the maximum likelihood to estimate the $\theta$</p><hr><h1 id="Sampling-Moments"><a href="#Sampling-Moments" class="headerlink" title="Sampling Moments"></a>Sampling Moments</h1><h3 id="Definition-Moments"><a href="#Definition-Moments" class="headerlink" title="Definition (Moments)"></a>Definition (Moments)</h3><p>Let $X$ be an r.v. with mean $\mu$ and variance $\sigma^2$, The $n^{th}$ moment of $X$ is $E(X^n)$, the $n^{th}$ central moment is $E((X-\mu)^n)$, and the $n^{th}$ standardized moment is $E((\frac{X-\mu}{\sigma})^n)$</p><h3 id="Definition-Sample-Moments"><a href="#Definition-Sample-Moments" class="headerlink" title="Definition (Sample Moments)"></a>Definition (Sample Moments)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables, the $k^{th}$ sample moment is the</p><script type="math/tex; mode=display">M_k = \frac{1}{n} \sum_{j=1}^n (X_j)^k</script><p>The sample mean $\bar{X}_n$ is the first sample moment:</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n} \sum_{j=1}^n X_j</script><h3 id="Theorem-Mean-and-Var-of-Sample-Mean"><a href="#Theorem-Mean-and-Var-of-Sample-Mean" class="headerlink" title="Theorem (Mean and Var of Sample Mean)"></a>Theorem (Mean and Var of Sample Mean)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. Then the sample mean $\bar{X}_n$ is unbiased for estimating $\mu$. That is</p><script type="math/tex; mode=display">E(\bar{X}_n) = \mu</script><p>The variance is</p><script type="math/tex; mode=display">Var(\bar{X}_n) = \frac{\sigma^2}{n}</script><h3 id="Definition-Sample-Variance"><a href="#Definition-Sample-Variance" class="headerlink" title="Definition (Sample Variance)"></a>Definition (Sample Variance)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables. The sample variance is the r.v.</p><script type="math/tex; mode=display">S_n^2 = \frac{1}{n-1} \sum_{j=1}^n (X_j-\bar{X}_n)^2</script><h3 id="Theorem-Unbiaseness-of-Sample-Var"><a href="#Theorem-Unbiaseness-of-Sample-Var" class="headerlink" title="Theorem (Unbiaseness of Sample Var)"></a>Theorem (Unbiaseness of Sample Var)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. The Sample Var $S_n^2$ is unbiased for estimating $\sigma^2$</p><script type="math/tex; mode=display">E(S_n^2) = \sigma^2</script><h3 id="Definition-Convergence-with-Probability"><a href="#Definition-Convergence-with-Probability" class="headerlink" title="Definition (Convergence with Probability)"></a>Definition (Convergence with Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges <strong>almost surely</strong> (a.s.) to the random variable $X$ as $n\rightarrow \infty$ and only if</p><script type="math/tex; mode=display">P(\left\{  \omega : X_n(\omega) \rightarrow X(\omega)\ as\ n\rightarrow \infty  \right\}) = 1</script><p>Notation: $X_n \xrightarrow{a.s.} X \text{ as }   n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img1.jpg" align="justify"></p><h3 id="Definition-Convergence-in-Probability"><a href="#Definition-Convergence-in-Probability" class="headerlink" title="Definition (Convergence in Probability)"></a>Definition (Convergence in Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges in probability to the random variable $X$ as $n\rightarrow \infty$ if and only if for every $\epsilon &gt;0$</p><script type="math/tex; mode=display">P(|X_n-X|>\epsilon) \rightarrow0 \ as \ n\rightarrow \infty</script><p>Notation: $X_n \xrightarrow{P} X \text{ as } n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img2.jpg" align="justify"></p><hr><h1 id="Law-of-large-Numbers"><a href="#Law-of-large-Numbers" class="headerlink" title="Law of large Numbers"></a>Law of large Numbers</h1><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v. with finite mean $\mu$ and finite variance $\sigma^2$. The samplw mean $\bar{X}_n$ is defined as</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n}\sum_{j=1}^n X_j</script><p>The Sample mean $\bar{X}_n$ is itself an r.v. with mean $\mu$ and variance $\sigma^2/n$</p><h3 id="Theorem-Strong-Law-of-Large-Numbers"><a href="#Theorem-Strong-Law-of-Large-Numbers" class="headerlink" title="Theorem (Strong Law of Large Numbers)"></a>Theorem (Strong Law of Large Numbers)</h3><p>The event $\bar{X}_n \rightarrow \mu$ has probability $1$</p><h3 id="Theorem-Weak-Law-of-Large-Numbers"><a href="#Theorem-Weak-Law-of-Large-Numbers" class="headerlink" title="Theorem (Weak Law of Large Numbers)"></a>Theorem (Weak Law of Large Numbers)</h3><p>For all $\epsilon &gt;0, P(|\bar{X}_n - \mu&gt;\epsilon) \rightarrow 0$ as $n\rightarrow \infty$</p><h3 id="Definition-Time-Average"><a href="#Definition-Time-Average" class="headerlink" title="Definition (Time Average)"></a>Definition (Time Average)</h3><script type="math/tex; mode=display">\bar{N}^{Time\ Average}(\omega)=\mathop{lim}_{t\rightarrow \infty} \frac{\int_0^t N(v,\omega)dv}{t}</script><h3 id="Definition-Ensemble-Average"><a href="#Definition-Ensemble-Average" class="headerlink" title="Definition (Ensemble Average)"></a>Definition (Ensemble Average)</h3><script type="math/tex; mode=display">\bar{N}^{Ensemble}(\omega)=\mathop{lim}_{t\rightarrow\infty} E[N(t)]=\sum_{i=0}^{\infty} i p_i</script><hr><h1 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h1><h3 id="Theorem-Central-Limit"><a href="#Theorem-Central-Limit" class="headerlink" title="Theorem (Central Limit)"></a>Theorem (Central Limit)</h3><p>As $n\rightarrow \infty$</p><script type="math/tex; mode=display">\sqrt{n}\left(  \frac{\bar{X}_n - \mu}{\sigma}  \right)  \rightarrow N(0,1)</script><p><strong>CLT Approximation</strong> For a large $n$, the distribution od $\bar{X}_n$ is approximately $N(\mu,\sigma^2/n)$</p><h3 id="Example-CLT-Example"><a href="#Example-CLT-Example" class="headerlink" title="Example (CLT Example)"></a>Example (CLT Example)</h3><p><img src="/2018/08/04/Stochastic-Process-10/sp10img3.jpg" align="justify"></p><ul><li><strong>Poisson Convergence to Normal</strong> Let $Y\sim Pois(n)$. Consider $Y$ as sum of $n$ i.i.d. $Pois(1)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(n,n)</script></li><li><strong>Gamma Convergence to Normal</strong> Let $Y\sim Gamma(n,\lambda)$. Consider $Y$ as sum of $n$ i.i.d. $Expo(\lambda)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(\frac{n}{\lambda},\frac{n}{\lambda^2})</script></li><li><strong>Binomial Convergence to Normal</strong> Let $Y\sim Bin(n,p)$. Consider $Y$ as sum of $n$ i.i.d. $Bern(p)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(np,np(1-p))</script></li></ul><h3 id="De-Moivre-Laplace-Approximation"><a href="#De-Moivre-Laplace-Approximation" class="headerlink" title="De Moivre-Laplace Approximation"></a>De Moivre-Laplace Approximation</h3><script type="math/tex; mode=display">\begin{align}P(Y=k) &= P(k-\frac{1}{2} < Y < k + \frac{1}{2}) \\       &\approx \Phi\left( \frac{k+\frac{1}{2} - np}{\sqrt{np(1-p)}} \right) -\Phi\left( \frac{k-\frac{1}{2} - np}{\sqrt{np(1-p)}} \right)\end{align}</script><ul><li><strong>Possion Approximation</strong> When $n$ is large and $p$ is samll</li><li><strong>Normal Approximation</strong> When $n$ is large and $p$ is around $1/2$</li></ul><hr><h1 id="Inequality"><a href="#Inequality" class="headerlink" title="Inequality"></a>Inequality</h1><h3 id="Basic-Inequalities"><a href="#Basic-Inequalities" class="headerlink" title="Basic Inequalities"></a>Basic Inequalities</h3><ul><li><p><strong>Cauchy-Schwarz Inequality</strong></p><script type="math/tex; mode=display">|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}</script></li><li><p><strong>Jensen’s Inequality</strong></p><ul><li>If $g$ is a convex function and $X$ is a r.v. then<script type="math/tex; mode=display">E(g(x)) \geq g(E(X))</script></li><li>If $g$ is a concave function<script type="math/tex; mode=display">E(g(x)) \leq g(E(X))</script></li></ul></li><li><p><strong>Markov’s Inequality</strong></p><script type="math/tex; mode=display">P(|X|\geq a) \leq \frac{E|X|}{a}</script></li><li><p><strong>Chebyshev’s Inequality</strong> Let $X$ have mean $\mu$ and variance $\sigma^2$</p><script type="math/tex; mode=display">P(|X-\mu|\geq a) \leq \frac{\sigma^2}{a^2}</script></li><li><p><strong>Chernoff’s Inequality</strong></p><script type="math/tex; mode=display">P(X\geq a) \leq \frac{E(e^{tX})}{e^{ta}}</script></li></ul><h3 id="Concentration-Inequalities"><a href="#Concentration-Inequalities" class="headerlink" title="Concentration Inequalities"></a>Concentration Inequalities</h3><p><strong>Hoeffding Lemma</strong> Let r.v. $X$ satisfy $E(X) = 0$ and $X\leq b$, Then for any $h&gt;0$</p><script type="math/tex; mode=display">E(e^{hX}) \leq e^{\frac{1}{8}h^2 (b-a)^2}</script><p><strong>Hoeffding Inequality</strong> Let the r.v. $X_1,X_2,…,X_n$ be independent, with $x_k\leq X_k\leq b_k$ for each $k$, Let $S_n = \sum_{k=1}^n X_k$. Then</p><script type="math/tex; mode=display">P(|S_n- \mu|\geq t) \leq 2e^{-\frac{2t^2}{\sum_{k=1}^n(b_k-a_k)^2}}</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Inference&quot;&gt;&lt;a href=&quot;#Bayesian-Inference&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Inference&quot;&gt;&lt;/a&gt;Bayesian Inference&lt;/h1&gt;&lt;h3 id=&quot;B
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conditional Expectation)</title>
    <link href="http://yoursite.com/2018/08/03/Stochastic-Process-8/"/>
    <id>http://yoursite.com/2018/08/03/Stochastic-Process-8/</id>
    <published>2018-08-03T12:33:40.000Z</published>
    <updated>2018-08-04T03:18:47.805Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Conditional-Expectation-Given-An-Event"><a href="#Conditional-Expectation-Given-An-Event" class="headerlink" title="Conditional Expectation Given An Event"></a>Conditional Expectation Given An Event</h1><p>If $Y$ is a discrete r.v.</p><script type="math/tex; mode=display">E(Y|A) = \sum_y P(Y=y|A)</script><p>If $Y$ is continuous r.v.</p><script type="math/tex; mode=display">E(Y|A) = \int_{-\infty}^{\infty} yf(y|A)dy</script><h3 id="Approximation"><a href="#Approximation" class="headerlink" title="Approximation"></a>Approximation</h3><p>Image a large number of $n$ of replication of experiments $y_1,…,y_n$</p><script type="math/tex; mode=display">E(Y)\approx \frac{1}{n}\sum_{j=1}^n y_j</script><p>If $I_j$ is the indicator of $A$ occurring</p><script type="math/tex; mode=display">E(Y|A) \approx \frac{\sum_{j=1}^n y_j I_j}{\sum_{j=1}^n I_j}</script><h3 id="Example-Life-Expectation"><a href="#Example-Life-Expectation" class="headerlink" title="Example (Life Expectation)"></a>Example (Life Expectation)</h3><p>Yang is 24 years old, he hear average life expectancy is $80$, Should he conclude he has 50 years of life left ?</p><p>Of Course not, cause he already live $24$ years and some people may die less than $24$</p><script type="math/tex; mode=display">E(T) < E(T|T\geq 30)</script><h3 id="Law-of-Total-Expectation"><a href="#Law-of-Total-Expectation" class="headerlink" title="Law of Total Expectation"></a>Law of Total Expectation</h3><p>Let $A_1,…,A_n$ be partition of a sample space, $Y$ be a random variable on sample space. Then</p><script type="math/tex; mode=display">E(Y) = \sum_{i=1}^n E(Y|A_i) P(A_i)</script><h3 id="Example-Geometric-Expectation-Redux"><a href="#Example-Geometric-Expectation-Redux" class="headerlink" title="Example (Geometric Expectation Redux)"></a>Example (Geometric Expectation Redux)</h3><p>Let $X\sim Geom(p)$, as the number of Tails before the first Heads in a sequence of coin flips with p. $p$ of head. To get $E(X)$ from sum of series, it also can be obtained in another way. We condition on the outcome of the first toss: if it lands heads, then $X$ is $0$ and we’re done ; if it lands Tails, then we wasted one toss and back to where we started by memorylessness Therefore</p><script type="math/tex; mode=display">\begin{align}E(X) &= E(X|\text{first toss }H)\cdot p + E(X|\text{first toss }T)\cdot q \\     &= 0 \cdot p + (1+E(X)) \cdot q\end{align}</script><p>which gives $E(X) = q/p$</p><h3 id="Example-Time-until-HH-vs-HT"><a href="#Example-Time-until-HH-vs-HT" class="headerlink" title="Example (Time until HH vs. HT)"></a>Example (Time until HH vs. HT)</h3><p>You toss a fair coin repeatedly. What is the expected number of tosses until the pattern HT/HH appears for the first times ?</p><p><strong>Times until HT</strong></p><ul><li>$W_{HT}$: number of tosses untill HT appears</li><li>$W_1$: waiting time for first H</li><li><p>$W_2$: additional waiting time for the first T</p><p>Then</p><p>$W_1\sim Fs(\frac{1}{2}),E[W_1] = 2$</p><p>$W_2\sim Fs(\frac{1}{2}),E[W_2] = 2$</p><script type="math/tex; mode=display">E[W_{HT}] = E[W_1+W_2]=E[W_1] + E[W_2] = 4</script></li></ul><p><strong>Times until HH</strong></p><script type="math/tex; mode=display">E[W_{HH}] = E[W_HH|\text{first toss }H]\cdot \frac{1}{2} + E[W_HH|\text{first toss }T]\cdot \frac{1}{2}</script><p>  where</p><script type="math/tex; mode=display">E[W_{HH}|\text{first toss }T] = 1 + E[W_{HH}]</script><p>  and</p><script type="math/tex; mode=display">\begin{align}  E[W_{HH}|\text{first toss }H] =& E[W_{HH}|\text{first toss }H, \text{second toss }H]\cdot \frac{1}{2} \\  &+ E[W_{HH}|\text{first toss }H, \text{second toss }T]\cdot \frac{1}{2} \\  =& 2\cdot \frac{1}{2} + (E[W_{HH}]+2)\cdot \frac{1}{2}  \end{align}</script><p>  Thus we get $E[W_{HH}] = 6$</p><p><strong><em>As we can see the above example use the memorylessness property of the Conditional Expectation of distribution, and construct target in both side to calculate the target</em></strong></p><hr><h1 id="Conditional-Expectation-Given-An-R-V"><a href="#Conditional-Expectation-Given-An-R-V" class="headerlink" title="Conditional Expectation Given An R.V."></a>Conditional Expectation Given An R.V.</h1><p>Let $g(x) = E(Y|X=x)$ Then the conditional expectation of $Y$ given $X$, denoted $E(Y|E)$ is defined to be the random variable $g(X)$</p><h3 id="Example-Stick-Length"><a href="#Example-Stick-Length" class="headerlink" title="Example (Stick Length)"></a>Example (Stick Length)</h3><p>Suppose we have a stick of length $1$ and break the stick at a point $X$ chosen uniformly at random. Given that $X=x$, we then choose another breakpoint $Y$ uniformly on the interval $[0,x]$, find $E(Y|X)$, and its mean and variance</p><script type="math/tex; mode=display">E(Y|X) = X/2</script><script type="math/tex; mode=display">E(E(Y|X)) = E(X/2) = \frac{1}{4}</script><script type="math/tex; mode=display">Var(E(Y|X)) = Var(X/2) = \frac{1}{48}</script><hr><h1 id="Properties-of-Conditional-Expectation"><a href="#Properties-of-Conditional-Expectation" class="headerlink" title="Properties of Conditional Expectation"></a>Properties of Conditional Expectation</h1><h3 id="Theorem-Dropping-independent"><a href="#Theorem-Dropping-independent" class="headerlink" title="Theorem (Dropping independent)"></a>Theorem (Dropping independent)</h3><p>If $X$ and $Y$ are independent, then $E(Y|X)=E(Y)$</p><h3 id="Taking-Out-What’s-Known"><a href="#Taking-Out-What’s-Known" class="headerlink" title="Taking Out What’s Known"></a>Taking Out What’s Known</h3><script type="math/tex; mode=display">E(h(X)Y|X) = h(X) E(Y|X)</script><h3 id="Theorem-Linearity"><a href="#Theorem-Linearity" class="headerlink" title="Theorem (Linearity)"></a>Theorem (Linearity)</h3><script type="math/tex; mode=display">E(Y_1+Y_2|X) = E(Y_1|X) + E(Y_2|X)</script><h3 id="Theorem-Adam’s-Law"><a href="#Theorem-Adam’s-Law" class="headerlink" title="Theorem (Adam’s Law)"></a>Theorem (Adam’s Law)</h3><p>For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><p><strong>Proof by LOTP</strong></p><p>  For $X$ discrete</p><script type="math/tex; mode=display">E(Y) = \sum_x E(Y|X=x) P(X=x)</script><p>  We let $E(Y|X=x) = g(x)$, then</p><script type="math/tex; mode=display">E(E(Y|X)) = E(g(X)) = \sum_x g(x) P(X=x) = \sum_x E(Y|X=x) P(X=x)</script><p>  So</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><h3 id="Theorem-Adam’s-Law-with-Extra-Conditioning"><a href="#Theorem-Adam’s-Law-with-Extra-Conditioning" class="headerlink" title="Theorem (Adam’s Law with Extra Conditioning)"></a>Theorem (Adam’s Law with Extra Conditioning)</h3><p>For any r.v.s $X,Y,Z$</p><script type="math/tex; mode=display">E(E(Y|X,Z)|Z) = E(Y|Z)</script><script type="math/tex; mode=display">E(E(X|Z,Y)|Y) = E(X|Y)</script><h3 id="Definition-Conditional-Variance"><a href="#Definition-Conditional-Variance" class="headerlink" title="Definition (Conditional Variance)"></a>Definition (Conditional Variance)</h3><script type="math/tex; mode=display">Var(Y|X) = E((Y-E(Y|X))^2|X)</script><p>this equivalent to</p><script type="math/tex; mode=display">Var(Y|X) = E(Y^2|X) - (E(Y|X))^2</script><h3 id="Theorem-Eve’s-Low"><a href="#Theorem-Eve’s-Low" class="headerlink" title="Theorem (Eve’s Low)"></a>Theorem (Eve’s Low)</h3><script type="math/tex; mode=display">Var(Y) = E(Var(Y|X)) + Var(E(Y|X))</script><h3 id="Example-Random-Sum"><a href="#Example-Random-Sum" class="headerlink" title="Example (Random Sum)"></a>Example (Random Sum)</h3><p>A store receives $N$ customers a day, $N$ is an r.v. with finite mean and variance. Let $X_j$ be the amount spend by the $j^{th}$ customer, $X_j$ has the mean $\mu$ and variance $\sigma^2$, $N$ and $X_j$ are independent of one another. Find the random sum $X = \sum_{j=1}^N X_j$ in terms of $\mu,\sigma^2,E(N),Var(N)$</p><p><strong>For E(X)</strong></p><script type="math/tex; mode=display">E(X|N) = E\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N E(X_j|N)= \sum_{j=1}^N E(X_j) = N\mu</script><p>  Finally, by Adam’s Law</p><script type="math/tex; mode=display">E(X) = E(E(X|N)) = E(N\mu) =\mu E(N)</script><p><strong>For Var(X)</strong></p><p>  We conditon on $N$ get $Var(X|N)$</p><script type="math/tex; mode=display">Var(X|N) = Var\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N Var(X_j|N) = \sum_{j=1}^N Var(X_j) = N\sigma^2</script><p>  Eve’s Law give the unconditional variance of $X$</p><script type="math/tex; mode=display">\begin{align}    Var(X) =& E(Var(X|N)) + Var(E(X|N))\\           =& E(N\sigma^2) + Var(N\mu) \\           =& \sigma^2 E(N) + \mu^2 Var(N)  \end{align}</script><hr><h1 id="Prediction-and-Estimation"><a href="#Prediction-and-Estimation" class="headerlink" title="Prediction and Estimation"></a>Prediction and Estimation</h1><h3 id="Theorem-Projection-Interpretation"><a href="#Theorem-Projection-Interpretation" class="headerlink" title="Theorem (Projection Interpretation)"></a>Theorem (Projection Interpretation)</h3><p>For any function $h$, the r.v. $Y-E(Y|X)$ is Uncorrelated with $h(X)$: $Cov(Y-E(Y|X),h(X)) = 0$, equivalently</p><script type="math/tex; mode=display">E((Y-E(Y|X))h(X)) = 0</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Conditional-Expectation-Given-An-Event&quot;&gt;&lt;a href=&quot;#Conditional-Expectation-Given-An-Event&quot; class=&quot;headerlink&quot; title=&quot;Conditional Expe
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>PGM-0</title>
    <link href="http://yoursite.com/2018/08/01/PGM-0/"/>
    <id>http://yoursite.com/2018/08/01/PGM-0/</id>
    <published>2018-08-01T02:28:56.000Z</published>
    <updated>2018-10-03T08:49:13.552Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h1><h3 id="Discriminative-amp-Generative-Model"><a href="#Discriminative-amp-Generative-Model" class="headerlink" title="Discriminative &amp; Generative Model"></a>Discriminative &amp; Generative Model</h3><ul><li>判别模型 Discriminative Model，又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)</li><li>生成模型 Generative Model，又叫产生式模型。估计的是联合概率分布（joint probability distribution）</li></ul><p>对于这两个模型，我们用一个例子就能很好的解释：假设我们有一些数据 $(x,y)$，数据只有四组：$(1,0), (1,0), (2,0), (2,1)$</p><p><strong>Generative Model</strong> :</p><div class="table-container"><table><thead><tr><th>p(x,y)</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1/2</td><td>0</td></tr><tr><td>x=1</td><td>1/4</td><td>1/4</td></tr></tbody></table></div><p><strong>Discriminative Model</strong> :</p><div class="table-container"><table><thead><tr><th>p(x\</th><th>y)</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1</td><td>0</td></tr><tr><td>x=1</td><td>1/2</td><td>1/2</td></tr></tbody></table></div><h1 id="PGM"><a href="#PGM" class="headerlink" title="PGM"></a>PGM</h1><p>概率图有这么几种推断方式</p><p>概率图的任务通俗的讲有两个</p><ul><li>Querying：给定或者不给定条件概率下，计算特定变量的概率称为：<strong>Inference</strong></li><li>Estimation：当模型某部分是未知的时候，通过数据 $D$ 来推断出未知的模型，这个称为：<strong>learning</strong></li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>Querying 任务通常来说有三种</p><p><strong>Likelihood</strong> : Likelihood is calculated to get the conditional probability of a different subset of variables conditioned based on Evidence $\mathbf{E} = \{ X_{k+1},…,X_n \}$, Evidence is the unknown variables, so we need eliminate the unsure variables to get the specific lieklihood.</p><script type="math/tex; mode=display">P(\mathbf{e}) = \sum_{x_1}\cdots \sum_{x_k} P(x_1,...,x_k,\mathbf{e})</script><p><strong>Conditional Probability</strong> : The conditional probability distribution of some query nodes conditioned on an<br>evidence.</p><script type="math/tex; mode=display">P(X|\mathbf{e}) = \frac{P(X,\mathbf{e})}{P(\mathbf{e})}= \frac{P(X,\mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}</script><p>Let $\mathbf{Y}$ be a subset of all domain variables $\mathbf{X} = \{ \mathbf{Y},\mathbf{Z} \}$, $\mathbf{Z}$ is the set of variables under elimination.</p><script type="math/tex; mode=display">P(\mathbf{Y}|\mathbf{e}) = \sum_{z}P(\mathbf{Y},\mathbf{Z}=z|\mathbf{e})</script><p><strong>Most Probable Assignment</strong> : In this query, we are interested in finding only one set of values for the query variables that maximize the given conditional probability instead of finding the entire distribution.</p><script type="math/tex; mode=display">MPA(\mathbf{Y}|\mathbf{e})=\mathop{arg}\mathop{max}_{y\in \mathbf{Y}}P(y|\mathbf{e})=\mathop{arg}\mathop{max}_{y\in \mathbf{Y}}\sum_{z} P(y,z|\mathbf{e})</script><p>Inference 算法可以分为两种</p><ul><li><p>Exact inference（准确推断）</p><ul><li>Elimination</li><li>Message-passing<ul><li>sum-product</li><li>belief propagation</li></ul></li><li>Junction Tree</li></ul></li><li><p>Approximate inference（近似推断）a</p><ul><li>Variational algorithms<ul><li>Loopy belief propagation</li><li>Mean field approximation</li></ul></li></ul><p>一般 Inference 最简单的方法是 Elimination 和 brute force，但是由于概率图模型往往有一些比较特别的结构，我们可以用 Message Passing 的算法来做</p></li></ul><hr><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>根据观测数据，来推理模型中的参数</p><p>在做 inference 的时候，我们希望能将有向图和无向图结合起来，比如，原先的子节点的父节点们，我们希望将他们</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Basis&quot;&gt;&lt;a href=&quot;#Basis&quot; class=&quot;headerlink&quot; title=&quot;Basis&quot;&gt;&lt;/a&gt;Basis&lt;/h1&gt;&lt;h3 id=&quot;Discriminative-amp-Generative-Model&quot;&gt;&lt;a href=&quot;#Discri
      
    
    </summary>
    
    
      <category term="PGM" scheme="http://yoursite.com/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conjugacy &amp; Bayesian)</title>
    <link href="http://yoursite.com/2018/07/31/Stochastic-Process-7/"/>
    <id>http://yoursite.com/2018/07/31/Stochastic-Process-7/</id>
    <published>2018-07-31T02:35:18.000Z</published>
    <updated>2018-08-03T05:40:01.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Beta-Binomial-Distribution"><a href="#Beta-Binomial-Distribution" class="headerlink" title="Beta-Binomial Distribution"></a>Beta-Binomial Distribution</h1><h3 id="Definition-Beta-Distribution"><a href="#Definition-Beta-Distribution" class="headerlink" title="Definition (Beta Distribution)"></a>Definition (Beta Distribution)</h3><p>An r.v. $X$ is said to have <strong>Beta distribution</strong> with parameters $a$ and $b$, if its PDF is</p><script type="math/tex; mode=display">f(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1},0<x<1</script><p>where $\beta(a,b)$ is constant to make PDF integrate to 1, We write this as $X\sim Beta(a,b)$</p><p>By varying the values of $a$ and $b$, we get PDFs with a variety of shapes<br><img src="/2018/07/31/Stochastic-Process-7/sp7img1.jpg" align="justify"></p><p><em>Beta Distribution is the generalization of uniform distribution while $a=b=1$</em></p><p>The Beta is a flexible family of continuous distributions on (0,1), and has many stories. One is Beta r.v. often used to represent an unknown probability. <strong>we can use Beta to put probabilities on unknown probabilities</strong> If a parameter $p$ satisfies $0&lt;p&lt;1$, we can assume the prior distribution of $p$ is Beta(a,b)</p><h3 id="Beta-Integral"><a href="#Beta-Integral" class="headerlink" title="Beta Integral"></a>Beta Integral</h3><p>One inportant issue to analyse the Beta Distribution is the Integral</p><script type="math/tex; mode=display">\beta(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1}dx</script><h3 id="Bayes’-billiards"><a href="#Bayes’-billiards" class="headerlink" title="Bayes’ billiards"></a>Bayes’ billiards</h3><p> It’s hard to get the reuslt directly from calculus, the left and right sides of the above formulation can be connected by one event $P(X=k)$</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) x^k(1-x)^{n-k}dx = \frac{1}{n+1}</script><p><strong>Left side Story :</strong> Having $n+1$ balls , $n$ white and $1$ gray. Randomly throw each ball onto the interval $[0,1]$, so the position of balls are i.i.d. $Unif(0,1)$. Let $X$ be the number of white balls to the left of the gray ball.</p><p>To get the probability of the event $X=k$, we use LOTP. Conditioning on the position of gray ball, call it $B$, Conditional on $B=p$, the number of the white ball landing to the left of $p$ has $Bin(n,p)$ distribution, The PDF of $B$ is $f(p) =1$ since $B\sim Unif(0,1)$</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|B=p)f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-k)^{n-k}dp</script><p><strong>Right side Story :</strong> Having $n+1$ balls, all white, randomly throw onto unit interval; then choose one ball at random and paint it gray. Again, let $X$ be the number of white balls to the left of gray ball. By symmetry, any one of the $n+1$ balls is equally likely to be painted gray, then</p><script type="math/tex; mode=display">P(X=k) = \frac{1}{n+1}</script><p>$X$ has the same distribution, then</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}dp= \frac{1}{n+1}</script><p>Using the result, we are capable to calculate the $\beta(a,b)$ by substituting $a-1$ for $k$ and $b-1$ for $n-k$</p><script type="math/tex; mode=display">\beta(a,b) = \frac{1}{(a+b-1) \left( \begin{array}{c} a+b-2\\a-1 \end{array} \right)} = \frac{(a-1)!(b-1)!}{(a+b-1)!}</script><p>For a r.v. $X\sim Beta(a,b)$, The Expectation</p><script type="math/tex; mode=display">\begin{align}E(X) &= \int_0^1 xf(x)dx= \int_0^1 x\cdot \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}dx=\frac{1}{\beta(a,b)}\int_0^1 x^a(1-x)^{b-1}dx \\&= \frac{1}{\beta(a,b)}\cdot \beta(a+1,b) = \frac{\frac{a!(b-1)!}{(a+b)!}}{\frac{(a-1)!(b-1)!}{(a+b-1)!}} = \frac{a}{a+b}\end{align}</script><h3 id="Beta-Binomial-Conjugacy"><a href="#Beta-Binomial-Conjugacy" class="headerlink" title="Beta-Binomial Conjugacy"></a>Beta-Binomial Conjugacy</h3><p> Now let’s see the connection between Beta distribution and Binomial distribution, the relation we call it Conjugacy.</p><p>We have a coin lands head with p. $p$, and we dont know what %p% is. Our goal is to infer the value of $p$ after observing the outcomes of n tosses of the coin.<br><strong>Bayesian Inference</strong></p><ul><li>Treat all unknown probability $p$ as r.v. and give $p$ a distribution</li><li>Above is called <strong>prior distribution</strong>, it reflects out uncertainty about the ture value of $p$ before observing</li><li>After experiment and data are gathered, prior distribution is updated using Baye’s rule,; This yields the <strong>posterior distribution</strong>, which reflects the new beliefs about $p$</li><li>Specifically<ul><li><strong>prior distribution</strong> $f(p)$</li><li><strong>posterior distribution</strong> $f(p|X=n)$</li></ul></li></ul><p>Suppose the prior distribution on $p$ is Beta distribution. Let $p\sim Beta(a,b)$ for known constants $a$ and $b$, $X$ be the number of heads in $n$ tosses of the coin. Conditional on knowing ture value of $p$ then</p><script type="math/tex; mode=display">X|p \sim Bin(n,p)</script><p>We use the Bayes rule. Letting $f(p)$ be the prior distribution and $f(p|X=k)$ be the posterior distribution after observing $k$ heads</p><script type="math/tex; mode=display">f(p|X=k) = \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}\cdot \frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}</script><p>The denominator $P(X=k)$ is the marginal PMF of $X$, is given by</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|p) f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}f(p)dp</script><p>If $a=b=1$ , $P(X=k) = 1/(n+1)$, but it not seem easy to find $P(X=k)$ in general , Are we stuck ?</p><p>Actually, is much easier than it appears at first, the conditional PDF $f(p|X=k)$ is a function of $p$, so everything doesn’t depend on $p$ is just a constant. After dropping constants gives</p><script type="math/tex; mode=display">f(p|X=k) \propto p^{a+k-1}(1-p)^{b+n-k-1}</script><p>which is the $Beta(a+k,b+n-k)$ PDF.Therefore the posterior distribution of $p$ is</p><script type="math/tex; mode=display">p|X = k\sim Beta(a+k,b+n-k)</script><p>The posterior distribution of $p$ after observing $X=k$ is still a Beta distribution!</p><p>We say <em>Beta is the Conjugate prior of the Binomial</em></p><ul><li>We add the observed successes $k$ to the first parameter</li><li>We add the observed successes $k-n$ to the second parameter</li><li>$a$ and $b$ have a concrete interpretation in this context<ul><li>$a$ as the number of prior successes in earlier experiments</li><li>$b$ as the number of prior failures in earlier experiments</li></ul></li></ul><h3 id="Mean-vs-Bayesian-Average"><a href="#Mean-vs-Bayesian-Average" class="headerlink" title="Mean vs Bayesian Average"></a>Mean vs Bayesian Average</h3><ul><li>Mean: $\frac{k}{n}$</li><li>Bayesian Average: $E(p|X=k) = \frac{a+k}{a+b+n}$</li></ul><hr><h1 id="Dirichlet-Multinomial-Distribution"><a href="#Dirichlet-Multinomial-Distribution" class="headerlink" title="Dirichlet-Multinomial Distribution"></a>Dirichlet-Multinomial Distribution</h1><p>$n$ objects are independently placed into one of $k$ categories, with probability of $p_j$ to category j, and $\sum_{j=1}^k p_j = 1$. Let $X_i$ be the number of objects in category $i$, $X_1 + … + X_n = n$. Then $X = (X_1,…,X_k)$ is said to have <strong>Multinomial distribution</strong> with parameters $n$ and $\mathbf{p} = (p_1,…,p_k)$, write as $\mathbf{X} \sim Mult_k(n,\mathbf{p})$</p><h3 id="Theorem-Multinomial-Joint-PMF"><a href="#Theorem-Multinomial-Joint-PMF" class="headerlink" title="Theorem (Multinomial Joint PMF)"></a>Theorem (Multinomial Joint PMF)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then the joint PMF of $\mathbf{X}$ is</p><script type="math/tex; mode=display">P(X_1=n_1,...,X_k=n_k) = \frac{n!}{n_1!n_2!...n_k!}\cdot p_1^{n_1}p_2^{n_2}\dotsb p_k^{n_k}</script><h3 id="Theorem-Multinomial-Marginals"><a href="#Theorem-Multinomial-Marginals" class="headerlink" title="Theorem (Multinomial Marginals)"></a>Theorem (Multinomial Marginals)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then $X_j \sim Bin(n,p_j)$</p><h3 id="Theorem-Multinomial-Lumping"><a href="#Theorem-Multinomial-Lumping" class="headerlink" title="Theorem (Multinomial Lumping)"></a>Theorem (Multinomial Lumping)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then for distinct $i$ and $j$, $X_i+X_j\sim Bin(n,p_i+p_j)$</p><script type="math/tex; mode=display">(X_1+X_2,X_3,...,X_k)\sim Mult_k(n,((p_1+p_2),p_3,...,p_n))</script><p><img src="/2018/07/31/Stochastic-Process-7/sp7img2.jpg" style=" width:500px;"></p><h3 id="Theorem-Multinomial-Conditioning"><a href="#Theorem-Multinomial-Conditioning" class="headerlink" title="Theorem (Multinomial Conditioning)"></a>Theorem (Multinomial Conditioning)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then</p><script type="math/tex; mode=display">(X_2,...,X_k)|X_1=n_1\sim Mult_k(n-n_1,(p_2^{\prime},...,p_k^{\prime}))</script><p>where $p_j^{\prime} = p_j/(p_2+\dotsb +p_k)$</p><h3 id="Theorem-Covariance-in-A-Multinomial"><a href="#Theorem-Covariance-in-A-Multinomial" class="headerlink" title="Theorem (Covariance in A Multinomial)"></a>Theorem (Covariance in A Multinomial)</h3><p>Let $X_1,…,X_k\sim Mult_k(n,\mathbf{p})$, where $\mathbf{p} = (p_1,…,p_k)$.</p><script type="math/tex; mode=display">Cov(X_i,X_j) = -np_ip_j</script><h3 id="Definition-Dirichlet-Distribution"><a href="#Definition-Dirichlet-Distribution" class="headerlink" title="Definition (Dirichlet Distribution)"></a>Definition (Dirichlet Distribution)</h3><p>Dirichlet distribution is parameterized by a vector $\mathbf{\alpha}$ of positive real numbers. The PDF is:</p><script type="math/tex; mode=display">f(p_1,p_2,...,p_k;\alpha_1,\alpha_2,...,\alpha_k)=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\prod_{i=1}^k\Gamma(\alpha_i)}\prod_{i=1}^kp_i^{\alpha_i-1}</script><p>where $p_1 + … + p_k = 1$</p><h3 id="Dirichlet-Multinomial-Conjugacy"><a href="#Dirichlet-Multinomial-Conjugacy" class="headerlink" title="Dirichlet-Multinomial Conjugacy"></a>Dirichlet-Multinomial Conjugacy</h3><p>Assume we already have the Multinomial distribution $\mathbf{X}\sim Mult_k(n,\mathbf{p})$. The prior distribution of $\mathbf{p}=(p_1,…,p_k)$ is a Dirichlet distribution, i.e. $\mathbf{p}\sim Dir(\alpha)$. Denote $\mathbf{X} = (X_1,…,X_k)$, then</p><script type="math/tex; mode=display">\mathbf{X}|\mathbf{p}\sim Mult_k(n,\mathbf{p})</script><p>Let $f(\mathbf{p})$ to be the prior distribution of $\mathbf{p}$. The observations of the experiment is $\mathbf{N} = (n_1,…,n_k)$ then</p><script type="math/tex; mode=display">\begin{align}f(\mathbf{p}|\mathbf{X}=\mathbf{N}) &= \frac{P(\mathbf{X}=\mathbf{N}|\mathbf{p})f(\mathbf{p})}{P(\mathbf{X}=\mathbf{N})} \\&= \frac{\frac{n!}{n_1!\dotsb n_k!}p_1^{n_1}\dotsb p_k^{n_k}\cdot \frac{\Gamma(\sum_{i=1}^k \alpha_i)}{\prod_{i=1}^k \Gamma(\alpha_i)}\prod_{i=1}^k p_i^{\alpha_i -1}}{P(\mathbf{X}= \mathbf{N})} \\&\propto p_1^{n_1+\alpha_1 - 1}\dotsb p_k^{n_k+\alpha_k - 1} \\&\sim Dir(\mathbf{\alpha} + \mathbf{N})\end{align}</script><p>Thus we can see that</p><script type="math/tex; mode=display">\text{prior}\ Dir(\mathbf{\alpha}) \rightarrow \text{posterior}\ Dir(\mathbf{\alpha} + \mathbf{N})</script><script type="math/tex; mode=display">\alpha_i \rightarrow \alpha_i + n_i</script><p>We can prove that</p><script type="math/tex; mode=display">E[p_i|\mathbf{X} = \mathbf{N}] = \frac{\alpha_i + n_i}{\sum_{i=1}^k(\alpha_i + n_i)}</script><hr><h1 id="Bayesian-Average"><a href="#Bayesian-Average" class="headerlink" title="Bayesian Average"></a>Bayesian Average</h1><p>One application for Bayesian Average is in <strong>Rating System</strong>, usually the customers will rate the movies in 5 star.</p><p>This will rose a problem, which One to Choose</p><ul><li>$5$ Average rating movie A by $1$ voter</li><li>$4.9998$ Average rating movie B by $1400010123$ voter (of course this One)</li></ul><p>To use the Bayesian estimation to compute the posterior probability for star ratings, we must use a <strong>joint distribution</strong>, the random variable is a categorical distribution with probability follows:</p><script type="math/tex; mode=display">p_1+p_2+p_3+p_4+p_5 = 1</script><h5 id="Multinomial-Distribution"><a href="#Multinomial-Distribution" class="headerlink" title="Multinomial Distribution"></a>Multinomial Distribution</h5><p>Let $O$ be the event of movie rating, we compute the posterior probability with $N$ observations for five categories with corresponding numbers $K_1,K_2,K_3,K_4,K_5$ as follows:</p><script type="math/tex; mode=display">Pr(O|p_1,p_2,p_3,p_4,p_5)\propto p_1^{K_1}p_2^{K_2}p_3^{K_3}p_4^{K_4}p_5^{K_5}</script><p>where $K_1+…+K_5 = N$</p><h5 id="Dirichlet-Distribution-Prior"><a href="#Dirichlet-Distribution-Prior" class="headerlink" title="Dirichlet Distribution: Prior"></a>Dirichlet Distribution: Prior</h5><script type="math/tex; mode=display">Pr(p_1,p_2,p_3,p_4,p_5|O) \propto \prod_{j=1}^5 p_j^{K_j+\alpha_j^0 - 1}</script><p>After considering the new votes we can update the distribution of the $\mathbf{p}$ by</p><script type="math/tex; mode=display">\alpha_j^1 = K_j +\alpha_k^0</script><h5 id="Expected-Average"><a href="#Expected-Average" class="headerlink" title="Expected Average"></a>Expected Average</h5><p>What we need is the average rating given posterior in the shape of our Dirichlet distribution:</p><script type="math/tex; mode=display">E(p_1+2p_2+3p_3+4p_4+5p_5|O) = \sum_{i=1}^5 iE(p_i|O)</script><p>According to</p><script type="math/tex; mode=display">E(p_i|O) = \frac{\alpha_i^1}{\sum_{j=1}^5 \alpha_j^1}</script><p>We have</p><script type="math/tex; mode=display">\sum_{i=1}^5 iE(p_i|O) = \frac{\sum_{i=1}^5 i\alpha_i^0 + \sum_{i=1}^5 i K_i}{\sum_{j=1}^5 \alpha_j^0 +N}</script><h3 id="Bayesian-Average-Rating"><a href="#Bayesian-Average-Rating" class="headerlink" title="Bayesian Average Rating"></a>Bayesian Average Rating</h3><p>The final formulation can be express as</p><script type="math/tex; mode=display">\text{Bayes Average Rating} = \frac{C\cdot m +\sum(ratings)}{C+N}</script><ul><li>N: The number of ratings</li><li>m: a prior for the average of rating scores</li><li>C: a prior for the number of rating scores</li></ul><hr><h1 id="Gamma-Distribution"><a href="#Gamma-Distribution" class="headerlink" title="Gamma Distribution"></a>Gamma Distribution</h1><h3 id="Definition-Gamma-Function"><a href="#Definition-Gamma-Function" class="headerlink" title="Definition (Gamma Function)"></a>Definition (Gamma Function)</h3><p>The Gamma function $\Gamma$ is defined by</p><script type="math/tex; mode=display">\Gamma(a) = \int_0^{\infty}x^ae^{-x}\frac{dx}{x}</script><p>Earlier Beta distribution can represent an unknown probability of success cause its support is $(0,1)$. The Gamma distribution can represent an unknown rate in a Poisson process because its support is $(0,\infty)$</p><h3 id="Property-of-Gamma-Function"><a href="#Property-of-Gamma-Function" class="headerlink" title="Property of Gamma Function"></a>Property of Gamma Function</h3><ul><li>$\Gamma(a+1) = a\Gamma(a)$</li><li>$\Gamma(n) = (n-1)!$ if $n$ is a integer</li></ul><h3 id="Definition-Gamma-Distribution"><a href="#Definition-Gamma-Distribution" class="headerlink" title="Definition (Gamma Distribution)"></a>Definition (Gamma Distribution)</h3><p>An r.v. $Y$ is said to have <strong>Gamma distribution</strong> with parameters $a&gt;0, \lambda&gt;0$, if its PDF is</p><script type="math/tex; mode=display">f(y) = \frac{1}{\Gamma(a)}(\lambda y)^a e^{-\lambda y} \frac{1}{y}, y>0</script><p>Write $Y\sim Gamma(a,\lambda)$. <strong>Gamma distribution is a generalization of exponential distribution</strong> when $a=1$</p><h3 id="PDF-of-Gamma-distribution"><a href="#PDF-of-Gamma-distribution" class="headerlink" title="PDF of Gamma distribution"></a>PDF of Gamma distribution</h3><p><img src="/2018/07/31/Stochastic-Process-7/sp7img3.jpg" align="justify"></p><h3 id="Moments-of-Gamma-Distribution"><a href="#Moments-of-Gamma-Distribution" class="headerlink" title="Moments of Gamma Distribution"></a>Moments of Gamma Distribution</h3><script type="math/tex; mode=display">E[X] = \frac{a}{\lambda}, Var(X) = \frac{a}{\lambda^2}</script><h3 id="Theorem-Gamma-Convolution-of-Exponential"><a href="#Theorem-Gamma-Convolution-of-Exponential" class="headerlink" title="Theorem (Gamma: Convolution of Exponential)"></a>Theorem (Gamma: Convolution of Exponential)</h3><p>Let $X_1,…,X_n$ be i.i.d. $Expo(\lambda)$ , Then</p><script type="math/tex; mode=display">X_1+\dotsb +X_n \sim Gamma(n,\lambda)</script><h3 id="Beta-Gamma-Connection"><a href="#Beta-Gamma-Connection" class="headerlink" title="Beta-Gamma Connection"></a>Beta-Gamma Connection</h3><p>We have independent Gamma r.v.s $X$ and $Y$ with the same rate $\lambda$</p><ul><li>$X+Y$ had Gamma distribution</li><li>$\frac{X}{X+Y}$ has Beta distribution</li></ul><h3 id="Binomial-amp-Poisson-amp-Gamma"><a href="#Binomial-amp-Poisson-amp-Gamma" class="headerlink" title="Binomial &amp; Poisson &amp; Gamma"></a>Binomial &amp; Poisson &amp; Gamma</h3><ul><li>The PMF of $Poisson(X=k|\lambda)$ is $P(X=k|\lambda) = \frac{\lambda^ke^{-\lambda}}{k!}$</li><li>The PDF of $X\sim Gamma(a,1)$ is $f_X(x) = \frac{1}{\Gamma(a)}a^{a-1}e^{-x}$. Given $a=k+1$ we have<script type="math/tex; mode=display">P(X=x|a=k+1,1)=\frac{x^{k+1}}{\Gamma(k+1)}e^{-x}= \frac{x^k}{k!}e^{-x}</script></li><li>For a r.v. $X\sim Bin(n,p)$, we have<script type="math/tex; mode=display">P(X\leq k)=\frac{n!}{k!(n-k-1)!}\int_p^1 t^k(1-t)^{n-k-1}dt</script>Let $t=\frac{x}{n}$, then<script type="math/tex; mode=display">P(X\leq k) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n})dx</script>It follows that<script type="math/tex; mode=display">Bin(X\leq k|n,p) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n}dx)</script></li><li>Let $\lambda = np$. We fix $\lambda$, and let $n\rightarrow \infty$, then<script type="math/tex; mode=display">Bin(n,p) \rightarrow Posi(\lambda)</script></li><li>When $\lambda \rightarrow 0$, we have<script type="math/tex; mode=display">1=\mathop{lim}_{\lambda \rightarrow 0} \int_{\lambda}^{\infty} \frac{\lambda^k e^{-x}}{k!}dx = \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx=\int_0^{\infty} Gamma(k+1,1)dx</script></li><li>$1=\int_0^{\infty} \frac{x^ke^{-x}}{k!}dx\Rightarrow k!=\int_0^{\infty}x^k e^{-x}dx$</li><li>Because $Pois(X\leq k|\lambda) = \int_{\lambda}^{\infty}\frac{x^k e^{-x}}{k!} dx$ we have<script type="math/tex; mode=display">Pois(X\leq k|\lambda) + \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx = 1</script></li></ul><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Beta-Binomial-Distribution&quot;&gt;&lt;a href=&quot;#Beta-Binomial-Distribution&quot; class=&quot;headerlink&quot; title=&quot;Beta-Binomial Distribution&quot;&gt;&lt;/a&gt;Beta-Bin
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Multivariate)</title>
    <link href="http://yoursite.com/2018/07/29/Stochastic-Process-6/"/>
    <id>http://yoursite.com/2018/07/29/Stochastic-Process-6/</id>
    <published>2018-07-29T00:41:09.000Z</published>
    <updated>2018-08-02T01:39:52.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Discrete-Multivariate-R-V-s"><a href="#Discrete-Multivariate-R-V-s" class="headerlink" title="Discrete Multivariate R.V.s"></a>Discrete Multivariate R.V.s</h1><p><strong>Definition (Joint CDF)</strong> The Joint CDf of r.v.s $X$ and $Y$ is the function $F_{X,Y}$ given by</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = P(X\leq x,Y\leq y)</script><p><strong>Definition (Joint PMF)</strong> The Joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X,Y}$ given by</p><script type="math/tex; mode=display">p_{X,Y}(x,y) = P(X=x,Y=y)</script><p><strong>Definition (Marginal PMF)</strong> For discrete r.v.s $X$ and $Y$, Marginal PMF of $X$ is</p><script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x,Y=y)</script><p><strong>Definition (Conditional PMF)</strong> For discrete r.v.s $X$ and $Y$, the Conditional PMF of $X$ given $Y=y$ is</p><script type="math/tex; mode=display">P_{X|Y}(x|y) = P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}</script><p><strong>Definition (Independence of Discrete R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x) F_Y(y)</script><p>for all x and y also equivalent to the condition</p><script type="math/tex; mode=display">P(Y=y|X=x) = P(Y=y)</script><hr><h1 id="Continuous-Multivariate-R-V-s"><a href="#Continuous-Multivariate-R-V-s" class="headerlink" title="Continuous Multivariate R.V.s"></a>Continuous Multivariate R.V.s</h1><p><strong>Definition (Joint PDF)</strong> If $X$ and $Y$ are continuous with joint CDF $F_{X,Y}$ then</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y)</script><p><strong>Definition (Marginal PDF)</strong> If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ then</p><script type="math/tex; mode=display">f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy</script><p><strong>Definition (Conditional PDF)</strong> For continuous r.v.s. $X$ and $Y$ with joint PDF $f_{X,Y}$ the Conditional PDF of $Y$ given $X=x$ is</p><script type="math/tex; mode=display">f_{Y|X}(y|x)= \frac{f_{X,Y}(x,y)}{f_X(x)}</script><p><strong>Definition (Independence of Continuous R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x)F_Y(y)</script><p>If $X$ and  $Y$ are continuous with joint PDF $f_{X,Y}$</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = f_X(x) f_Y(y)</script><p><strong>Theorem (2D LOTUS)</strong> Let g be a function from $R^2$ to $R$</p><p>If $X$ and $Y$ are discrete</p><script type="math/tex; mode=display">E(g(X,Y)) = \sum_x \sum_y g(x,y) P(X=x,Y=y)</script><p>If $X$ and $Y$ are continuous</p><script type="math/tex; mode=display">E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f_{X,Y}(x,y) dxdy</script><h3 id="General-Bayes’-Rule"><a href="#General-Bayes’-Rule" class="headerlink" title="General Bayes’ Rule"></a>General Bayes’ Rule</h3><p><img src="/2018/07/29/Stochastic-Process-6/img1.jpg" align="justify"></p><hr><h1 id="Convariance-and-Correlation"><a href="#Convariance-and-Correlation" class="headerlink" title="Convariance and Correlation"></a>Convariance and Correlation</h1><p><strong>Covariance</strong></p><ul><li>Measure a tendency of two r.v.s $X\&amp;Y$ to go up or down together</li><li>Positive Covariance: $X$ go up, $Y$ tends go up</li><li>Negative Covariance: $X$ go up, $Y$ tends go down</li></ul><p><strong>Definition (Covariance)</strong> The covariance between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Cov(X,Y) = E((X-EX)(Y-EY))=E(XY)-E(X)E(Y)</script><p><strong>Theorem (Uncorrelated)</strong> If $X$ and $Y$ are independent, then they are Uncorrelated($Cov(X,Y)=0$)</p><h3 id="Properties-of-Covariance"><a href="#Properties-of-Covariance" class="headerlink" title="Properties of Covariance"></a>Properties of Covariance</h3><ul><li>$Cov(X,X) = Var(X)$</li><li>$Cov(X,Y) = Cov(Y,X)$</li><li>$Cov(X,c) = 0$</li><li>$Cov(a\cdot X,Y) = a\cdot Cov(X,Y)$</li><li>$Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$</li><li>$Cov(X+Y,W+Z) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$</li><li>$Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$</li><li>For n r.v.s $X_1,\dotsb ,X_n$ <script type="math/tex">Var(X_1+\dotsb +X_n)=Var(X_a)+\dotsb+Var(X_n)+2\sum_{i<j}Cov(X_i,Y_j)</script></li></ul><p><strong>Definition (Correlation)</strong> The Correlation between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}</script><p><em>Shifting and Scaling $X$ and $Y$ has no effect on correlation</em><br><img src="/2018/07/29/Stochastic-Process-6/img2.jpg" align="justify"></p><p><strong>Theorem (Correlation Bounds)</strong> For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">-1\leq Corr(X,Y) \leq 1</script><hr><h1 id="Change-of-Variables"><a href="#Change-of-Variables" class="headerlink" title="Change of Variables"></a>Change of Variables</h1><p><strong>Theorem (Change of Variables in One Dimension)</strong> Let $X$ be a continuous r.v. with PDF $f_X$, and let $Y = g(X)$, where $g$ is differentiable and strictly increasing. Then the PDF of $Y$ is given by</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left|  \frac{dx}{dy} \right|</script><p>where $x = g^{-1}(y)$</p><p><strong><em>Proof</em></strong>:</p><script type="math/tex; mode=display">F_Y(y) = P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_X(g^{-1}(y))=F_X(x)</script><p>Then result obtained By the chain rule</p><p><strong>Theorem (Change of Variables)</strong> Let $X = (X_1,…,X_n)$ be a continuous random vector with joint PDF $f_X(x)$ and $Y=g(X)$, $g$ is an invertible function from $R^n$ to $R^n$ then $\frac{\partial \mathbf{x}}{\partial \mathbf{y}}$ form a <strong><em>Jacobian  matrix</em></strong></p><script type="math/tex; mode=display">\frac{\partial \mathbf{x}}{\partial \mathbf{y}} =\left( \begin{array}{cccc}\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \dotsb & \frac{\partial x_1}{\partial y_n} \\\vdots & \vdots & & \vdots \\\frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \dotsb & \frac{\partial x_n}{\partial y_n} \\\end{array} \right)</script><p>Then the joint PDF of $Y$ is</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left| \frac{\partial \mathbf{x}}{\partial \mathbf{y}} \right|</script><hr><h1 id="Convolutions"><a href="#Convolutions" class="headerlink" title="Convolutions"></a>Convolutions</h1><p><strong>Theorem (Convolution Sums and Integrals)</strong><br>If $X$ and $Y$ are independent discrete r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  P(T=t) =& \sum_x P(Y = t-x) P(X=x) \\         =& \sum_y P(X= t-y) P(Y=y)\end{align}</script><p>If $X$ and $Y$ are independent continuous r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  f_T(t)  =& \int_{-\infty}^{\infty} f_Y(t-x) f_X(x) dx \\          =& \int_{-\infty}^{\infty} f_X(t-y) f_Y(y) dy\end{align}</script><hr><h1 id="Order-Statistics"><a href="#Order-Statistics" class="headerlink" title="Order Statistics"></a>Order Statistics</h1><p><strong>Definition (Order Statistics)</strong> For r.v.s $X_1,X_2,…,X_n$ the order statistics sre the random variables $X_{(1)},…,X_{(2)}$, where</p><ul><li>$X_{(1)}  = min (X_1,…,X_n)$</li><li>$X_{(2)}$ is the $2^{nd}$ of $X_1,…,X_n$</li><li>$\vdots$</li><li>$X_{n} = max(X_1,…,X_n)$</li></ul><p>The order statistics are dependent, for example , if $X_{(1)} = 100$, then $X_{(n)}$ is forced to be $\geq 100$</p><p>We foucs on the case $X_1,…,X_n$ are i.i.d continuous r.v.s, with CDF $F$ and PDF $f$</p><p><strong>Theorem (CDF of Order Statistics)</strong>  Let $X_1,…,X_n$ be i.i.d continuous r.v.s with CDF F, Then the CDF of the $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">P(X_{(j)}\leq x)=\sum_{k=j}^n\left( \begin{array}{c} n\\k  \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><p><strong><em>Proof:</em></strong></p><hr><p>Let’s start with a specical case when $j=n, X_{(n)}=max(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(n)}} (x) &= P[max(X_1,...,X_n)\leq x] \\&=P(X_1\leq  x)\dotsb P(X_n\leq x) \\&=[F(x)]^n\end{align}</script><hr><p>Then, consider another special case when $j=1, X_{(1)} = min(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(1)}} (x) &= P[min(X_1,...,X_n)\leq x] \\&=1 - P(X_1>  x)\dotsb P(X_n> x) \\&=1-[1-F(x)]^n\end{align}</script><p>The result here can be rewrite as $\sum_{k=1}^n\left( \begin{array}{c} n\\k \end{array} \right) F(x)^k (1-F(x))^{n-k}$</p><p>This result can be obtained by expand $[F(x) + 1 -F(x)]^n$</p><hr><p>Finally, let’s consider more general case where $1&lt;j&lt;n, X_{(j)}\leq x$, this means at least $j$ of $\{X_i \}$ fall to the left of $x$</p><p>Denote $N$ as the nunber of $X_i$ landing to the left of $x$. $X_i$ lands to the left of $x$ w.p. $P(X_i\leq x) = F(x)$. Then $N\sim Bin(n,F(x))$</p><script type="math/tex; mode=display">P(X_{(j)}\leq x) = P(N\geq j=\sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><hr><p><strong>Theorem (PDF of Order Statistic)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with CDF $F$ and PDF $f$. Then the marginal PDF of $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">f_{X_{(j)}} (x) = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) f(x) F(x)^{j-1} (1-F(x))^{n-j}</script><p><strong>Theorem (Joint PDF)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with PDF $f$, Then the joint PDF of all order statistics is</p><script type="math/tex; mode=display">f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n) = n! \prod_{i=1}^n f(x_i), x_1<x_2<\dotsb <x_n</script><p><strong>Example 1(Order Statistics of Uniforms)</strong> $U_1,U_2,…,U_n$ are i.i.d. $Unif(0,1)$ r.v.s with CDF F and PDF $f$</p><p>For $0\leq x\leq 1$ ,$f(x) = 1$ , $F(x) = x$, Then</p><script type="math/tex; mode=display">f_{U_{(j)}} = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) x^{j-1} (1-x)^{n-j}</script><script type="math/tex; mode=display">F_{U_{(j)}}(x) = \sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right)x^k (1-x)^{n-k} = \int_0^x f_{U_{(j)}} (t) dt = \frac{n!}{(j-1)!(n-j)!}\int_0^x t^{j-1}(1-t)^{n-j}dt</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Discrete-Multivariate-R-V-s&quot;&gt;&lt;a href=&quot;#Discrete-Multivariate-R-V-s&quot; class=&quot;headerlink&quot; title=&quot;Discrete Multivariate R.V.s&quot;&gt;&lt;/a&gt;Discr
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
</feed>
