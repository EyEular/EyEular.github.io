<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EyEular</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-04T08:05:19.362Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Eulring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stochastic-Process-11</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-11/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-11/</id>
    <published>2018-08-04T08:05:19.000Z</published>
    <updated>2018-08-04T08:05:19.362Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Stochastic-Process-10</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-10/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-10/</id>
    <published>2018-08-04T08:05:13.000Z</published>
    <updated>2018-08-05T08:29:32.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Inference"><a href="#Bayesian-Inference" class="headerlink" title="Bayesian Inference"></a>Bayesian Inference</h1><h3 id="Bayesian-Inference-Framework"><a href="#Bayesian-Inference-Framework" class="headerlink" title="Bayesian Inference Framework"></a>Bayesian Inference Framework</h3><p>We aim to extract information about $\Theta$, based on observing a collection $X = (X_1,…,X_n)$</p><ul><li>Unknow $\Theta$<ul><li>treated as a random variable</li><li>prior distribution $p_\Theta$ or $f_\Theta$</li></ul></li><li>Observation $X$<ul><li>observation model $p_X|\Theta$ or $f_X|\Theta$</li></ul></li><li>Use appropriate version of the bayes rule to find $p_{\Theta|X}(\cdot|X=x)$</li></ul><h3 id="Principal-Bayesian-Estimation-Method"><a href="#Principal-Bayesian-Estimation-Method" class="headerlink" title="Principal Bayesian Estimation Method"></a>Principal Bayesian Estimation Method</h3><ul><li><strong>Maximum a posterior probability (MAP) rule</strong> Select the possible parameter with maximum conditional/posterior probability given the data</li><li><strong>Least mean squares (LMS) estimation</strong> Select an estimator/function of the data that minimizes the mean squared error between the parameterand its estimate<ul><li>$p_{\Theta|X}(\theta^*|x)=\mathop{max}_{\theta}p_{\Theta|X}(\theta|x)$</li></ul></li></ul><h3 id="Example-Inferring-the-Unknown-Bias-of-A-Coin"><a href="#Example-Inferring-the-Unknown-Bias-of-A-Coin" class="headerlink" title="Example (Inferring the Unknown Bias of A Coin)"></a>Example (Inferring the Unknown Bias of A Coin)</h3><p>We wish to estimate the probability of heads, denoted by $p$, suppose the prior is a beta density with $a,b$, that is , $p \sim Beta(a,b)$. We consider n independent tosses and let $X$ be the number of heads observed</p><p><strong>MAP Estimate</strong> The posterior PDF of $p$ has the form</p><script type="math/tex; mode=display">\begin{align}f(p|X=k) &= \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)} \\&= c\cdot p^{a+k-1} (1-p)^{b+(n-k)-1}\end{align}</script><p>Hence the posterior density is beta with parameters $a+k$ and $b+(n-k)$</p><p>By MAP rule, we select the eatimator as</p><script type="math/tex; mode=display">\hat{p}_{MAP} = \mathop{arg}\mathop{max}_p f(p|X=k) = \mathop{arg}\mathop{max}_p p^{a+k-1}(1-p)^{b+(n-k)-1}</script><p>Let $g(p) = p^{a+k-1}(1-p)^{b+(n-k)-1}$,then we have</p><script type="math/tex; mode=display">log(g(p)) = (a+k-1)\mathop{log}p + (b+(n-k)-1)\mathop{log}(1-p)</script><p>To find $p^*$ let</p><script type="math/tex; mode=display">\partial \frac{  \mathop{log}(g(p))} {\partial p}|_{p=p^*}  = 0</script><p>which yields</p><script type="math/tex; mode=display">\hat{p}_{MAP}  = \frac{a+k-1}{a+b+n-2}</script><p>When the prior distribution of $p$ is $Unif(0,1)$, that is $a=0,b=0$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k}{n}$</p><p><strong>LMS Estimate</strong> By Beta-Binomial conjugacy, $f(p|X=k) \sim Beta(a+k,b+n-k)$, the expectation of random variable $Y\sim Beta(a,b)$ is $E(Y) = \frac{a}{a+b}$, we have</p><script type="math/tex; mode=display">\hat{p}_{LMS} = E(p|X=k) = \frac{a+k}{(a+k)+(b+n-k)} = \frac{a+k}{a+b+k}</script><p>When the prior distribution of $p$ id $Unif(0,1)$, that is $a = 1,b = 1$, the estimator under MAP rule is $\hat{p}_{MAP} = \frac{k+1}{n+2}$</p><hr><h1 id="Classical-Inference"><a href="#Classical-Inference" class="headerlink" title="Classical Inference"></a>Classical Inference</h1><ul><li>Classical Statistics: unknown constant $\theta$<ul><li>also for vectors $X$ and $\theta$ : $p_{X_1,…,X_n}(x_1,…,x_n; \theta_1,…,\theta_m)$</li><li>$p_X(x;\theta)$ are NOT conditional probabilities; $\theta$ is not random</li><li>mathematically: many models, one for each possible value of $\theta$</li></ul></li></ul><p>For example, the data observation model is $X\sim Binomial(n,\theta)$, Then under each possible value of $\theta$, the candidate model is</p><script type="math/tex; mode=display">p_X(x;\theta) = P(X=x;\theta) = \left( \begin{array}{c} n\\k \end{array} \right) \theta^x (1-\theta)^{n-x}</script><p>Classical Inference use the maximum likelihood to estimate the $\theta$</p><hr><h1 id="Sampling-Moments"><a href="#Sampling-Moments" class="headerlink" title="Sampling Moments"></a>Sampling Moments</h1><h3 id="Definition-Moments"><a href="#Definition-Moments" class="headerlink" title="Definition (Moments)"></a>Definition (Moments)</h3><p>Let $X$ be an r.v. with mean $\mu$ and variance $\sigma^2$, The $n^{th}$ moment of $X$ is $E(X^n)$, the $n^{th}$ central moment is $E((X-\mu)^n)$, and the $n^{th}$ standardized moment is $E((\frac{X-\mu}{\sigma})^n)$</p><h3 id="Definition-Sample-Moments"><a href="#Definition-Sample-Moments" class="headerlink" title="Definition (Sample Moments)"></a>Definition (Sample Moments)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables, the $k^{th}$ sample moment is the</p><script type="math/tex; mode=display">M_k = \frac{1}{n} \sum_{j=1}^n (X_j)^k</script><p>The sample mean $\bar{X}_n$ is the first sample moment:</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n} \sum_{j=1}^n X_j</script><h3 id="Theorem-Mean-and-Var-of-Sample-Mean"><a href="#Theorem-Mean-and-Var-of-Sample-Mean" class="headerlink" title="Theorem (Mean and Var of Sample Mean)"></a>Theorem (Mean and Var of Sample Mean)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. Then the sample mean $\bar{X}_n$ is unbiased for estimating $\mu$. That is</p><script type="math/tex; mode=display">E(\bar{X}_n) = \mu</script><p>The variance is</p><script type="math/tex; mode=display">Var(\bar{X}_n) = \frac{\sigma^2}{n}</script><h3 id="Definition-Sample-Variance"><a href="#Definition-Sample-Variance" class="headerlink" title="Definition (Sample Variance)"></a>Definition (Sample Variance)</h3><p>Let $X_1,…,X_n$ be i.i.d. random variables. The sample variance is the r.v.</p><script type="math/tex; mode=display">S_n^2 = \frac{1}{n-1} \sum_{j=1}^n (X_j-\bar{X}_n)^2</script><h3 id="Theorem-Unbiaseness-of-Sample-Var"><a href="#Theorem-Unbiaseness-of-Sample-Var" class="headerlink" title="Theorem (Unbiaseness of Sample Var)"></a>Theorem (Unbiaseness of Sample Var)</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v.s with unknown mean $\mu$ and variance $\sigma^2$. The Sample Var $S_n^2$ is unbiased for estimating $\sigma^2$</p><script type="math/tex; mode=display">E(S_n^2) = \sigma^2</script><h3 id="Definition-Convergence-with-Probability"><a href="#Definition-Convergence-with-Probability" class="headerlink" title="Definition (Convergence with Probability)"></a>Definition (Convergence with Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges <strong>almost surely</strong> (a.s.) to the random variable $X$ as $n\rightarrow \infty$ and only if</p><script type="math/tex; mode=display">P(\left\{  \omega : X_n(\omega) \rightarrow X(\omega)\ as\ n\rightarrow \infty  \right\}) = 1</script><p>Notation: $X_n \xrightarrow{a.s.} X \text{ as }   n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img1.jpg" align="justify"></p><h3 id="Definition-Convergence-in-Probability"><a href="#Definition-Convergence-in-Probability" class="headerlink" title="Definition (Convergence in Probability)"></a>Definition (Convergence in Probability)</h3><p>Let $X_1,X_2,…$ be random variables. $X_n$ converges in probability to the random variable $X$ as $n\rightarrow \infty$ if and only if for every $\epsilon &gt;0$</p><script type="math/tex; mode=display">P(|X_n-X|>\epsilon) \rightarrow0 \ as \ n\rightarrow \infty</script><p>Notation: $X_n \xrightarrow{P} X \text{ as } n\rightarrow \infty$</p><p><strong>Example</strong><br><img src="/2018/08/04/Stochastic-Process-10/sp10img2.jpg" align="justify"></p><hr><h1 id="Law-of-large-Numbers"><a href="#Law-of-large-Numbers" class="headerlink" title="Law of large Numbers"></a>Law of large Numbers</h1><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Let $X_1,…,X_n$ be i.i.d. r.v. with finite mean $\mu$ and finite variance $\sigma^2$. The samplw mean $\bar{X}_n$ is defined as</p><script type="math/tex; mode=display">\bar{X}_n = \frac{1}{n}\sum_{j=1}^n X_j</script><p>The Sample mean $\bar{X}_n$ is itself an r.v. with mean $\mu$ and variance $\sigma^2/n$</p><h3 id="Theorem-Strong-Law-of-Large-Numbers"><a href="#Theorem-Strong-Law-of-Large-Numbers" class="headerlink" title="Theorem (Strong Law of Large Numbers)"></a>Theorem (Strong Law of Large Numbers)</h3><p>The event $\bar{X}_n \rightarrow \mu$ has probability $1$</p><h3 id="Theorem-Weak-Law-of-Large-Numbers"><a href="#Theorem-Weak-Law-of-Large-Numbers" class="headerlink" title="Theorem (Weak Law of Large Numbers)"></a>Theorem (Weak Law of Large Numbers)</h3><p>For all $\epsilon &gt;0, P(|\bar{X}_n - \mu&gt;\epsilon) \rightarrow 0$ as $n\rightarrow \infty$</p><h3 id="Definition-Time-Average"><a href="#Definition-Time-Average" class="headerlink" title="Definition (Time Average)"></a>Definition (Time Average)</h3><script type="math/tex; mode=display">\bar{N}^{Time\ Average}(\omega)=\mathop{lim}_{t\rightarrow \infty} \frac{\int_0^t N(v,\omega)dv}{t}</script><h3 id="Definition-Ensemble-Average"><a href="#Definition-Ensemble-Average" class="headerlink" title="Definition (Ensemble Average)"></a>Definition (Ensemble Average)</h3><script type="math/tex; mode=display">\bar{N}^{Ensemble}(\omega)=\mathop{lim}_{t\rightarrow\infty} E[N(t)]=\sum_{i=0}^{\infty} i p_i</script><hr><h1 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h1><h3 id="Theorem-Central-Limit"><a href="#Theorem-Central-Limit" class="headerlink" title="Theorem (Central Limit)"></a>Theorem (Central Limit)</h3><p>As $n\rightarrow \infty$</p><script type="math/tex; mode=display">\sqrt{n}\left(  \frac{\bar{X}_n - \mu}{\sigma}  \right)  \rightarrow N(0,1)</script><p><strong>CLT Approximation</strong> For a large $n$, the distribution od $\bar{X}_n$ is approximately $N(\mu,\sigma^2/n)$</p><h3 id="Example-CLT-Example"><a href="#Example-CLT-Example" class="headerlink" title="Example (CLT Example)"></a>Example (CLT Example)</h3><p><img src="/2018/08/04/Stochastic-Process-10/sp10img3.jpg" align="justify"></p><ul><li><strong>Poisson Convergence to Normal</strong> Let $Y\sim Pois(n)$. Consider $Y$ as sum of $n$ i.i.d. $Pois(1)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(n,n)</script></li><li><strong>Gamma Convergence to Normal</strong> Let $Y\sim Gamma(n,\lambda)$. Consider $Y$ as sum of $n$ i.i.d. $Expo(\lambda)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(\frac{n}{\lambda},\frac{n}{\lambda^2})</script></li><li><strong>Binomial Convergence to Normal</strong> Let $Y\sim Bin(n,p)$. Consider $Y$ as sum of $n$ i.i.d. $Bern(p)$ r.v.s. For large $n$:<script type="math/tex; mode=display">Y\sim N(np,np(1-p))</script></li></ul><h3 id="De-Moivre-Laplace-Approximation"><a href="#De-Moivre-Laplace-Approximation" class="headerlink" title="De Moivre-Laplace Approximation"></a>De Moivre-Laplace Approximation</h3><script type="math/tex; mode=display">\begin{align}P(Y=k) &= P(k-\frac{1}{2} < Y < k + \frac{1}{2}) \\       &\approx \Phi\left( \frac{k+\frac{1}{2} - np}{\sqrt{np(1-p)}} \right) -\Phi\left( \frac{k-\frac{1}{2} - np}{\sqrt{np(1-p)}} \right)\end{align}</script><ul><li><strong>Possion Approximation</strong> When $n$ is large and $p$ is samll</li><li><strong>Normal Approximation</strong> When $n$ is large and $p$ is around $1/2$</li></ul><hr><h1 id="Inequality"><a href="#Inequality" class="headerlink" title="Inequality"></a>Inequality</h1><h3 id="Basic-Inequalities"><a href="#Basic-Inequalities" class="headerlink" title="Basic Inequalities"></a>Basic Inequalities</h3><ul><li><p><strong>Cauchy-Schwarz Inequality</strong></p><script type="math/tex; mode=display">|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}</script></li><li><p><strong>Jensen’s Inequality</strong></p><ul><li>If $g$ is a convex function and $X$ is a r.v. then<script type="math/tex; mode=display">E(g(x)) \geq g(E(X))</script></li><li>If $g$ is a concave function<script type="math/tex; mode=display">E(g(x)) \leq g(E(X))</script></li></ul></li><li><p><strong>Markov’s Inequality</strong></p><script type="math/tex; mode=display">P(|X|\geq a) \leq \frac{E|X|}{a}</script></li><li><p><strong>Chebyshev’s Inequality</strong> Let $X$ have mean $\mu$ and variance $\sigma^2$</p><script type="math/tex; mode=display">P(|X-\mu|\geq a) \leq \frac{\sigma^2}{a^2}</script></li><li><p><strong>Chernoff’s Inequality</strong></p><script type="math/tex; mode=display">P(X\geq a) \leq \frac{E(e^{tX})}{e^{ta}}</script></li></ul><h3 id="Concentration-Inequalities"><a href="#Concentration-Inequalities" class="headerlink" title="Concentration Inequalities"></a>Concentration Inequalities</h3><p><strong>Hoeffding Lemma</strong> Let r.v. $X$ satisfy $E(X) = 0$ and $X\leq b$, Then for any $h&gt;0$</p><script type="math/tex; mode=display">E(e^{hX}) \leq e^{\frac{1}{8}h^2 (b-a)^2}</script><p><strong>Hoeffding Inequality</strong> Let the r.v. $X_1,X_2,…,X_n$ be independent, with $x_k\leq X_k\leq b_k$ for each $k$, Let $S_n = \sum_{k=1}^n X_k$. Then</p><script type="math/tex; mode=display">P(|S_n- \mu|\geq t) \leq 2e^{-\frac{2t^2}{\sum_{k=1}^n(b_k-a_k)^2}}</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Inference&quot;&gt;&lt;a href=&quot;#Bayesian-Inference&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Inference&quot;&gt;&lt;/a&gt;Bayesian Inference&lt;/h1&gt;&lt;h3 id=&quot;B
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Martingale)</title>
    <link href="http://yoursite.com/2018/08/04/Stochastic-Process-9/"/>
    <id>http://yoursite.com/2018/08/04/Stochastic-Process-9/</id>
    <published>2018-08-04T07:16:54.000Z</published>
    <updated>2018-08-04T07:46:59.326Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Stopping-Time"><a href="#Stopping-Time" class="headerlink" title="Stopping Time"></a>Stopping Time</h1><h3 id="Theorem-Mean-of-Random-Sum-is-Sum-of-Means"><a href="#Theorem-Mean-of-Random-Sum-is-Sum-of-Means" class="headerlink" title="Theorem (Mean of Random Sum is Sum of Means)"></a>Theorem (Mean of Random Sum is Sum of Means)</h3><p>Let $X_1,X_2,…,X_n$ be i.i.d. random variables and $N\geq 0$ independent of all $X_n$ Then</p><script type="math/tex; mode=display">E\left( \sum_{n=1}^N X_n \right) = E(N) E(X_1)</script><p>  <strong>Proof</strong> using Adam’s Law</p><script type="math/tex; mode=display">E\left(  \sum_{n=1}^N X_n \right) = E\left( E\left( \sum_{n=1}^N X_n|N \right) \right) = E(NE(X_1|N))</script><p>  Since $N$ is independent of $X_1$, we have</p><script type="math/tex; mode=display">E\left(  \sum_{n=1}^N X_n \right) = E(NE(X_1)) = E(X_1)E(N)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Stopping-Time&quot;&gt;&lt;a href=&quot;#Stopping-Time&quot; class=&quot;headerlink&quot; title=&quot;Stopping Time&quot;&gt;&lt;/a&gt;Stopping Time&lt;/h1&gt;&lt;h3 id=&quot;Theorem-Mean-of-Rando
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conditional Expectation)</title>
    <link href="http://yoursite.com/2018/08/03/Stochastic-Process-8/"/>
    <id>http://yoursite.com/2018/08/03/Stochastic-Process-8/</id>
    <published>2018-08-03T12:33:40.000Z</published>
    <updated>2018-08-04T03:18:47.805Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Conditional-Expectation-Given-An-Event"><a href="#Conditional-Expectation-Given-An-Event" class="headerlink" title="Conditional Expectation Given An Event"></a>Conditional Expectation Given An Event</h1><p>If $Y$ is a discrete r.v.</p><script type="math/tex; mode=display">E(Y|A) = \sum_y P(Y=y|A)</script><p>If $Y$ is continuous r.v.</p><script type="math/tex; mode=display">E(Y|A) = \int_{-\infty}^{\infty} yf(y|A)dy</script><h3 id="Approximation"><a href="#Approximation" class="headerlink" title="Approximation"></a>Approximation</h3><p>Image a large number of $n$ of replication of experiments $y_1,…,y_n$</p><script type="math/tex; mode=display">E(Y)\approx \frac{1}{n}\sum_{j=1}^n y_j</script><p>If $I_j$ is the indicator of $A$ occurring</p><script type="math/tex; mode=display">E(Y|A) \approx \frac{\sum_{j=1}^n y_j I_j}{\sum_{j=1}^n I_j}</script><h3 id="Example-Life-Expectation"><a href="#Example-Life-Expectation" class="headerlink" title="Example (Life Expectation)"></a>Example (Life Expectation)</h3><p>Yang is 24 years old, he hear average life expectancy is $80$, Should he conclude he has 50 years of life left ?</p><p>Of Course not, cause he already live $24$ years and some people may die less than $24$</p><script type="math/tex; mode=display">E(T) < E(T|T\geq 30)</script><h3 id="Law-of-Total-Expectation"><a href="#Law-of-Total-Expectation" class="headerlink" title="Law of Total Expectation"></a>Law of Total Expectation</h3><p>Let $A_1,…,A_n$ be partition of a sample space, $Y$ be a random variable on sample space. Then</p><script type="math/tex; mode=display">E(Y) = \sum_{i=1}^n E(Y|A_i) P(A_i)</script><h3 id="Example-Geometric-Expectation-Redux"><a href="#Example-Geometric-Expectation-Redux" class="headerlink" title="Example (Geometric Expectation Redux)"></a>Example (Geometric Expectation Redux)</h3><p>Let $X\sim Geom(p)$, as the number of Tails before the first Heads in a sequence of coin flips with p. $p$ of head. To get $E(X)$ from sum of series, it also can be obtained in another way. We condition on the outcome of the first toss: if it lands heads, then $X$ is $0$ and we’re done ; if it lands Tails, then we wasted one toss and back to where we started by memorylessness Therefore</p><script type="math/tex; mode=display">\begin{align}E(X) &= E(X|\text{first toss }H)\cdot p + E(X|\text{first toss }T)\cdot q \\     &= 0 \cdot p + (1+E(X)) \cdot q\end{align}</script><p>which gives $E(X) = q/p$</p><h3 id="Example-Time-until-HH-vs-HT"><a href="#Example-Time-until-HH-vs-HT" class="headerlink" title="Example (Time until HH vs. HT)"></a>Example (Time until HH vs. HT)</h3><p>You toss a fair coin repeatedly. What is the expected number of tosses until the pattern HT/HH appears for the first times ?</p><p><strong>Times until HT</strong></p><ul><li>$W_{HT}$: number of tosses untill HT appears</li><li>$W_1$: waiting time for first H</li><li><p>$W_2$: additional waiting time for the first T</p><p>Then</p><p>$W_1\sim Fs(\frac{1}{2}),E[W_1] = 2$</p><p>$W_2\sim Fs(\frac{1}{2}),E[W_2] = 2$</p><script type="math/tex; mode=display">E[W_{HT}] = E[W_1+W_2]=E[W_1] + E[W_2] = 4</script></li></ul><p><strong>Times until HH</strong></p><script type="math/tex; mode=display">E[W_{HH}] = E[W_HH|\text{first toss }H]\cdot \frac{1}{2} + E[W_HH|\text{first toss }T]\cdot \frac{1}{2}</script><p>  where</p><script type="math/tex; mode=display">E[W_{HH}|\text{first toss }T] = 1 + E[W_{HH}]</script><p>  and</p><script type="math/tex; mode=display">\begin{align}  E[W_{HH}|\text{first toss }H] =& E[W_{HH}|\text{first toss }H, \text{second toss }H]\cdot \frac{1}{2} \\  &+ E[W_{HH}|\text{first toss }H, \text{second toss }T]\cdot \frac{1}{2} \\  =& 2\cdot \frac{1}{2} + (E[W_{HH}]+2)\cdot \frac{1}{2}  \end{align}</script><p>  Thus we get $E[W_{HH}] = 6$</p><p><strong><em>As we can see the above example use the memorylessness property of the Conditional Expectation of distribution, and construct target in both side to calculate the target</em></strong></p><hr><h1 id="Conditional-Expectation-Given-An-R-V"><a href="#Conditional-Expectation-Given-An-R-V" class="headerlink" title="Conditional Expectation Given An R.V."></a>Conditional Expectation Given An R.V.</h1><p>Let $g(x) = E(Y|X=x)$ Then the conditional expectation of $Y$ given $X$, denoted $E(Y|E)$ is defined to be the random variable $g(X)$</p><h3 id="Example-Stick-Length"><a href="#Example-Stick-Length" class="headerlink" title="Example (Stick Length)"></a>Example (Stick Length)</h3><p>Suppose we have a stick of length $1$ and break the stick at a point $X$ chosen uniformly at random. Given that $X=x$, we then choose another breakpoint $Y$ uniformly on the interval $[0,x]$, find $E(Y|X)$, and its mean and variance</p><script type="math/tex; mode=display">E(Y|X) = X/2</script><script type="math/tex; mode=display">E(E(Y|X)) = E(X/2) = \frac{1}{4}</script><script type="math/tex; mode=display">Var(E(Y|X)) = Var(X/2) = \frac{1}{48}</script><hr><h1 id="Properties-of-Conditional-Expectation"><a href="#Properties-of-Conditional-Expectation" class="headerlink" title="Properties of Conditional Expectation"></a>Properties of Conditional Expectation</h1><h3 id="Theorem-Dropping-independent"><a href="#Theorem-Dropping-independent" class="headerlink" title="Theorem (Dropping independent)"></a>Theorem (Dropping independent)</h3><p>If $X$ and $Y$ are independent, then $E(Y|X)=E(Y)$</p><h3 id="Taking-Out-What’s-Known"><a href="#Taking-Out-What’s-Known" class="headerlink" title="Taking Out What’s Known"></a>Taking Out What’s Known</h3><script type="math/tex; mode=display">E(h(X)Y|X) = h(X) E(Y|X)</script><h3 id="Theorem-Linearity"><a href="#Theorem-Linearity" class="headerlink" title="Theorem (Linearity)"></a>Theorem (Linearity)</h3><script type="math/tex; mode=display">E(Y_1+Y_2|X) = E(Y_1|X) + E(Y_2|X)</script><h3 id="Theorem-Adam’s-Law"><a href="#Theorem-Adam’s-Law" class="headerlink" title="Theorem (Adam’s Law)"></a>Theorem (Adam’s Law)</h3><p>For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><p><strong>Proof by LOTP</strong></p><p>  For $X$ discrete</p><script type="math/tex; mode=display">E(Y) = \sum_x E(Y|X=x) P(X=x)</script><p>  We let $E(Y|X=x) = g(x)$, then</p><script type="math/tex; mode=display">E(E(Y|X)) = E(g(X)) = \sum_x g(x) P(X=x) = \sum_x E(Y|X=x) P(X=x)</script><p>  So</p><script type="math/tex; mode=display">E(E(Y|X)) = E(Y)</script><h3 id="Theorem-Adam’s-Law-with-Extra-Conditioning"><a href="#Theorem-Adam’s-Law-with-Extra-Conditioning" class="headerlink" title="Theorem (Adam’s Law with Extra Conditioning)"></a>Theorem (Adam’s Law with Extra Conditioning)</h3><p>For any r.v.s $X,Y,Z$</p><script type="math/tex; mode=display">E(E(Y|X,Z)|Z) = E(Y|Z)</script><script type="math/tex; mode=display">E(E(X|Z,Y)|Y) = E(X|Y)</script><h3 id="Definition-Conditional-Variance"><a href="#Definition-Conditional-Variance" class="headerlink" title="Definition (Conditional Variance)"></a>Definition (Conditional Variance)</h3><script type="math/tex; mode=display">Var(Y|X) = E((Y-E(Y|X))^2|X)</script><p>this equivalent to</p><script type="math/tex; mode=display">Var(Y|X) = E(Y^2|X) - (E(Y|X))^2</script><h3 id="Theorem-Eve’s-Low"><a href="#Theorem-Eve’s-Low" class="headerlink" title="Theorem (Eve’s Low)"></a>Theorem (Eve’s Low)</h3><script type="math/tex; mode=display">Var(Y) = E(Var(Y|X)) + Var(E(Y|X))</script><h3 id="Example-Random-Sum"><a href="#Example-Random-Sum" class="headerlink" title="Example (Random Sum)"></a>Example (Random Sum)</h3><p>A store receives $N$ customers a day, $N$ is an r.v. with finite mean and variance. Let $X_j$ be the amount spend by the $j^{th}$ customer, $X_j$ has the mean $\mu$ and variance $\sigma^2$, $N$ and $X_j$ are independent of one another. Find the random sum $X = \sum_{j=1}^N X_j$ in terms of $\mu,\sigma^2,E(N),Var(N)$</p><p><strong>For E(X)</strong></p><script type="math/tex; mode=display">E(X|N) = E\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N E(X_j|N)= \sum_{j=1}^N E(X_j) = N\mu</script><p>  Finally, by Adam’s Law</p><script type="math/tex; mode=display">E(X) = E(E(X|N)) = E(N\mu) =\mu E(N)</script><p><strong>For Var(X)</strong></p><p>  We conditon on $N$ get $Var(X|N)$</p><script type="math/tex; mode=display">Var(X|N) = Var\left( \sum_{j=1}^N X_j|N \right) = \sum_{j=1}^N Var(X_j|N) = \sum_{j=1}^N Var(X_j) = N\sigma^2</script><p>  Eve’s Law give the unconditional variance of $X$</p><script type="math/tex; mode=display">\begin{align}    Var(X) =& E(Var(X|N)) + Var(E(X|N))\\           =& E(N\sigma^2) + Var(N\mu) \\           =& \sigma^2 E(N) + \mu^2 Var(N)  \end{align}</script><hr><h1 id="Prediction-and-Estimation"><a href="#Prediction-and-Estimation" class="headerlink" title="Prediction and Estimation"></a>Prediction and Estimation</h1><h3 id="Theorem-Projection-Interpretation"><a href="#Theorem-Projection-Interpretation" class="headerlink" title="Theorem (Projection Interpretation)"></a>Theorem (Projection Interpretation)</h3><p>For any function $h$, the r.v. $Y-E(Y|X)$ is Uncorrelated with $h(X)$: $Cov(Y-E(Y|X),h(X)) = 0$, equivalently</p><script type="math/tex; mode=display">E((Y-E(Y|X))h(X)) = 0</script><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Conditional-Expectation-Given-An-Event&quot;&gt;&lt;a href=&quot;#Conditional-Expectation-Given-An-Event&quot; class=&quot;headerlink&quot; title=&quot;Conditional Expe
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Conjugacy &amp; Bayesian)</title>
    <link href="http://yoursite.com/2018/07/31/Stochastic-Process-7/"/>
    <id>http://yoursite.com/2018/07/31/Stochastic-Process-7/</id>
    <published>2018-07-31T02:35:18.000Z</published>
    <updated>2018-08-03T05:40:01.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Beta-Binomial-Distribution"><a href="#Beta-Binomial-Distribution" class="headerlink" title="Beta-Binomial Distribution"></a>Beta-Binomial Distribution</h1><h3 id="Definition-Beta-Distribution"><a href="#Definition-Beta-Distribution" class="headerlink" title="Definition (Beta Distribution)"></a>Definition (Beta Distribution)</h3><p>An r.v. $X$ is said to have <strong>Beta distribution</strong> with parameters $a$ and $b$, if its PDF is</p><script type="math/tex; mode=display">f(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1},0<x<1</script><p>where $\beta(a,b)$ is constant to make PDF integrate to 1, We write this as $X\sim Beta(a,b)$</p><p>By varying the values of $a$ and $b$, we get PDFs with a variety of shapes<br><img src="/2018/07/31/Stochastic-Process-7/sp7img1.jpg" align="justify"></p><p><em>Beta Distribution is the generalization of uniform distribution while $a=b=1$</em></p><p>The Beta is a flexible family of continuous distributions on (0,1), and has many stories. One is Beta r.v. often used to represent an unknown probability. <strong>we can use Beta to put probabilities on unknown probabilities</strong> If a parameter $p$ satisfies $0&lt;p&lt;1$, we can assume the prior distribution of $p$ is Beta(a,b)</p><h3 id="Beta-Integral"><a href="#Beta-Integral" class="headerlink" title="Beta Integral"></a>Beta Integral</h3><p>One inportant issue to analyse the Beta Distribution is the Integral</p><script type="math/tex; mode=display">\beta(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1}dx</script><h3 id="Bayes’-billiards"><a href="#Bayes’-billiards" class="headerlink" title="Bayes’ billiards"></a>Bayes’ billiards</h3><p> It’s hard to get the reuslt directly from calculus, the left and right sides of the above formulation can be connected by one event $P(X=k)$</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) x^k(1-x)^{n-k}dx = \frac{1}{n+1}</script><p><strong>Left side Story :</strong> Having $n+1$ balls , $n$ white and $1$ gray. Randomly throw each ball onto the interval $[0,1]$, so the position of balls are i.i.d. $Unif(0,1)$. Let $X$ be the number of white balls to the left of the gray ball.</p><p>To get the probability of the event $X=k$, we use LOTP. Conditioning on the position of gray ball, call it $B$, Conditional on $B=p$, the number of the white ball landing to the left of $p$ has $Bin(n,p)$ distribution, The PDF of $B$ is $f(p) =1$ since $B\sim Unif(0,1)$</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|B=p)f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-k)^{n-k}dp</script><p><strong>Right side Story :</strong> Having $n+1$ balls, all white, randomly throw onto unit interval; then choose one ball at random and paint it gray. Again, let $X$ be the number of white balls to the left of gray ball. By symmetry, any one of the $n+1$ balls is equally likely to be painted gray, then</p><script type="math/tex; mode=display">P(X=k) = \frac{1}{n+1}</script><p>$X$ has the same distribution, then</p><script type="math/tex; mode=display">\int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}dp= \frac{1}{n+1}</script><p>Using the result, we are capable to calculate the $\beta(a,b)$ by substituting $a-1$ for $k$ and $b-1$ for $n-k$</p><script type="math/tex; mode=display">\beta(a,b) = \frac{1}{(a+b-1) \left( \begin{array}{c} a+b-2\\a-1 \end{array} \right)} = \frac{(a-1)!(b-1)!}{(a+b-1)!}</script><p>For a r.v. $X\sim Beta(a,b)$, The Expectation</p><script type="math/tex; mode=display">\begin{align}E(X) &= \int_0^1 xf(x)dx= \int_0^1 x\cdot \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}dx=\frac{1}{\beta(a,b)}\int_0^1 x^a(1-x)^{b-1}dx \\&= \frac{1}{\beta(a,b)}\cdot \beta(a+1,b) = \frac{\frac{a!(b-1)!}{(a+b)!}}{\frac{(a-1)!(b-1)!}{(a+b-1)!}} = \frac{a}{a+b}\end{align}</script><h3 id="Beta-Binomial-Conjugacy"><a href="#Beta-Binomial-Conjugacy" class="headerlink" title="Beta-Binomial Conjugacy"></a>Beta-Binomial Conjugacy</h3><p> Now let’s see the connection between Beta distribution and Binomial distribution, the relation we call it Conjugacy.</p><p>We have a coin lands head with p. $p$, and we dont know what %p% is. Our goal is to infer the value of $p$ after observing the outcomes of n tosses of the coin.<br><strong>Bayesian Inference</strong></p><ul><li>Treat all unknown probability $p$ as r.v. and give $p$ a distribution</li><li>Above is called <strong>prior distribution</strong>, it reflects out uncertainty about the ture value of $p$ before observing</li><li>After experiment and data are gathered, prior distribution is updated using Baye’s rule,; This yields the <strong>posterior distribution</strong>, which reflects the new beliefs about $p$</li><li>Specifically<ul><li><strong>prior distribution</strong> $f(p)$</li><li><strong>posterior distribution</strong> $f(p|X=n)$</li></ul></li></ul><p>Suppose the prior distribution on $p$ is Beta distribution. Let $p\sim Beta(a,b)$ for known constants $a$ and $b$, $X$ be the number of heads in $n$ tosses of the coin. Conditional on knowing ture value of $p$ then</p><script type="math/tex; mode=display">X|p \sim Bin(n,p)</script><p>We use the Bayes rule. Letting $f(p)$ be the prior distribution and $f(p|X=k)$ be the posterior distribution after observing $k$ heads</p><script type="math/tex; mode=display">f(p|X=k) = \frac{P(X=k|p)f(p)}{P(X=k)}=\frac{\left( \begin{array}{c} n\\k \end{array} \right) p^k (1-p)^{n-k}\cdot \frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}</script><p>The denominator $P(X=k)$ is the marginal PMF of $X$, is given by</p><script type="math/tex; mode=display">P(X=k) = \int_0^1 P(X=k|p) f(p)dp = \int_0^1 \left( \begin{array}{c} n\\k \end{array} \right) p^k(1-p)^{n-k}f(p)dp</script><p>If $a=b=1$ , $P(X=k) = 1/(n+1)$, but it not seem easy to find $P(X=k)$ in general , Are we stuck ?</p><p>Actually, is much easier than it appears at first, the conditional PDF $f(p|X=k)$ is a function of $p$, so everything doesn’t depend on $p$ is just a constant. After dropping constants gives</p><script type="math/tex; mode=display">f(p|X=k) \propto p^{a+k-1}(1-p)^{b+n-k-1}</script><p>which is the $Beta(a+k,b+n-k)$ PDF.Therefore the posterior distribution of $p$ is</p><script type="math/tex; mode=display">p|X = k\sim Beta(a+k,b+n-k)</script><p>The posterior distribution of $p$ after observing $X=k$ is still a Beta distribution!</p><p>We say <em>Beta is the Conjugate prior of the Binomial</em></p><ul><li>We add the observed successes $k$ to the first parameter</li><li>We add the observed successes $k-n$ to the second parameter</li><li>$a$ and $b$ have a concrete interpretation in this context<ul><li>$a$ as the number of prior successes in earlier experiments</li><li>$b$ as the number of prior failures in earlier experiments</li></ul></li></ul><h3 id="Mean-vs-Bayesian-Average"><a href="#Mean-vs-Bayesian-Average" class="headerlink" title="Mean vs Bayesian Average"></a>Mean vs Bayesian Average</h3><ul><li>Mean: $\frac{k}{n}$</li><li>Bayesian Average: $E(p|X=k) = \frac{a+k}{a+b+n}$</li></ul><hr><h1 id="Dirichlet-Multinomial-Distribution"><a href="#Dirichlet-Multinomial-Distribution" class="headerlink" title="Dirichlet-Multinomial Distribution"></a>Dirichlet-Multinomial Distribution</h1><p>$n$ objects are independently placed into one of $k$ categories, with probability of $p_j$ to category j, and $\sum_{j=1}^k p_j = 1$. Let $X_i$ be the number of objects in category $i$, $X_1 + … + X_n = n$. Then $X = (X_1,…,X_k)$ is said to have <strong>Multinomial distribution</strong> with parameters $n$ and $\mathbf{p} = (p_1,…,p_k)$, write as $\mathbf{X} \sim Mult_k(n,\mathbf{p})$</p><h3 id="Theorem-Multinomial-Joint-PMF"><a href="#Theorem-Multinomial-Joint-PMF" class="headerlink" title="Theorem (Multinomial Joint PMF)"></a>Theorem (Multinomial Joint PMF)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then the joint PMF of $\mathbf{X}$ is</p><script type="math/tex; mode=display">P(X_1=n_1,...,X_k=n_k) = \frac{n!}{n_1!n_2!...n_k!}\cdot p_1^{n_1}p_2^{n_2}\dotsb p_k^{n_k}</script><h3 id="Theorem-Multinomial-Marginals"><a href="#Theorem-Multinomial-Marginals" class="headerlink" title="Theorem (Multinomial Marginals)"></a>Theorem (Multinomial Marginals)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then $X_j \sim Bin(n,p_j)$</p><h3 id="Theorem-Multinomial-Lumping"><a href="#Theorem-Multinomial-Lumping" class="headerlink" title="Theorem (Multinomial Lumping)"></a>Theorem (Multinomial Lumping)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then for distinct $i$ and $j$, $X_i+X_j\sim Bin(n,p_i+p_j)$</p><script type="math/tex; mode=display">(X_1+X_2,X_3,...,X_k)\sim Mult_k(n,((p_1+p_2),p_3,...,p_n))</script><p><img src="/2018/07/31/Stochastic-Process-7/sp7img2.jpg" style=" width:500px;"></p><h3 id="Theorem-Multinomial-Conditioning"><a href="#Theorem-Multinomial-Conditioning" class="headerlink" title="Theorem (Multinomial Conditioning)"></a>Theorem (Multinomial Conditioning)</h3><p>If $\mathbf{X}\sim Mult_k(n,\mathbf{p})$, then</p><script type="math/tex; mode=display">(X_2,...,X_k)|X_1=n_1\sim Mult_k(n-n_1,(p_2^{\prime},...,p_k^{\prime}))</script><p>where $p_j^{\prime} = p_j/(p_2+\dotsb +p_k)$</p><h3 id="Theorem-Covariance-in-A-Multinomial"><a href="#Theorem-Covariance-in-A-Multinomial" class="headerlink" title="Theorem (Covariance in A Multinomial)"></a>Theorem (Covariance in A Multinomial)</h3><p>Let $X_1,…,X_k\sim Mult_k(n,\mathbf{p})$, where $\mathbf{p} = (p_1,…,p_k)$.</p><script type="math/tex; mode=display">Cov(X_i,X_j) = -np_ip_j</script><h3 id="Definition-Dirichlet-Distribution"><a href="#Definition-Dirichlet-Distribution" class="headerlink" title="Definition (Dirichlet Distribution)"></a>Definition (Dirichlet Distribution)</h3><p>Dirichlet distribution is parameterized by a vector $\mathbf{\alpha}$ of positive real numbers. The PDF is:</p><script type="math/tex; mode=display">f(p_1,p_2,...,p_k;\alpha_1,\alpha_2,...,\alpha_k)=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\prod_{i=1}^k\Gamma(\alpha_i)}\prod_{i=1}^kp_i^{\alpha_i-1}</script><p>where $p_1 + … + p_k = 1$</p><h3 id="Dirichlet-Multinomial-Conjugacy"><a href="#Dirichlet-Multinomial-Conjugacy" class="headerlink" title="Dirichlet-Multinomial Conjugacy"></a>Dirichlet-Multinomial Conjugacy</h3><p>Assume we already have the Multinomial distribution $\mathbf{X}\sim Mult_k(n,\mathbf{p})$. The prior distribution of $\mathbf{p}=(p_1,…,p_k)$ is a Dirichlet distribution, i.e. $\mathbf{p}\sim Dir(\alpha)$. Denote $\mathbf{X} = (X_1,…,X_k)$, then</p><script type="math/tex; mode=display">\mathbf{X}|\mathbf{p}\sim Mult_k(n,\mathbf{p})</script><p>Let $f(\mathbf{p})$ to be the prior distribution of $\mathbf{p}$. The observations of the experiment is $\mathbf{N} = (n_1,…,n_k)$ then</p><script type="math/tex; mode=display">\begin{align}f(\mathbf{p}|\mathbf{X}=\mathbf{N}) &= \frac{P(\mathbf{X}=\mathbf{N}|\mathbf{p})f(\mathbf{p})}{P(\mathbf{X}=\mathbf{N})} \\&= \frac{\frac{n!}{n_1!\dotsb n_k!}p_1^{n_1}\dotsb p_k^{n_k}\cdot \frac{\Gamma(\sum_{i=1}^k \alpha_i)}{\prod_{i=1}^k \Gamma(\alpha_i)}\prod_{i=1}^k p_i^{\alpha_i -1}}{P(\mathbf{X}= \mathbf{N})} \\&\propto p_1^{n_1+\alpha_1 - 1}\dotsb p_k^{n_k+\alpha_k - 1} \\&\sim Dir(\mathbf{\alpha} + \mathbf{N})\end{align}</script><p>Thus we can see that</p><script type="math/tex; mode=display">\text{prior}\ Dir(\mathbf{\alpha}) \rightarrow \text{posterior}\ Dir(\mathbf{\alpha} + \mathbf{N})</script><script type="math/tex; mode=display">\alpha_i \rightarrow \alpha_i + n_i</script><p>We can prove that</p><script type="math/tex; mode=display">E[p_i|\mathbf{X} = \mathbf{N}] = \frac{\alpha_i + n_i}{\sum_{i=1}^k(\alpha_i + n_i)}</script><hr><h1 id="Bayesian-Average"><a href="#Bayesian-Average" class="headerlink" title="Bayesian Average"></a>Bayesian Average</h1><p>One application for Bayesian Average is in <strong>Rating System</strong>, usually the customers will rate the movies in 5 star.</p><p>This will rose a problem, which One to Choose</p><ul><li>$5$ Average rating movie A by $1$ voter</li><li>$4.9998$ Average rating movie B by $1400010123$ voter (of course this One)</li></ul><p>To use the Bayesian estimation to compute the posterior probability for star ratings, we must use a <strong>joint distribution</strong>, the random variable is a categorical distribution with probability follows:</p><script type="math/tex; mode=display">p_1+p_2+p_3+p_4+p_5 = 1</script><h5 id="Multinomial-Distribution"><a href="#Multinomial-Distribution" class="headerlink" title="Multinomial Distribution"></a>Multinomial Distribution</h5><p>Let $O$ be the event of movie rating, we compute the posterior probability with $N$ observations for five categories with corresponding numbers $K_1,K_2,K_3,K_4,K_5$ as follows:</p><script type="math/tex; mode=display">Pr(O|p_1,p_2,p_3,p_4,p_5)\propto p_1^{K_1}p_2^{K_2}p_3^{K_3}p_4^{K_4}p_5^{K_5}</script><p>where $K_1+…+K_5 = N$</p><h5 id="Dirichlet-Distribution-Prior"><a href="#Dirichlet-Distribution-Prior" class="headerlink" title="Dirichlet Distribution: Prior"></a>Dirichlet Distribution: Prior</h5><script type="math/tex; mode=display">Pr(p_1,p_2,p_3,p_4,p_5|O) \propto \prod_{j=1}^5 p_j^{K_j+\alpha_j^0 - 1}</script><p>After considering the new votes we can update the distribution of the $\mathbf{p}$ by</p><script type="math/tex; mode=display">\alpha_j^1 = K_j +\alpha_k^0</script><h5 id="Expected-Average"><a href="#Expected-Average" class="headerlink" title="Expected Average"></a>Expected Average</h5><p>What we need is the average rating given posterior in the shape of our Dirichlet distribution:</p><script type="math/tex; mode=display">E(p_1+2p_2+3p_3+4p_4+5p_5|O) = \sum_{i=1}^5 iE(p_i|O)</script><p>According to</p><script type="math/tex; mode=display">E(p_i|O) = \frac{\alpha_i^1}{\sum_{j=1}^5 \alpha_j^1}</script><p>We have</p><script type="math/tex; mode=display">\sum_{i=1}^5 iE(p_i|O) = \frac{\sum_{i=1}^5 i\alpha_i^0 + \sum_{i=1}^5 i K_i}{\sum_{j=1}^5 \alpha_j^0 +N}</script><h3 id="Bayesian-Average-Rating"><a href="#Bayesian-Average-Rating" class="headerlink" title="Bayesian Average Rating"></a>Bayesian Average Rating</h3><p>The final formulation can be express as</p><script type="math/tex; mode=display">\text{Bayes Average Rating} = \frac{C\cdot m +\sum(ratings)}{C+N}</script><ul><li>N: The number of ratings</li><li>m: a prior for the average of rating scores</li><li>C: a prior for the number of rating scores</li></ul><hr><h1 id="Gamma-Distribution"><a href="#Gamma-Distribution" class="headerlink" title="Gamma Distribution"></a>Gamma Distribution</h1><h3 id="Definition-Gamma-Function"><a href="#Definition-Gamma-Function" class="headerlink" title="Definition (Gamma Function)"></a>Definition (Gamma Function)</h3><p>The Gamma function $\Gamma$ is defined by</p><script type="math/tex; mode=display">\Gamma(a) = \int_0^{\infty}x^ae^{-x}\frac{dx}{x}</script><p>Earlier Beta distribution can represent an unknown probability of success cause its support is $(0,1)$. The Gamma distribution can represent an unknown rate in a Poisson process because its support is $(0,\infty)$</p><h3 id="Property-of-Gamma-Function"><a href="#Property-of-Gamma-Function" class="headerlink" title="Property of Gamma Function"></a>Property of Gamma Function</h3><ul><li>$\Gamma(a+1) = a\Gamma(a)$</li><li>$\Gamma(n) = (n-1)!$ if $n$ is a integer</li></ul><h3 id="Definition-Gamma-Distribution"><a href="#Definition-Gamma-Distribution" class="headerlink" title="Definition (Gamma Distribution)"></a>Definition (Gamma Distribution)</h3><p>An r.v. $Y$ is said to have <strong>Gamma distribution</strong> with parameters $a&gt;0, \lambda&gt;0$, if its PDF is</p><script type="math/tex; mode=display">f(y) = \frac{1}{\Gamma(a)}(\lambda y)^a e^{-\lambda y} \frac{1}{y}, y>0</script><p>Write $Y\sim Gamma(a,\lambda)$. <strong>Gamma distribution is a generalization of exponential distribution</strong> when $a=1$</p><h3 id="PDF-of-Gamma-distribution"><a href="#PDF-of-Gamma-distribution" class="headerlink" title="PDF of Gamma distribution"></a>PDF of Gamma distribution</h3><p><img src="/2018/07/31/Stochastic-Process-7/sp7img3.jpg" align="justify"></p><h3 id="Moments-of-Gamma-Distribution"><a href="#Moments-of-Gamma-Distribution" class="headerlink" title="Moments of Gamma Distribution"></a>Moments of Gamma Distribution</h3><script type="math/tex; mode=display">E[X] = \frac{a}{\lambda}, Var(X) = \frac{a}{\lambda^2}</script><h3 id="Theorem-Gamma-Convolution-of-Exponential"><a href="#Theorem-Gamma-Convolution-of-Exponential" class="headerlink" title="Theorem (Gamma: Convolution of Exponential)"></a>Theorem (Gamma: Convolution of Exponential)</h3><p>Let $X_1,…,X_n$ be i.i.d. $Expo(\lambda)$ , Then</p><script type="math/tex; mode=display">X_1+\dotsb +X_n \sim Gamma(n,\lambda)</script><h3 id="Beta-Gamma-Connection"><a href="#Beta-Gamma-Connection" class="headerlink" title="Beta-Gamma Connection"></a>Beta-Gamma Connection</h3><p>We have independent Gamma r.v.s $X$ and $Y$ with the same rate $\lambda$</p><ul><li>$X+Y$ had Gamma distribution</li><li>$\frac{X}{X+Y}$ has Beta distribution</li></ul><h3 id="Binomial-amp-Poisson-amp-Gamma"><a href="#Binomial-amp-Poisson-amp-Gamma" class="headerlink" title="Binomial &amp; Poisson &amp; Gamma"></a>Binomial &amp; Poisson &amp; Gamma</h3><ul><li>The PMF of $Poisson(X=k|\lambda)$ is $P(X=k|\lambda) = \frac{\lambda^ke^{-\lambda}}{k!}$</li><li>The PDF of $X\sim Gamma(a,1)$ is $f_X(x) = \frac{1}{\Gamma(a)}a^{a-1}e^{-x}$. Given $a=k+1$ we have<script type="math/tex; mode=display">P(X=x|a=k+1,1)=\frac{x^{k+1}}{\Gamma(k+1)}e^{-x}= \frac{x^k}{k!}e^{-x}</script></li><li>For a r.v. $X\sim Bin(n,p)$, we have<script type="math/tex; mode=display">P(X\leq k)=\frac{n!}{k!(n-k-1)!}\int_p^1 t^k(1-t)^{n-k-1}dt</script>Let $t=\frac{x}{n}$, then<script type="math/tex; mode=display">P(X\leq k) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n})dx</script>It follows that<script type="math/tex; mode=display">Bin(X\leq k|n,p) = \int_{np}^n Bin(X=k|n-1,\frac{x}{n}dx)</script></li><li>Let $\lambda = np$. We fix $\lambda$, and let $n\rightarrow \infty$, then<script type="math/tex; mode=display">Bin(n,p) \rightarrow Posi(\lambda)</script></li><li>When $\lambda \rightarrow 0$, we have<script type="math/tex; mode=display">1=\mathop{lim}_{\lambda \rightarrow 0} \int_{\lambda}^{\infty} \frac{\lambda^k e^{-x}}{k!}dx = \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx=\int_0^{\infty} Gamma(k+1,1)dx</script></li><li>$1=\int_0^{\infty} \frac{x^ke^{-x}}{k!}dx\Rightarrow k!=\int_0^{\infty}x^k e^{-x}dx$</li><li>Because $Pois(X\leq k|\lambda) = \int_{\lambda}^{\infty}\frac{x^k e^{-x}}{k!} dx$ we have<script type="math/tex; mode=display">Pois(X\leq k|\lambda) + \int_0^{\infty} \frac{x^ke^{-x}}{k!}dx = 1</script></li></ul><p>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Beta-Binomial-Distribution&quot;&gt;&lt;a href=&quot;#Beta-Binomial-Distribution&quot; class=&quot;headerlink&quot; title=&quot;Beta-Binomial Distribution&quot;&gt;&lt;/a&gt;Beta-Bin
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Multivariate)</title>
    <link href="http://yoursite.com/2018/07/29/Stochastic-Process-6/"/>
    <id>http://yoursite.com/2018/07/29/Stochastic-Process-6/</id>
    <published>2018-07-29T00:41:09.000Z</published>
    <updated>2018-08-02T01:39:52.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Discrete-Multivariate-R-V-s"><a href="#Discrete-Multivariate-R-V-s" class="headerlink" title="Discrete Multivariate R.V.s"></a>Discrete Multivariate R.V.s</h1><p><strong>Definition (Joint CDF)</strong> The Joint CDf of r.v.s $X$ and $Y$ is the function $F_{X,Y}$ given by</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = P(X\leq x,Y\leq y)</script><p><strong>Definition (Joint PMF)</strong> The Joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X,Y}$ given by</p><script type="math/tex; mode=display">p_{X,Y}(x,y) = P(X=x,Y=y)</script><p><strong>Definition (Marginal PMF)</strong> For discrete r.v.s $X$ and $Y$, Marginal PMF of $X$ is</p><script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x,Y=y)</script><p><strong>Definition (Conditional PMF)</strong> For discrete r.v.s $X$ and $Y$, the Conditional PMF of $X$ given $Y=y$ is</p><script type="math/tex; mode=display">P_{X|Y}(x|y) = P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}</script><p><strong>Definition (Independence of Discrete R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x) F_Y(y)</script><p>for all x and y also equivalent to the condition</p><script type="math/tex; mode=display">P(Y=y|X=x) = P(Y=y)</script><hr><h1 id="Continuous-Multivariate-R-V-s"><a href="#Continuous-Multivariate-R-V-s" class="headerlink" title="Continuous Multivariate R.V.s"></a>Continuous Multivariate R.V.s</h1><p><strong>Definition (Joint PDF)</strong> If $X$ and $Y$ are continuous with joint CDF $F_{X,Y}$ then</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y)</script><p><strong>Definition (Marginal PDF)</strong> If $X$ and $Y$ are continuous with joint PDF $f_{X,Y}$ then</p><script type="math/tex; mode=display">f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy</script><p><strong>Definition (Conditional PDF)</strong> For continuous r.v.s. $X$ and $Y$ with joint PDF $f_{X,Y}$ the Conditional PDF of $Y$ given $X=x$ is</p><script type="math/tex; mode=display">f_{Y|X}(y|x)= \frac{f_{X,Y}(x,y)}{f_X(x)}</script><p><strong>Definition (Independence of Continuous R.V.s)</strong> Random variables $X$ and $Y$ are independent if for all x and y</p><script type="math/tex; mode=display">F_{X,Y}(x,y) = F_X(x)F_Y(y)</script><p>If $X$ and  $Y$ are continuous with joint PDF $f_{X,Y}$</p><script type="math/tex; mode=display">f_{X,Y}(x,y) = f_X(x) f_Y(y)</script><p><strong>Theorem (2D LOTUS)</strong> Let g be a function from $R^2$ to $R$</p><p>If $X$ and $Y$ are discrete</p><script type="math/tex; mode=display">E(g(X,Y)) = \sum_x \sum_y g(x,y) P(X=x,Y=y)</script><p>If $X$ and $Y$ are continuous</p><script type="math/tex; mode=display">E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f_{X,Y}(x,y) dxdy</script><h3 id="General-Bayes’-Rule"><a href="#General-Bayes’-Rule" class="headerlink" title="General Bayes’ Rule"></a>General Bayes’ Rule</h3><p><img src="/2018/07/29/Stochastic-Process-6/img1.jpg" align="justify"></p><hr><h1 id="Convariance-and-Correlation"><a href="#Convariance-and-Correlation" class="headerlink" title="Convariance and Correlation"></a>Convariance and Correlation</h1><p><strong>Covariance</strong></p><ul><li>Measure a tendency of two r.v.s $X\&amp;Y$ to go up or down together</li><li>Positive Covariance: $X$ go up, $Y$ tends go up</li><li>Negative Covariance: $X$ go up, $Y$ tends go down</li></ul><p><strong>Definition (Covariance)</strong> The covariance between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Cov(X,Y) = E((X-EX)(Y-EY))=E(XY)-E(X)E(Y)</script><p><strong>Theorem (Uncorrelated)</strong> If $X$ and $Y$ are independent, then they are Uncorrelated($Cov(X,Y)=0$)</p><h3 id="Properties-of-Covariance"><a href="#Properties-of-Covariance" class="headerlink" title="Properties of Covariance"></a>Properties of Covariance</h3><ul><li>$Cov(X,X) = Var(X)$</li><li>$Cov(X,Y) = Cov(Y,X)$</li><li>$Cov(X,c) = 0$</li><li>$Cov(a\cdot X,Y) = a\cdot Cov(X,Y)$</li><li>$Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$</li><li>$Cov(X+Y,W+Z) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$</li><li>$Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$</li><li>For n r.v.s $X_1,\dotsb ,X_n$ <script type="math/tex">Var(X_1+\dotsb +X_n)=Var(X_a)+\dotsb+Var(X_n)+2\sum_{i<j}Cov(X_i,Y_j)</script></li></ul><p><strong>Definition (Correlation)</strong> The Correlation between r.v.s $X$ and $Y$ is</p><script type="math/tex; mode=display">Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}</script><p><em>Shifting and Scaling $X$ and $Y$ has no effect on correlation</em><br><img src="/2018/07/29/Stochastic-Process-6/img2.jpg" align="justify"></p><p><strong>Theorem (Correlation Bounds)</strong> For any r.v.s $X$ and $Y$</p><script type="math/tex; mode=display">-1\leq Corr(X,Y) \leq 1</script><hr><h1 id="Change-of-Variables"><a href="#Change-of-Variables" class="headerlink" title="Change of Variables"></a>Change of Variables</h1><p><strong>Theorem (Change of Variables in One Dimension)</strong> Let $X$ be a continuous r.v. with PDF $f_X$, and let $Y = g(X)$, where $g$ is differentiable and strictly increasing. Then the PDF of $Y$ is given by</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left|  \frac{dx}{dy} \right|</script><p>where $x = g^{-1}(y)$</p><p><strong><em>Proof</em></strong>:</p><script type="math/tex; mode=display">F_Y(y) = P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_X(g^{-1}(y))=F_X(x)</script><p>Then result obtained By the chain rule</p><p><strong>Theorem (Change of Variables)</strong> Let $X = (X_1,…,X_n)$ be a continuous random vector with joint PDF $f_X(x)$ and $Y=g(X)$, $g$ is an invertible function from $R^n$ to $R^n$ then $\frac{\partial \mathbf{x}}{\partial \mathbf{y}}$ form a <strong><em>Jacobian  matrix</em></strong></p><script type="math/tex; mode=display">\frac{\partial \mathbf{x}}{\partial \mathbf{y}} =\left( \begin{array}{cccc}\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \dotsb & \frac{\partial x_1}{\partial y_n} \\\vdots & \vdots & & \vdots \\\frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \dotsb & \frac{\partial x_n}{\partial y_n} \\\end{array} \right)</script><p>Then the joint PDF of $Y$ is</p><script type="math/tex; mode=display">f_Y(y) = f_X(x) \left| \frac{\partial \mathbf{x}}{\partial \mathbf{y}} \right|</script><hr><h1 id="Convolutions"><a href="#Convolutions" class="headerlink" title="Convolutions"></a>Convolutions</h1><p><strong>Theorem (Convolution Sums and Integrals)</strong><br>If $X$ and $Y$ are independent discrete r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  P(T=t) =& \sum_x P(Y = t-x) P(X=x) \\         =& \sum_y P(X= t-y) P(Y=y)\end{align}</script><p>If $X$ and $Y$ are independent continuous r.v.s, then the PMF of their sum $T = X+Y$ is</p><script type="math/tex; mode=display">\begin{align}  f_T(t)  =& \int_{-\infty}^{\infty} f_Y(t-x) f_X(x) dx \\          =& \int_{-\infty}^{\infty} f_X(t-y) f_Y(y) dy\end{align}</script><hr><h1 id="Order-Statistics"><a href="#Order-Statistics" class="headerlink" title="Order Statistics"></a>Order Statistics</h1><p><strong>Definition (Order Statistics)</strong> For r.v.s $X_1,X_2,…,X_n$ the order statistics sre the random variables $X_{(1)},…,X_{(2)}$, where</p><ul><li>$X_{(1)}  = min (X_1,…,X_n)$</li><li>$X_{(2)}$ is the $2^{nd}$ of $X_1,…,X_n$</li><li>$\vdots$</li><li>$X_{n} = max(X_1,…,X_n)$</li></ul><p>The order statistics are dependent, for example , if $X_{(1)} = 100$, then $X_{(n)}$ is forced to be $\geq 100$</p><p>We foucs on the case $X_1,…,X_n$ are i.i.d continuous r.v.s, with CDF $F$ and PDF $f$</p><p><strong>Theorem (CDF of Order Statistics)</strong>  Let $X_1,…,X_n$ be i.i.d continuous r.v.s with CDF F, Then the CDF of the $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">P(X_{(j)}\leq x)=\sum_{k=j}^n\left( \begin{array}{c} n\\k  \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><p><strong><em>Proof:</em></strong></p><hr><p>Let’s start with a specical case when $j=n, X_{(n)}=max(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(n)}} (x) &= P[max(X_1,...,X_n)\leq x] \\&=P(X_1\leq  x)\dotsb P(X_n\leq x) \\&=[F(x)]^n\end{align}</script><hr><p>Then, consider another special case when $j=1, X_{(1)} = min(X_1,…,X_n)$:</p><script type="math/tex; mode=display">\begin{align}F_{X_{(1)}} (x) &= P[min(X_1,...,X_n)\leq x] \\&=1 - P(X_1>  x)\dotsb P(X_n> x) \\&=1-[1-F(x)]^n\end{align}</script><p>The result here can be rewrite as $\sum_{k=1}^n\left( \begin{array}{c} n\\k \end{array} \right) F(x)^k (1-F(x))^{n-k}$</p><p>This result can be obtained by expand $[F(x) + 1 -F(x)]^n$</p><hr><p>Finally, let’s consider more general case where $1&lt;j&lt;n, X_{(j)}\leq x$, this means at least $j$ of $\{X_i \}$ fall to the left of $x$</p><p>Denote $N$ as the nunber of $X_i$ landing to the left of $x$. $X_i$ lands to the left of $x$ w.p. $P(X_i\leq x) = F(x)$. Then $N\sim Bin(n,F(x))$</p><script type="math/tex; mode=display">P(X_{(j)}\leq x) = P(N\geq j=\sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right) F(x)^k(1-F(x))^{n-k}</script><hr><p><strong>Theorem (PDF of Order Statistic)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with CDF $F$ and PDF $f$. Then the marginal PDF of $j^{th}$ order statistic $X_{(j)}$ is</p><script type="math/tex; mode=display">f_{X_{(j)}} (x) = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) f(x) F(x)^{j-1} (1-F(x))^{n-j}</script><p><strong>Theorem (Joint PDF)</strong> Let $X_1,…,X_n$ be i.i.d. continuous r.v.s with PDF $f$, Then the joint PDF of all order statistics is</p><script type="math/tex; mode=display">f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n) = n! \prod_{i=1}^n f(x_i), x_1<x_2<\dotsb <x_n</script><p><strong>Example 1(Order Statistics of Uniforms)</strong> $U_1,U_2,…,U_n$ are i.i.d. $Unif(0,1)$ r.v.s with CDF F and PDF $f$</p><p>For $0\leq x\leq 1$ ,$f(x) = 1$ , $F(x) = x$, Then</p><script type="math/tex; mode=display">f_{U_{(j)}} = n \left( \begin{array}{c} n-1\\j-1 \end{array} \right) x^{j-1} (1-x)^{n-j}</script><script type="math/tex; mode=display">F_{U_{(j)}}(x) = \sum_{k=j}^n \left( \begin{array}{c} n\\k \end{array} \right)x^k (1-x)^{n-k} = \int_0^x f_{U_{(j)}} (t) dt = \frac{n!}{(j-1)!(n-j)!}\int_0^x t^{j-1}(1-t)^{n-j}dt</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Discrete-Multivariate-R-V-s&quot;&gt;&lt;a href=&quot;#Discrete-Multivariate-R-V-s&quot; class=&quot;headerlink&quot; title=&quot;Discrete Multivariate R.V.s&quot;&gt;&lt;/a&gt;Discr
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Topology-(引论)</title>
    <link href="http://yoursite.com/2018/07/28/Topology-1/"/>
    <id>http://yoursite.com/2018/07/28/Topology-1/</id>
    <published>2018-07-28T12:19:29.000Z</published>
    <updated>2018-07-29T00:40:25.323Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Eular-定理"><a href="#Eular-定理" class="headerlink" title="Eular 定理"></a>Eular 定理</h1><p>对于一个多面体 P，我们定义</p><ul><li>v：定点数</li><li>e：棱边数</li><li>f：面数</li></ul><p>Eular 定理：$v+f-e = 2$</p><p>但是满足这个定理的多面体是有条件的：</p><ul><li>P 的任何两个顶点可以用一串棱相连接<ul><li>反例：中空的立方体</li></ul></li><li>P 上任意由直线段构成的圈，把 P 分割成两片<ul><li>反例：螺帽柱状体</li></ul></li></ul><p>Eular 定理证明：我们首先来看看树形，这个在图论里面常常出现，树有个性质就是 $v-e=1$，我们可以尝试用一棵树 T 来表示一个多面体，表示的方法是，树中的点就是 P 中的点（树 T 中的点囊括了所有 P 的点），树中的边就是 P 中的棱（边只是一部分的棱哦）。</p><p>然后我们来构造 T 的一种对偶，称为 $\Gamma$，$\Gamma$ 也是一颗树后面会证明，只不过这棵树的点由 P 中面的中心点来表示（也就是用来表示面的数量），这样面与面之间的边在多面体中是可以有一个曲折的，可以想象一下。。</p><p>上面采用这个形式只是因为这样构造能囊括所有的面，下面来证明一下这个 $\Gamma$ 是树，而且曲折所在的棱刚好是 T 的边对于 P 中棱的补集：</p><ul><li>连通性：如果 $\Gamma$ 的某两个顶点不能用 $\Gamma$ 内的一串棱连接，则它们必然被一个圈分开。由于 T 不含任何圈，$\Gamma$ 必然联通。</li><li>无圈：如果 $\Gamma$ 有圈，那么就会把顶点分开成两份，T 中的棱想要连接所有顶点就不可避免的要触碰到这个圈，所以 $\Gamma$ 无圈</li><li>$T,\Gamma$ 包含所有棱：假设一条棱没有被用着，这个棱本可以这样被用：棱两侧的面中点相连（$\Gamma$），或者棱两端的点相连（T），但是都没用着，这样 $\Gamma ,T$ 就会在后面相交。。（这是我的数学直觉，书上并没有这个的证明，我自己补的。。。不是很严谨。。。）</li></ul><p>最后我们有 $v(T) - e(T) = 1$, $v(\Gamma) - e(\Gamma) = 1$, 加起来有</p><script type="math/tex; mode=display">v(T) - [e(T)+e(\Gamma)] + v(\Gamma) = 2</script><p>同时，根据构造有</p><script type="math/tex; mode=display">v(T) = v, e(T) + e(\Gamma)+e, v(\Gamma) = f</script><h2 id="其他的证明方式可以用数学归纳法"><a href="#其他的证明方式可以用数学归纳法" class="headerlink" title="其他的证明方式可以用数学归纳法"></a>其他的证明方式可以用数学归纳法</h2><h1 id="拓扑等价"><a href="#拓扑等价" class="headerlink" title="拓扑等价"></a>拓扑等价</h1><p>我们考虑一个正四面体未冲气的气球，我们把它吹胖，吹成了一个圆形。</p><p>这样多面体的点和球面的点之间的对应就是<strong>拓扑等价</strong>或<strong>同胚</strong>的一个例子，确切的说就是一对一的连续满映射</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Eular-定理&quot;&gt;&lt;a href=&quot;#Eular-定理&quot; class=&quot;headerlink&quot; title=&quot;Eular 定理&quot;&gt;&lt;/a&gt;Eular 定理&lt;/h1&gt;&lt;p&gt;对于一个多面体 P，我们定义&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v：定点数&lt;/li&gt;
&lt;li&gt;e：棱
      
    
    </summary>
    
    
      <category term="Topology" scheme="http://yoursite.com/tags/Topology/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Generating Function)</title>
    <link href="http://yoursite.com/2018/07/27/Stochastic-Process-5/"/>
    <id>http://yoursite.com/2018/07/27/Stochastic-Process-5/</id>
    <published>2018-07-27T02:23:34.000Z</published>
    <updated>2018-07-28T09:06:43.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Generating-Function"><a href="#Generating-Function" class="headerlink" title="Generating Function"></a>Generating Function</h1><h3 id="Three-kinds-of-generating-functions"><a href="#Three-kinds-of-generating-functions" class="headerlink" title="Three kinds of generating functions"></a>Three kinds of generating functions</h3><ul><li>Probability Generating Function (PGF) : related to Z-transform</li><li>Moment Generating Function (MGF) : related to Laplace transform</li><li>Characteristic Function (CF) : related to Fourier transform</li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>PGF: handling non-negative integral random variables</li><li>MGF: handling general random variables</li><li>CF: equally useful with MGF</li></ul><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><ul><li>Easy to characterizing the distribution of the sum of independent random variables</li><li>Play a central role in the study of branching processes</li><li>Provide a bridge between complex analysis and probability</li><li>……</li></ul><hr><h1 id="Moment-Generating-Function"><a href="#Moment-Generating-Function" class="headerlink" title="Moment Generating Function"></a>Moment Generating Function</h1><p><strong>Definition (Moment Generating Function)</strong> MGF of an r.v. $X$ is $M(t) = E(e^{tX})$, as a function of $t$ (different t denote different valued moment) <strong><em>and this must finite on some open interval (-a,a) containing 0</em></strong> or dont exist.</p><h3 id="Why-we-need-MGF"><a href="#Why-we-need-MGF" class="headerlink" title="Why we need MGF"></a>Why we need MGF</h3><ul><li>MGF encodes the moments of an r.v.</li><li>MGF of an r.v. Determines its distribution, like CDF and PMF/PDF</li><li>MFG make it easy to find the distribution of a sum of i.r.v.s.</li></ul><p><strong>Theorem (Moments via Derivatives of the MGF)</strong> $E(X^n) = M^{(n)}(0)$<br>Using Taylor expansion of $M(t)$ at 0</p><script type="math/tex; mode=display">M(t) = \sum_{n=0}^{\infty} M^{(n)}(0) \frac{t^n}{n!}</script><p>Using Taylor expansion of $E(X)$</p><script type="math/tex; mode=display">M(t) = E(e^{tX}) = E\left(  \sum_{n=0}^{\infty} X^n \frac{t^n}{n!}  \right)=\sum_{n=0}^{\infty} E(X^n) \frac{t^n}{n!}</script><p>Matching the coefficients of two expansions, we get $E(X^n) = M^{(n)}(0)$</p><h3 id="MGF-of-Distribution"><a href="#MGF-of-Distribution" class="headerlink" title="MGF of Distribution"></a>MGF of Distribution</h3><p><strong>Theorem (MGF Determines the Distribution)</strong> Two r.v. have the same MGF have the same distribution, more strictly, if there is even a tiny interval containing 0 on which the MGF are equal, the the r.v.s must have same distribution.</p><p><strong>Example 1 (Bernoulli MGF)</strong> MGF of $X\sim Bern(p)$<br>$e^{tX}=e^t$ with probability $p$, and $1$ with probability $q$, so $M(t) = E(e^{tX})=pe^t + q$</p><p><strong>Example 2 (Geometric MGF)</strong> MGF of $X\sim Geom(p)$</p><script type="math/tex; mode=display">M(t) = E(e^{tX})=\sum_{k=0}^{\infty} e^{tk}q^kp=p\sum_{k=0}^{\infty} (qe^t)^k=\frac{p}{1-qe^t}</script><p>t in $(-\infty, log(1/p))$</p><p><strong>Example 3 (Uniform MGF)</strong> MGF of $U\sim Unif(a,b)$</p><script type="math/tex; mode=display">M(t) = E(e^{tU}) = \int_a^b e^{tu}\frac{1}{b-a} du = \frac{e^{tb}-e^{ta}}{t(b-a)}</script><p>and $M(0) = 1$</p><p><strong>Example 4 (Binomial MGF)</strong> $Bin(n,p)$</p><script type="math/tex; mode=display">M(t) = (pe^t+q)^n</script><p><strong>Example 5 (Negative Binomial)</strong> $NBin(r,p)$</p><script type="math/tex; mode=display">M(t) = \left(  \frac{p}{1-qe^t}  \right)^r</script><p><strong>Theorem (MGF of Location-Scale Transformation)</strong> If $X$ has MGF $M(t)$, then MGF of $a+bX$ is</p><script type="math/tex; mode=display">E(e^{t(a+bX)})=e^{at}E(e^{btX})=e^{at}M(bt)</script><p><strong>Example 6 (Normal MGF)</strong> MGF of $(X = \mu + \sigma Z) \sim N(\mu,\sigma^2)$</p><script type="math/tex; mode=display">M_Z(t) = E(e^{tZ})=\int_{-\infty}^{\infty}e^{tz}\frac{1}{\sqrt{2\pi}}e^{-z^2/2}dz=e^{t^2/2}</script><p>Use the Theorem above then</p><script type="math/tex; mode=display">M_X(t) = e^{\mu t}M_Z(\sigma t) = e^{\mu t}e^{(\sigma t)^2/2}  = e^{\mu t + \frac{1}{2} \sigma^2 t^2}</script><h3 id="Sum-of-Independent-Distributions"><a href="#Sum-of-Independent-Distributions" class="headerlink" title="Sum of Independent Distributions"></a>Sum of Independent Distributions</h3><p><strong>Theorem (MGF of A Sum of Independent R.V.s)</strong> If $X$ and $Y$ are independent, Then</p><script type="math/tex; mode=display">M_{X+Y} (t) = M_X(t) M_Y(t)</script><p><strong>Example 1 (Sum of Poissons)</strong> $X\sim Pois(\lambda), Y\sim Pois(\mu)$, $X$ and $Y$ are independent. Then $X+Y \sim Pois(\lambda + \mu)$<br>The MGF of $X$ is</p><script type="math/tex; mode=display">E(e^{tX}) = \sum_{k=0}^{\infty}e^{tk}\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^{\infty}\frac{(\lambda e^t)^k}{k!}=e^{-\lambda}e^{\lambda e^t}=e^{\lambda(e^t-1)}</script><p>The MGF of $X+Y$ is</p><script type="math/tex; mode=display">E(e^{tX})E(e^{tY}) = e^{\lambda (e^t-1)} e^{\mu (e^t-1)} = e^{(\lambda + \mu)(e^t-1)}</script><p>Which is the $Pois(\lambda + \mu)$, so $X+Y\sum Pois(\lambda+\mu)$</p><p><strong>Example 2 (Sum of Normals)</strong> $X_1\sim N(\mu_1,\sigma_1^2)$ and $X_2 \sim N(\mu_2,\sigma_2^2)$, $X_1+X_2 = ?$<br>MGF of $X_1+X_2$ is</p><script type="math/tex; mode=display">M_{X_1+X_2}(t)=  M_{X_1}(t)M_{X_2}(t)= e^{\mu_1t+\frac{1}{2}\sigma^2_1t^2}\cdot e^{\mu_2 t+\frac{1}{2}\sigma_2^2 t^2}=e^{(\mu_1+\mu_2)t+\frac{1}{2}(\sigma_1^2+\sigma_2^2)t^2}</script><p>Which is the N(\mu_1 + \mu_2, \sigma_2^2 + \sigma_1^2) MGF.</p><hr><h1 id="Probability-Generating-Function"><a href="#Probability-Generating-Function" class="headerlink" title="Probability Generating Function"></a>Probability Generating Function</h1><p><strong>Definition (PGF)</strong> PGF of a nonnegative integer-valued r.v. $X$ with PMF $p_k = P(X=k)$ is the generating function of the PMF, By LOTUS , this is</p><script type="math/tex; mode=display">E(t^X) = \sum_{k=0}^{\infty} p_k t^k</script><p><strong>Example 1 (Generating Dice Probabilities)</strong> Let $X$ be the sum from rolling 6 pair dice, $X_1,…,X_6$ be the individual rolls, what is $P(X=18)$ ?<br>The PGF of $X_1$ is</p><script type="math/tex; mode=display">E(t^{X_1}) = \frac{1}{6}(t+t^2+\dotsb+t^6)</script><p>The PGF of $X$ is</p><script type="math/tex; mode=display">E(t^X) = E(t^{X_1}\dotsb t^{X_6}) = E(t^{X_1})\dotsb E(t^{X_6})=\frac{t^6}{6^6}(1+t+\dotsb +t^5)^6</script><p>The coefficient of $t^{18}$ in the PGF is $P(X=18)$, so</p><script type="math/tex; mode=display">P(X=18) = \frac{3421}{6^6}</script><p><strong>Theorem (PMF \&amp; PGF)</strong></p><script type="math/tex; mode=display">P(X=k) = \frac{g_{X}^{(k)}(0)}{k!}</script><hr><h1 id="Characteristic-Function"><a href="#Characteristic-Function" class="headerlink" title="Characteristic Function"></a>Characteristic Function</h1><p><strong>Definition CF</strong> The Characteristic function of a random variable $X$ is the function $\phi : R \rightarrow C$ defined by</p><script type="math/tex; mode=display">\phi(t) = E(e^{itX}), i = \sqrt{-1}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Generating-Function&quot;&gt;&lt;a href=&quot;#Generating-Function&quot; class=&quot;headerlink&quot; title=&quot;Generating Function&quot;&gt;&lt;/a&gt;Generating Function&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process (Continuous Random Variable)</title>
    <link href="http://yoursite.com/2018/07/25/Stochastic-Process-4/"/>
    <id>http://yoursite.com/2018/07/25/Stochastic-Process-4/</id>
    <published>2018-07-25T12:04:57.000Z</published>
    <updated>2018-07-28T12:05:50.907Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Probability-Density-Function"><a href="#Probability-Density-Function" class="headerlink" title="Probability Density Function"></a>Probability Density Function</h1><p><strong>Definition (Probability Density Function)</strong>: For a continuous r.v. $X$ with CDF $F$, the <strong><em>probability density function (PDF)</em></strong> of $X$ is the derivative $f$ of the $F$</p><script type="math/tex; mode=display">P(a\leq X \leq b) = \int_a^b f_X(x)dx = F(b) - F(a)</script><script type="math/tex; mode=display">P(X\in [x,x+\delta]) \approx f_X(x) \cdot \delta</script><p><strong>Relation between PDF &amp; PMF</strong>: The PDF is the analogous to the PMF in many ways. But, the PDF $f(x)$ is not a probability.</p><p><strong>Relation between PDF &amp; CDF</strong>: Let $X$ be a continuous r.v. with PDF $f$. Then the CDF of $X$ is given by</p><script type="math/tex; mode=display">F(x) = \int_{-\infty}^x f(x) dt</script><p><strong>CDF of Logistic Distribution</strong></p><script type="math/tex; mode=display">F(x) = \frac{e^x}{1+e^x}, x\in R</script><p><strong>CDF of Rayleigh Distribution</strong></p><script type="math/tex; mode=display">F(x) = 1 - e^{-x^2/2}, x>0</script><p><strong>Theorem (Expectation of Continuous R.V.)</strong></p><script type="math/tex; mode=display">E(X) = \int_{-\infty}^{\infty} x f(x) dx</script><p><strong>Theorem (Expectation via Survial Function)</strong> Let $G$ be the survial function of $X$, Then</p><script type="math/tex; mode=display">E(X) = \int_0^{\infty} G(x) dx</script><p><strong>Theorem (LOTUS: Continuous)</strong></p><script type="math/tex; mode=display">E(g(X)) = \int_{-\infty}^{\infty} g(x) f(x) dx</script><hr><h1 id="Uniform-Distribution"><a href="#Uniform-Distribution" class="headerlink" title="Uniform Distribution"></a>Uniform Distribution</h1><p><strong>Definition (Uniform Distribution)</strong> Distribution on the interval$(a,b)$, and its PDF is</p><script type="math/tex; mode=display">f(x) = \left \{  \begin{array}{ll} \frac{1}{b-a} & if\ a<x<b \\ 0 & otherwise \end{array} \right.</script><p>We denote this by $U\sim Unif(a,b)$</p><p><strong>Example</strong>: suppose $X_1,X_2,…,X_n$ are i.i.d $Unif(0,1)$ random variable and let $Y = min(X_1,X_2,…,X_n)$ be their minimum. Find $E(Y)$</p><p>If $0&lt;x&lt;1$, we have $P(X_i \leq x) = x$, it follows that</p><script type="math/tex; mode=display">\begin{align}  P(Y>x)&= P(min (X_1,...,X_n)>x) \\        &= P(X_1>x,...,X_n>x)\\        &= \prod_{i=1}^n P(X_i > x) = (1-x)^n\end{align}</script><p>From above, we use the survial function to calculate the expectation</p><script type="math/tex; mode=display">E(Y) = \int_0^{\infty} P(Y>x) dx = \int_0^1(1-x)^ndx = \int_0^1 x^n dx = \frac{1}{n+1}</script><hr><h1 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h1><p><strong>Definition (Standard Normal Distribution)</strong> A c.r.v. $Z$ is said to have the standard Normal Distribution if its $PDF$ $\varphi$ is given by:</p><script type="math/tex; mode=display">\varphi(z) = \frac{1}{\sqrt(2\pi)} e^{-z^2/2},-\infty < z < \infty</script><p>We write this as $Z\sim N(0,1)$, and $Z$ has mean 0 and variance 1, the CDF $\phi$ is</p><script type="math/tex; mode=display">\phi(z) = \int_{-\infty}^z \varphi(t) dt = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt</script><p><strong>Definition (Normal Distribution)</strong> If $Z\sim N(0,1)$ then</p><script type="math/tex; mode=display">X = \mu + \sigma Z</script><p>is said to have the Normal Distribution with mean $\mu$ and variance $\sigma^2$, denote this by $X\sim N(\mu,\sigma^2)$</p><p><strong>Theorem (Normal CDF and PDF)</strong> Let $X\sim N(\mu, \sigma^2)$,<br>CDF of $X$ is</p><script type="math/tex; mode=display">F(x) = \phi(\frac{x-\mu}{\sigma})</script><p>PDF of $X$ is</p><script type="math/tex; mode=display">f(x) = \varphi (\frac{x-\mu}{\sigma})\frac{1}{\sigma}</script><hr><h1 id="Exponential"><a href="#Exponential" class="headerlink" title="Exponential"></a>Exponential</h1><p><strong>Definition (Exponential Distribution)</strong></p><script type="math/tex; mode=display">f(x) = \lambda e ^{-\lambda x}, x>0</script><p>we denote this by $X\sim Expo(\lambda)$. The corresponding CDF is</p><script type="math/tex; mode=display">F(x) = 1-e^{-\lambda x},x>0</script><p><strong>Theorem (Memoryless Property)</strong></p><script type="math/tex; mode=display">P(X\geq s+t | X\geq s) = P(X\geq t)</script><ul><li>If $X$ is a positive continuous random variable with memoryless property, then $X$ has an Exponential distribution</li><li>Geometric Distribution is also Memoryless</li><li>Exponential distribution as the “continuous counterpart” of the Geometric distribution</li></ul><p><strong>Exponential \&amp; Geometric via $\delta$- Steps</strong><br>We devide a unit of time into n pieces, each of size $\delta = \frac{1}{n}$, and the trial occurs every $\delta$ time period and success with probability $\lambda \delta$. Denote $Y$ as the number of trials until first success, $\hat{Y}$ as the time until first success under $Y$.</p><script type="math/tex; mode=display">Y\sim FS(\lambda \delta)</script><p>Thus we have</p><script type="math/tex; mode=display">F(\hat{Y}) = E(Y)\cdot \delta = \frac{1}{\lambda\delta}\cdot \delta=\frac{1}{\lambda}</script><p>And</p><script type="math/tex; mode=display">\begin{array}P(Y>t) &= P\{ all\ trials\ up\ to\ time\ t\ has\ been\ failures \} \\&=P\{ at\ least\ \frac{t}{\delta}\ failures\}\\&=(1-p)^{\frac{t}{\delta}}=(1-\lambda \delta)^{\frac{t}{\delta}}\\&=\left[  (1-\lambda\delta)^{\frac{1}{\lambda\delta}}  \right]^{\lambda t} \xrightarrow{\delta \rightarrow 0} e^{-\lambda t}\end{array}</script><p><strong>Theorem property of Exponential</strong> Given $X_1\sim Expo(\lambda_1)$, $X_2\sim Expo(\lambda_2)$, $X_1 \bot X_2$ ($X_1$ and $X_2$ are independent), then</p><script type="math/tex; mode=display">P(X_1<X_2) = \frac{\lambda_1}{\lambda_1 + \lambda_2}</script><p>It can be solved by $\delta$-step</p><p>Some physical phenomenon follow the Exponential distribution like the radioactive decay.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Probability-Density-Function&quot;&gt;&lt;a href=&quot;#Probability-Density-Function&quot; class=&quot;headerlink&quot; title=&quot;Probability Density Function&quot;&gt;&lt;/a&gt;Pr
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Generative-Adversarial-Nets</title>
    <link href="http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/"/>
    <id>http://yoursite.com/2018/07/25/Generative-Adversarial-Nets/</id>
    <published>2018-07-25T11:21:44.000Z</published>
    <updated>2018-07-25T11:57:38.016Z</updated>
    
    <content type="html"><![CDATA[<script type="math/tex; mode=display">\mathop{min}_G \mathop{max}_D = E_{}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathop{min}_G \mathop{max}_D = E_{}&lt;/script&gt;
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Process (Expectation)</title>
    <link href="http://yoursite.com/2018/07/22/Stochastic-Process-3/"/>
    <id>http://yoursite.com/2018/07/22/Stochastic-Process-3/</id>
    <published>2018-07-22T02:30:26.000Z</published>
    <updated>2018-07-27T12:39:21.846Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h1><p><strong>Definition (Expectation of R.V.)</strong></p><script type="math/tex; mode=display">E(X) = \sum_{x}\underbrace{x}_{value} \underbrace{P(X=x)}_{PMF\ at\ x}</script><p><strong>Theorem (Monotonicity)</strong>: $X$ and $Y$ are r.v.s. such that $X&gt;Y$ with probability 1.<br>Then $E(X)\geq E(Y)$</p><p><strong>Theorem (Expectation via Survial Function)</strong>: Let $X$ be a nonnegative r.v. Let $F$ be the CDF of $X$, and define survial function of $X$ named $G$ as $G(x) = 1-F(x) = P(X&gt;x)$, Then</p><script type="math/tex; mode=display">E(x) = \sum_{n=0}^{\infty} G(x)</script><p><strong>Theorem (Low Of The Unconscious Statistician(LOTUS))</strong>: If $X$ is discrete r.v. and $g$ is a function from $R$ to $R$, then</p><script type="math/tex; mode=display">E(g(x)) = \sum_x g(x)P(X=x)</script><h3 id="Propertise-of-Expectation"><a href="#Propertise-of-Expectation" class="headerlink" title="Propertise of Expectation"></a>Propertise of Expectation</h3><ul><li>$E(X+Y) = E(X) + E(Y)$</li><li>$E(cX) = c E(x)$</li><li>If $X$ and $Y$ are independent, $E(XY) = E(X) E(Y)$</li></ul><h3 id="Inequalities-of-Expectation"><a href="#Inequalities-of-Expectation" class="headerlink" title="Inequalities of Expectation"></a>Inequalities of Expectation</h3><ul><li>Cauchy–Bunyakovsky–Schwarz inequality<script type="math/tex; mode=display">E[XY]^2\leq E[X^2] E[Y^2]</script></li><li><a href="https://en.wikipedia.org/wiki/Expected_value#Inequalities" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Expected_value#Inequalities</a> …</li></ul><hr><h1 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h1><p><strong>Definition (Variance and Standard Deviation)</strong> variance of an r.v. $X$ is</p><script type="math/tex; mode=display">Var(X) = E(X-EX)^2</script><p>Square root of the variance is standard deviation (SD):</p><script type="math/tex; mode=display">SD(X)  = \sqrt{Var(X)}</script><h3 id="Propertise-of-Variance"><a href="#Propertise-of-Variance" class="headerlink" title="Propertise of Variance"></a>Propertise of Variance</h3><ul><li>For any r.v. $X$, $Var(X) = E(X^2) - (EX)^2$</li><li>$Var(X + c ) = Var(X)$</li><li>$Var(c X ) = c^2Var(X)$</li><li>If $X$ and $Y$ are independent, then $Var(X+Y) = Var(X) + Var(Y)$</li></ul><hr><h1 id="Geometric-and-Negative-Binomial"><a href="#Geometric-and-Negative-Binomial" class="headerlink" title="Geometric and Negative Binomial"></a>Geometric and Negative Binomial</h1><p><strong>Definition (Geometric Distribution)</strong>: Consider a sequence of independent Bernoulli trials, each with the same success probability $p\in (0,1)$, trails performed until a success occurs. Let $X$ be the number of the failures before the first successful trail. Then $X$ has the Geometric Distributions, denote by $X\sim Geom(p)$</p><p><strong>Theorem (Geometric PMF)</strong>: If $X\sim Geom(p)$, then the PMF of $X$ is</p><script type="math/tex; mode=display">P(X=k) = (1-p)^kp</script><p><strong>Theorem (Memoryless Property)</strong>: If $X\sim Geom(p)$, then for positive integer n</p><script type="math/tex; mode=display">P(X\geq n+k | X \geq k ) = P(X\geq n)</script><p><strong>Definition (First Success Distribution)</strong>: very similay to Geometric $X$, Let it be $Y$, and $X+1 = Y$ …. , we denote it by $FS(p)$</p><p><strong>Definition (Negative Binomial Distribution)</strong>: In a sequence of independent Bernoulli trails with p, if $X$ is the number of failures before $r^{th}$ success, then $X$ is the Negative Binomial Distribution with $r$ and $p$, denoted by $X\sim NBin(r, p)$</p><p><strong>Theorem (Negative Binomial PMF)</strong>: If $X\sim NBin(r,p)$, then the PMF of $X$ is</p><script type="math/tex; mode=display">P(X=n) = \left ( \begin{array}{c} n+r-1 \\ r-1 \end{array}   \right )p^r(1-p)^n</script><p><strong>Theorem (Geometric &amp; Negative Binomial)</strong>: Let $X\sim NBin(r,p)$, and $X_i$ are $i.i.d. Geom(p)$ , Then we have $X= X_1+\dotsb + X_r$</p><hr><h1 id="Indicator-R-V"><a href="#Indicator-R-V" class="headerlink" title="Indicator R.V."></a>Indicator R.V.</h1><p><strong>Definition (Indicator R.V.)</strong></p><script type="math/tex; mode=display">I_A =\left \{  \begin{array}{ll}    1 & if\ A\ occurs \\    0 & otherwise  \end{array}\right .</script><h3 id="Propertise-of-Indicator-R-V"><a href="#Propertise-of-Indicator-R-V" class="headerlink" title="Propertise of Indicator R.V."></a>Propertise of Indicator R.V.</h3><ul><li>$(I_A)^k = I_A$</li><li>$I_{A^c} = 1- I_A$</li><li>$I_{A\cap B} = I_A I_B$</li><li>$I_{A\cup B} = I_A + I_B - I_A I_B$</li></ul><p><strong>Theorem (Bridge between Probability &amp; Expectation)</strong></p><script type="math/tex; mode=display">P(A) = E(I_A)</script><p><strong>Example 1</strong>: Au urn contain R G B three balls, r g b is probability draw a ball from it (r+g+b = 1), whats the expected number of different <strong><em>colors</em></strong> of ball before getting the first R ball ?<br>Let $I_g$ be the $1$ if G is obtained before R, and define the $I_b$ similarly. Then</p><script type="math/tex; mode=display">E(I_g) = P(green\ before\ red) = \frac{g}{g+r}</script><p>since “green before red” means that first non-blue ball is green , so probability is $frac{g}{g+r}$, then, the final result is</p><script type="math/tex; mode=display">E(I_g+I_b) = \frac{g}{g+r} + \frac{b}{b+r}</script><h3 id="Moments-amp-Indicators"><a href="#Moments-amp-Indicators" class="headerlink" title="Moments &amp; Indicators"></a>Moments &amp; Indicators</h3><p>Given n events $A_1,\dotsb, A_n$ and indicators $I_j, j = 1, \dotsb, n$</p><ul><li>$X = \sum_{j=1}^n I_j$: the number of events occur</li><li><script type="math/tex">\left( \begin{array}{c} X \\ 2 \end{array} \right) = \sum_{i<j}I_iI_j</script>: the number of pairs of events that occur</li><li><script type="math/tex">E( \left( \begin{array}{c} X \\ 2 \end{array} \right) ) = \sum_{i<j} P(A_iA_j)</script> .<ul><li>$E(X^2) = 2\sum_{i&lt;j} P(A_iA_j) + E(X)$</li><li>$Var(X) = 2\sum_{i&lt;j} P(A_iA_j) + E(X) - (E(X))^2$</li></ul></li></ul><hr><h1 id="Poisson"><a href="#Poisson" class="headerlink" title="Poisson"></a>Poisson</h1><p><strong>Definition (Poisson Distribution)</strong> $X\sim Pois(\lambda)$</p><script type="math/tex; mode=display">P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!},k=0,1,2,\dotsb</script><h3 id="Property-of-poisson"><a href="#Property-of-poisson" class="headerlink" title="Property of poisson"></a>Property of poisson</h3><ul><li>$E(X) = \lambda$</li><li>$E(X^2)  = \lambda(1+\lambda)$</li><li>$Var(X) = \lambda$</li></ul><p><strong>Poisson Approximation</strong>: Let $A_1,A_2,\dotsb,A_n$ be events with $p_j = P(A_j)$, here $n$ is larger where $p_j$ is small , and $A_j$ is independent or weakly dependent, let <script type="math/tex">X = \sum_{j=1}^n I(A_j)</script> count how many $A_j$ occur. Then $X$ is approximately $Pois(\lambda)$, with $\lambda = \sum_{j=1}^n p_j$</p><p><strong>Theorem（Sum of Independent Poisson)</strong>: If $X\sim Pois(\lambda_1), Y\sim Pois(\lambda_2)$, and $X$ is independent of $Y$, then $X+Y \sim Pois(\lambda_1 + \lambda_2)$</p><p><strong>Theorem（Poisson Approximation to Binomial)</strong>: If $X\sim Bin(n,p)$ and we let $n \rightarrow \infty and p\rightarrow 0$ , such that $\lambda = np$, then the PMF of $X$ converges to the $Pois(\lambda)$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Expectation&quot;&gt;&lt;a href=&quot;#Expectation&quot; class=&quot;headerlink&quot; title=&quot;Expectation&quot;&gt;&lt;/a&gt;Expectation&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Definition (Expectati
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Neural Style</title>
    <link href="http://yoursite.com/2018/07/19/Neural-Style/"/>
    <id>http://yoursite.com/2018/07/19/Neural-Style/</id>
    <published>2018-07-19T02:25:16.000Z</published>
    <updated>2018-07-22T02:31:09.823Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Nerual-Style"><a href="#Nerual-Style" class="headerlink" title="Nerual Style"></a>Nerual Style</h1><p>There are two aspect for a image, one is the <strong>content</strong> of the image, which can be descriped as elements or object in the image, another is the <strong>style</strong> of the image, it might be abstract, and usually revealed by the painting skill or technique.</p><p>Shortly, we have two image, one for style while the other for content. now, we want to combine the style in image1 and the content in image2 together, and it can be achieved from deep neural net work, and we call it <strong>Neural Style</strong>.</p><p>Moreover we simply define the loss function care both style and content</p><script type="math/tex; mode=display">L_{total} = \alpha L_{content}+\beta L_{style}</script><p>Now let’s have a look about what neural network can do here, and analysis the affect of $L_{content}$ and $L_{style}$ independently.</p><p>Suppose we have the content image, and send it to the neural network, it will have the responses in each layer by filters, we also construct a white noisy image, filter it in the same way, and define a loss $L_{content}$ between filtered content and filtered noisy, we take the noisy image as input,and it can update iterativly.</p><p><img src="/2018/07/19/Neural-Style/img1.jpg" alt="reconstruction"></p><p>The image above show the reconstruction result between different layers, and reconstruction from lower layers(a,b,c) is alomost perfect, the style reconstruction may be more realistic in the deeper layer.</p><p>Let’s get familiar with some notion of the formulation first( suppse we are in the $l^{th}$ level of the net ):</p><ul><li><strong>$\vec{p}$</strong>: Original <strong>content</strong> image (input)<ul><li><strong>$P^l$</strong>: Content feature representation in layer $l$ respect to $\vec{p}$</li></ul></li><li><strong>$\vec{a}$</strong>: Original <strong>style</strong> image (input)<ul><li><strong>$A^l$</strong>: Style feature representation in layer $l$ respect to $\vec{a}$</li></ul></li><li><p><strong>$\vec{x}$</strong>: Target image (output)</p><ul><li><strong>$F^l$</strong>: Content feature representation in layer $l$ respect to $\vec{x}$</li><li><strong>$G^l$</strong>: Style feature representation in layer $l$ respect to $\vec{x}$</li><li><strong>$F_{ij}^l$</strong>: Element of $i^{th}$ filter at $j^{th}$ position in layer $l$</li></ul></li><li><p><strong>$N_l$</strong>: The number of the filters in the $l^{th}$ level</p></li><li><strong>$M_l$</strong>: The size of a feature map produced by a filter,usually it equals to $height \times weight$</li></ul><p>The squared-error loss between two content feature representations is:</p><script type="math/tex; mode=display">L_{content}(\vec{p},\vec{x},l) = \frac{1}{2} \sum_{i,j}(F_{ij}^l - P_{ij}^l)^2</script><p>In each layer, build a style representation compute the correlations between the different filter responses, which is called Gram Matrix $G^l\in R^{N_l \times N_l}$, and $G_{ij}^l$ is the inner product between the vectorized feature map between $i$ and $j$ in layer $l$</p><script type="math/tex; mode=display">G_{ij}^l = \sum_k F_{ik}^l F_{jk}^l</script><p>Also we have</p><script type="math/tex; mode=display">A_{ij}^l = \sum_k P_{ik}^l P_{jk}^l</script><p>The contribution of the layer to the total loss is</p><script type="math/tex; mode=display">E_l = \frac{1}{4N_l^2 M_l^2 }\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2</script><p>And the total loss is</p><script type="math/tex; mode=display">L_{style}(\vec{a},\vec{x}) = \sum_{l=0}^L w_lE_l</script><p>Let’s focus more on the detail about the gradient of the loss:</p><p>The derivative of content loss respect to activations in layer l equals</p><script type="math/tex; mode=display">\frac{\partial L_{content}}{\partial F_{ij}^l} =\left \{  \begin{array}{ll}    (F^l - P^l)_{ij} & if\ F_{ij}^l > 0 \\    0 & if\ F_{ij}^l < 0  \end{array}\right .</script><p>The derivative of style loss respect to activations in layer l equals</p><script type="math/tex; mode=display">\frac{\partial E_l}{\partial F_{ij}^l} =\left \{  \begin{array}{ll}    \frac{1}{N_l^2M_l^2}((F^l)^T(G^l - A^l))_{ij}& if\ F_{ij}^l > 0 \\    0 & if\ F_{ij}^l < 0  \end{array}\right .</script><p>The final loss function we want to minimize is</p><script type="math/tex; mode=display">L_{total}(\vec{p},\vec{a},\vec{x}) = \alpha L_{content}(\vec{p},\vec{x}) + \beta L_{style}(\vec{a},\vec{x})</script><hr><h1 id="Fast-Neural-Style"><a href="#Fast-Neural-Style" class="headerlink" title="Fast Neural Style"></a>Fast Neural Style</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2018/07/19/Neural-Style/img2.jpg" alt="FastNet" title="">                </div>                <div class="image-caption">FastNet</div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Nerual-Style&quot;&gt;&lt;a href=&quot;#Nerual-Style&quot; class=&quot;headerlink&quot; title=&quot;Nerual Style&quot;&gt;&lt;/a&gt;Nerual Style&lt;/h1&gt;&lt;p&gt;There are two aspect for a ima
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Process (Random Variable)</title>
    <link href="http://yoursite.com/2018/07/07/Stochastic-Process-1/"/>
    <id>http://yoursite.com/2018/07/07/Stochastic-Process-1/</id>
    <published>2018-07-07T00:21:04.000Z</published>
    <updated>2018-07-22T02:55:04.070Z</updated>
    
    <content type="html"><![CDATA[<p>There are some notation occationals</p><p><strong>Definition (Discrete Random Variable)</strong> A variable $X$ is <strong><em>discrete</em></strong> if there is a finite list of value $a_1,a_2,…,a_n$ that $P(X=a_j) = 1$, $P(X=x)&gt;0$ is the <strong><em>support</em></strong> of $X$</p><p><strong>Definition (Probability Mass Function)</strong> <strong><em>The probability mass function (PMF)</em></strong> of a discrete r.v. $X$ is the function $p_X$ given by $p_X(x) = P(X=x)$.</p><h1 id="Bernoulli-amp-Binomial"><a href="#Bernoulli-amp-Binomial" class="headerlink" title="Bernoulli &amp; Binomial"></a>Bernoulli &amp; Binomial</h1><p><strong>Definition (Bernoulli Distribution)</strong> shortly, $P(X=1)=p$ and $P(X=0) = 1 - p$, and write as $X\sim Bern(p)$</p><p><strong>Definition (Indicator Random Variable)</strong> The indicator random variable of an event $A$ is the r.v. equals 1 if $A$ occurs and 0 otherwise, We denote the indicator of $A$ by $I_A$ or $I(A)$. Note $I_A \sim Bern(p)$ with p = P(A)</p><p><strong>Theorem (Binomial PMF)</strong> Binomial Distribution is the repeatation of Bernoulli Distribution. If $X\sim Bin(n, p)$ then the PMF of X is</p><script type="math/tex; mode=display">P(X=k) = \left( \begin{array}{c} n \\ k \end{array} \right) p^k (1-p)^{n-k}</script><h1 id="Hypergeometric"><a href="#Hypergeometric" class="headerlink" title="Hypergeometric"></a>Hypergeometric</h1><p><strong>urn Model</strong> A box is fiiled with $w$ white and $b$ black balls, then drawing n balls</p><ul><li>With replacement: $Bin(n,w/(w+b))$ for the number of white balls</li><li>Without replacement : Hypergeometric distribution $HGeom(w,b,n)$</li></ul><p><strong>Theorem (Hypergeometric PMF)</strong> If $X \sim HGeom(w,b,n)$, then the PMF of $X$ is</p><script type="math/tex; mode=display">P(X=k) = \frac {\left ( \begin{array}{c} w \\ k \end{array} \right ) \left( \begin{array}{c} b \\ n-k \end{array} \right)}  {\left( \begin{array}{c} w+b \\ n \end{array} \right)}</script><p><strong>Zipf Distribution</strong> If $X\sim Zipf(\alpha &gt; 0)$, then PMF of $X$ is:</p><script type="math/tex; mode=display">P(X=k) = \frac{\frac{1}{k^{\alpha + 1}}}  {\sum_{j=1}^{\infty}(\frac{1}{j})^{\alpha + 1}}</script><ul><li>Zipf Distribution can measure the Word Frequency</li></ul><h1 id="Cumulative-Distribution-Functions"><a href="#Cumulative-Distribution-Functions" class="headerlink" title="Cumulative Distribution Functions"></a>Cumulative Distribution Functions</h1><p><strong>Definition (Cumulative Distribution Function)</strong> The <strong><em>cumulative distribution function(CDF)</em></strong> os an r.v. $X$ is the function $F_X$ given by $F_X(x) = P(X\leq x)$</p><p><strong>Theorem (Valid CDFs)</strong> CDF has the following properties</p><ul><li>Increasing: If $x_1 &lt; x_2$, then $F(x_1) &lt; F(x_2)$</li><li>Right-Continuous: $F(a)  = lim_{x\rightarrow a^+} F(x)$</li><li>Convergence to $0$ and $1$: $lim_{x\rightarrow - \infty} F(x) = 0$ and $lim_{x \rightarrow \infty} F(x) = 1$</li></ul><h1 id="Functions-of-Random-Variable"><a href="#Functions-of-Random-Variable" class="headerlink" title="Functions of Random Variable:"></a>Functions of Random Variable:</h1><p><strong>Definition (Function of an r.v.)</strong> An experiment with sample space S, an r.v. $X$, and a function $g$, also the $g(X)$ is the variable that maps $s$ to $g(X(s))$, for all $s\in S$</p><p><strong>Theorem (PMF of $g(X)$)</strong> for all y in the support of $g(X)$</p><script type="math/tex; mode=display">P(g(X) = y) = \sum_{x:g(x)=y} P(X=x)</script><p>The function of r.v. map the sample space into real number, which is easy for us calculate in mathematic.</p><h1 id="Independence-of-R-V-s"><a href="#Independence-of-R-V-s" class="headerlink" title="Independence of R.V.s"></a>Independence of R.V.s</h1><p><strong>Definition (Independence of two R.V.s)</strong> Random variables $X$ and $Y$ are said to be <strong><em>independent</em></strong></p><script type="math/tex; mode=display">P(X\leq x,Y\leq y) = P(X\leq x)P(Y\leq y)</script><p>for all $x,y\in R$,<br>In the discrete case, equivalent to :</p><script type="math/tex; mode=display">P(X=x,Y=y) = P(X=x)P(Y=y)</script><p><strong>Definition (Independence of many R.V.s)</strong> Random variables $X_1,…,X_n$ are independent if</p><script type="math/tex; mode=display">P(X_1 \leq x_1,\dotsb , X_n \leq x_n) = P(X_1 \leq x_1) \dotsb P(X_n \leq x_n)</script><p>for all $x_1,\dotsb,x_n \in R$</p><p><strong>Definition (i.i.d)</strong> We call some r.v. that are independent and have the same distribution <strong><em>independent and identicallly distributed</em></strong> or <strong><em>i.i.d</em></strong> for short</p><ul><li>Independent: r.v.s provide no information about each others</li><li>Identically distributed: r.v.s have the same PMF</li></ul><p><strong>Theorem</strong> If $X\sim Bin(n,p)$ , $Y \sim Bin(m,p)$, and $X$ is independent of $Y$, then $X+Y \sim Bin(n+m,p)$</p><p><strong>Definition (Conditional Independence of two R.V.s)</strong></p><script type="math/tex; mode=display">P(X\leq x, Y \leq y| Z= z) = P(X\leq x |Z =z) P(Y\leq y|Z=z)</script><p>w</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There are some notation occationals&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition (Discrete Random Variable)&lt;/strong&gt; A variable $X$ is &lt;strong&gt;&lt;em&gt;discrete&lt;
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic Process (Conditional Probability)</title>
    <link href="http://yoursite.com/2018/07/06/Stochastic-Process-2/"/>
    <id>http://yoursite.com/2018/07/06/Stochastic-Process-2/</id>
    <published>2018-07-06T00:24:34.000Z</published>
    <updated>2018-07-23T12:01:35.232Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Defination-of-Conditonal-Probability"><a href="#Defination-of-Conditonal-Probability" class="headerlink" title="Defination of Conditonal Probability"></a>Defination of Conditonal Probability</h1><p><strong>Defination</strong> Two events $A$ and $B$, with $P(B)&gt;0$, the conditional probability of $A$ given $B$ , denoted by $P(A|B)$, is defined as :</p><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><ul><li>$P(A)$: prior probability</li><li>$P(A|B)$: posterior probability</li></ul><hr><h1 id="Bayes’-Rule-amp-LOTP"><a href="#Bayes’-Rule-amp-LOTP" class="headerlink" title="Bayes’ Rule &amp; LOTP"></a>Bayes’ Rule &amp; LOTP</h1><h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2018/07/06/Stochastic-Process-2/sp1.jpg" alt="chain rule" title="">                </div>                <div class="image-caption">chain rule</div>            </figure><script type="math/tex; mode=display">P(A_1,...,A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1,...,A_{n-1})</script><h3 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h3><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)P(A)}{P(B)}</script><h3 id="LOTP-Law-of-Total-Probability"><a href="#LOTP-Law-of-Total-Probability" class="headerlink" title="LOTP (Law of Total Probability)"></a>LOTP (Law of Total Probability)</h3><p><img src="/2018/07/06/Stochastic-Process-2/sp2.jpg" align="center" style=" width:300px;"></p><p><strong>Theorem</strong>: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_1)&gt;0$, Then :</p><script type="math/tex; mode=display">P(B) = \sum_i^n P(B|A_i)P(A_i)</script><p><strong>Theorem</strong>: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, for any event $B$ such that $P(B) &gt; 0$, we have :</p><script type="math/tex; mode=display">P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\dotsb+P(A_n)P(B|A_n)}</script><hr><h1 id="Conditional-Probabilitiy"><a href="#Conditional-Probabilitiy" class="headerlink" title="Conditional Probabilitiy"></a>Conditional Probabilitiy</h1><p>Conditional Probability is also the probability, so it inherent the property of probability (suppose the sample space is $S$):</p><ul><li>$P(S|E) = 1$ and $P(\emptyset|E) = 0$</li><li>if events $A_1,…$ are disjoint, then $P(\cup_{j=1}^{\infty}A_j|E) = \sum_{j=1}^{\infty}P(A_j|E)$</li><li>$P(A^c|E) = 1 - P(A|E)$</li><li>Inclusion-Exclusion : $P(A\cup B|E) = P(A|E) + P(B|E) - P(A\cap B|E)$</li></ul><h3 id="Bayes’-Rule-with-Extra-Condition"><a href="#Bayes’-Rule-with-Extra-Condition" class="headerlink" title="Bayes’ Rule with Extra Condition:"></a>Bayes’ Rule with Extra Condition:</h3><p><strong>Theorem</strong>: Provided that $P(A\cap E)&gt;0$ and $P(B\cap E)&gt;0$, we have:</p><script type="math/tex; mode=display">P(A|B,E) = \frac{P(B|A,E)P(A|E)}{P(B|E)}</script><h3 id="LOTP-with-Extra-Condition"><a href="#LOTP-with-Extra-Condition" class="headerlink" title="LOTP with Extra Condition:"></a>LOTP with Extra Condition:</h3><p><strong>Theorem</strong>: Let $A_1,A_2,…,A_n$ be the partition of the sample space $S$, with $P(A_i \cap E) &gt;0$, Then:</p><script type="math/tex; mode=display">P(B|E) = \sum_{i=1}^n P(B|A_i,E)P(A_i|E)</script><h3 id="Approaches-for-P-A-B-C"><a href="#Approaches-for-P-A-B-C" class="headerlink" title="Approaches for $P(A|B,C)$"></a>Approaches for $P(A|B,C)$</h3><ul><li><script type="math/tex; mode=display">P(A|B,C) = \frac{P(A,B,C)}{P(B,C)}</script></li><li><script type="math/tex; mode=display">P(A|B,C) = \frac{P(B|A,C)P(A|C)}{P(B|C)}</script></li><li><script type="math/tex; mode=display">P(A|B,C) = \frac{P(C|A,B)P(A|B)}{P(A|C)}</script></li></ul><hr><h1 id="Independence-of-Events"><a href="#Independence-of-Events" class="headerlink" title="Independence of Events"></a>Independence of Events</h1><h3 id="Independence-of-Two-Events"><a href="#Independence-of-Two-Events" class="headerlink" title="Independence of Two Events"></a>Independence of Two Events</h3><p><strong>Defination</strong>: Events $A$ and $B$ are independent if</p><script type="math/tex; mode=display">P(A\cap B) = P(A) P(B)  \Leftrightarrow P(A|B) = P(A) , P(B|A) = P(B)</script><h3 id="Independence-vs-Disjointness"><a href="#Independence-vs-Disjointness" class="headerlink" title="Independence vs Disjointness"></a>Independence vs Disjointness</h3><ul><li>$A,B$ is disjoint : $P(A\cap B) = 0$</li><li>$A,B$ is independent : $P(A) = 0, P(B) = 0$</li></ul><h3 id="Conditional-Independence"><a href="#Conditional-Independence" class="headerlink" title="Conditional Independence"></a>Conditional Independence</h3><p><strong>Defination</strong>: Events $A$ and $B$ are conditionally independent given E if:</p><script type="math/tex; mode=display">P(A\cap B|E) = P(A|E)P(B|E)</script><script type="math/tex; mode=display">Contitional\ Independence \nRightarrow Independence</script><script type="math/tex; mode=display">Independence \nRightarrow Contitional\ Independence</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Defination-of-Conditonal-Probability&quot;&gt;&lt;a href=&quot;#Defination-of-Conditonal-Probability&quot; class=&quot;headerlink&quot; title=&quot;Defination of C
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Robust PCA</title>
    <link href="http://yoursite.com/2018/07/04/Robust-PCA/"/>
    <id>http://yoursite.com/2018/07/04/Robust-PCA/</id>
    <published>2018-07-04T14:47:11.000Z</published>
    <updated>2018-07-25T11:29:24.589Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="Introduction-to-RPCA"><a href="#Introduction-to-RPCA" class="headerlink" title="Introduction to RPCA"></a>Introduction to RPCA</h1><p><br><br>The data we collect usually have the low rank property, but the property will vanished when the data is collected causing the noisy, but we can still decomposite the matrix into low-rank matrix and spares error matrix from the corruped data.</p><script type="math/tex; mode=display">  D=\underbrace{A}_{\text{low rank matrix}}+\underbrace{E}_{\text{sparse matrix}} \notag</script><p>Traditional approach for solving this problem is using PCA (Principal Components Analysis), there are many interpretation to PCA, one relate to rank is despiting the low value singular value as this componets contribute less to the data. Thus, it can be considered as the noisy. So we take the $k^{th}$ largest singular value and drop the rest , this can be represnt as the following formulation :</p><script type="math/tex; mode=display">  \mathop{min}_{A,E} \|E \|_F, \ \ \ \  \text{subject to } \ rank(A)\leq r, D = A + E \notag</script><p>PCA has a shortage that it is not robust to the outliers,  then the RPCA (Robust Principal Components Analysis) came out, RPCA could making the matrix recovery whether the noisy is large or not only if the sparse property is confirmed, the original form of the RPCA can be written as :</p><script type="math/tex; mode=display">  \mathop{min}_{A,E} rank(A) + \|E\|_0, \ \ \ \  \text{subject to } \ D = A + E \notag</script><p>The optimization formulation above is non-convex and is hard to get the solution, we can use the convex relax technology apply on it, then it turn out into the most used and the most efficient from:</p><script type="math/tex; mode=display">  \mathop{min}_{A,E} \|A\|_* + \|E\|_1, \ \ \ \  \text{subject to } \ D = A + E \notag</script><p>$| \cdot |_*$ is the unclear norm, which is the sum of the all singular values : $\sum_i^n\sigma_i$, $l_1$ norm of matrix $| \cdot |_1$ is the sum of absolute value of all the element : $\sum_i^n \sum_j^n |D_{ij}|$ .</p><hr><h1 id="Algorithm-of-RPCA"><a href="#Algorithm-of-RPCA" class="headerlink" title="Algorithm of RPCA"></a>Algorithm of RPCA</h1><p><br><br>Before introducting the Algorithm, we first introducing the two operators</p><h2 id="Singular-Value-Thresholding"><a href="#Singular-Value-Thresholding" class="headerlink" title="Singular Value Thresholding"></a>Singular Value Thresholding</h2><p>The optimal solution to the optimization problem : $\frac{1}{2} | X- Y |_F^2 + \tau |X|_*$ with the variable $X$ is thresholing the singular value of $X$</p><script type="math/tex; mode=display">\begin{align}  \mathcal{D}_{\tau}(X) :=  U \mathcal{D}_{\tau} (\Sigma) V^{\prime} ,  \ \ \mathcal{D}_{\tau}(\Sigma) = diag (  \{  \sigma_i - \tau  \}  )  \notag \\  \mathcal{D}_{\tau}(Y) = \mathop{arg} \mathop{min}_{X} \left \{    \frac{1}{2} \| X- Y \|_F^2 + \tau \|X\|_*  \right \} \notag\end{align}</script><h2 id="Soft-Thresholding"><a href="#Soft-Thresholding" class="headerlink" title="Soft Thresholding"></a>Soft Thresholding</h2><p>As same as the $l_1$ norm in vector, thresholding the absolute value of all the element in $X$.</p><script type="math/tex; mode=display">\begin{align}  \psi_{st}(Y) =  \mathop{arg} \mathop{min}_{X} \left \{    \frac{1}{2} \| X- Y \|_F^2 + \tau \|X\|_1  \right \} \notag\end{align}</script><p>There are various methods to solving the RPCA problem, the most successful one is slove the Augmented Lagrangian function of the original problem which we called ALM algorithm, the Augmented Lagrangian function is:</p><script type="math/tex; mode=display">\begin{align}  L(A,E,Y,\mu) = \|A\|_* + \lambda\|E\|_1+ \langle Y,D-A-E \rangle + \frac{\mu}{2} \| D- A -E \|_F^2 \notag\end{align}</script><p>Usually, we use ADMM to slove the ALM problems :</p><script type="math/tex; mode=display">\begin{align}  A_{k+1} &= SVT_{1/\mu_k}(D-E_k + \mu_k^{-1} Y_k) \notag \\  E_{k+1} &= ST_{\lambda/\mu_k} (D - A_{k+1} + \mu_k^{-1} Y_k) \notag \\  Y_{k+1} &= Y_k + \mu_k ( D - A_{k+1} - E_{k+1} ) \notag\end{align}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;Introduction-to-RPCA&quot;&gt;&lt;a href=&quot;#Introduction-to-RPCA&quot; class=&quot;headerlink&quot; title=&quot;Introduction to RPCA&quot;&gt;&lt;/a&gt;Introduction to RPCA&lt;
      
    
    </summary>
    
    
      <category term="Low-Rank" scheme="http://yoursite.com/tags/Low-Rank/"/>
    
  </entry>
  
  <entry>
    <title>Stochastic-Process （Abbreviate &amp; Notation）</title>
    <link href="http://yoursite.com/2018/07/02/Stochastic-Process-0/"/>
    <id>http://yoursite.com/2018/07/02/Stochastic-Process-0/</id>
    <published>2018-07-02T08:54:34.000Z</published>
    <updated>2018-08-04T07:18:11.420Z</updated>
    
    <content type="html"><![CDATA[<p><strong>r.v.s.</strong>: random variable sequence</p><p><strong>i.i.d</strong>: individual identical distribution</p><p><strong>w.p.</strong>: with probability</p><p><strong>PMF (Probability Mass Function)</strong> $P(X=x)$</p><p><strong>CDF (Cumulative Distribution Function)</strong> $P(X \leq x)$</p><p><strong>PDF (Probability Density Function)</strong> derivate of CDF</p><p><strong>PGF (Probability Generating Function)</strong> $E(t^X) = \sum_{k=0}^{\infty} p_k t^k$</p><p><strong>MGF (Moment Generating Function)</strong> $M(t) = E(e^{tX})$</p><hr><h1 id="Distributions"><a href="#Distributions" class="headerlink" title="Distributions"></a>Distributions</h1><ul><li><strong>Binomial Distribution</strong> $X\sim B(n,p)$: number of success in n trails</li><li><strong>HyperGeometric Distribution</strong> $X\sim HGeom(w,b,n)$: draw n balls between w white and b black</li><li><strong>Geometric Distribution</strong> $X\sim Geom(p)$: number of the Bernoulli trails before success (First Success Distribution)</li><li><strong>Negative Binomial Distribution</strong> $X\sim NBin(r,p)$: number of the Bernoulli trails before $r^{th}$ success</li><li><strong>Poisson Distribution</strong> $X\sim Pois(\lambda)$: number of times an event occurs in an interval of time or space</li><li><strong>Uniform Distribution</strong> $U\sim Unif(a,b)$: Distribution on the interval $(a,b)$</li><li><strong>Standard Normal Distribution</strong> $X\sim N(0,1)$</li><li><strong>Normal Distribution</strong> $X\sim N(\mu,\sigma^2)$</li><li><strong>Beta Distribution</strong> $X\sim Beta(a,b)$</li><li><strong>Multinomial Distribution</strong> $\mathbf{X}\sim Mult_k(n,\mathbf{p})$</li></ul><p><img src="/2018/07/02/Stochastic-Process-0/img1.jpg" align="justify" style=" width:600px;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;r.v.s.&lt;/strong&gt;: random variable sequence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;i.i.d&lt;/strong&gt;: individual identical distribution&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;w.p.&lt;/s
      
    
    </summary>
    
    
      <category term="Probability" scheme="http://yoursite.com/tags/Probability/"/>
    
  </entry>
  
</feed>
